,id,raw_text,preprossed
0,1810.04158,"
        While convolutional neural networks are dominating the field of computer vision, one usually does not have access to the large amount of domain-relevant data needed for their training. It thus became common to use available synthetic samples along domain adaptation schemes to prepare algorithms for the target domain. Tackling this problem from a different angle, we introduce a pipeline to map unseen target samples into the synthetic domain used to train task-specific methods. Denoising the data and retaining only the features these recognition algorithms are familiar with, our solution greatly improves their performance. As this mapping is easier to learn than the opposite one (ie to learn to generate realistic features to augment the source samples), we demonstrate how our whole solution can be trained purely on augmented synthetic data, and still perform better than methods trained with domain-relevant information (eg real images or realistic textures for the 3D models). Applying our approach to object recognition from texture-less CAD data, we present a custom generative network which fully utilizes the purely geometrical information to learn robust features and achieve a more refined mapping for unseen color images.
        △ Less
",convolut neural network domin field comput vision one usual access larg amount domain relev data need train thu becam common use avail synthet sampl along domain adapt scheme prepar algorithm target domain tackl problem differ angl introduc pipelin map unseen target sampl synthet domain use train task specif method denois data retain featur recognit algorithm familiar solut greatli improv perform map easier learn opposit one ie learn gener realist featur augment sourc sampl demonstr whole solut train pure augment synthet data still perform better method train domain relev inform eg real imag realist textur model appli approach object recognit textur less cad data present custom gener network fulli util pure geometr inform learn robust featur achiev refin map unseen color imag less
1,1810.04152,"
        Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling, 2013; Rezende et al., 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al., 2018; Le et al., 2018). Roeder et al. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio, 2014), and the jackknife variational inference (JVI) gradient (Nowozin, 2018). Finally, we show that this computationally efficient, unbiased drop-in gradient estimator translates to improved performance for all three objectives on several modeling tasks.
        △ Less
",deep latent variabl model becom popular model choic due scalabl learn algorithm introduc kingma well rezend et al approach maxim variat lower bound intract log likelihood observ data burda et al introduc multi sampl variat bound iwa least tight standard variat lower bound becom increasingli tight number sampl increas counterintuit typic infer network gradient estim iwa bound perform poorli number sampl increas rainforth et al le et al roeder et al propos improv gradient estim howev unabl show unbias show fact bias bia estim effici second applic reparameter trick doubli reparameter gradient dreg estim suffer number sampl increas resolv previous rais issu idea use improv mani recent introduc train techniqu latent variabl model particular show estim reduc varianc iwa gradient reweight wake sleep updat rw bornschein bengio jackknif variat infer jvi gradient nowozin final show comput effici unbias drop gradient estim translat improv perform three object sever model task less
2,1810.04150,"
        The success of the exascale supercomputer is largely debated to remain dependent on novel breakthroughs in technology that effectively reduce the power consumption and thermal dissipation requirements. In this work, we consider the integration of co-processors in high-performance computing (HPC) to enable low-power, seamless computation offloading of certain operations. In particular, we explore the so-called Vision Processing Unit (VPU), a highly-parallel vector processor with a power envelope of less than 1W. We evaluate this chip during inference using a pre-trained GoogLeNet convolutional network model and a large image dataset from the ImageNet ILSVRC challenge. Preliminary results indicate that a multi-VPU configuration provides similar performance compared to reference CPU and GPU implementations, while reducing the thermal-design power (TDP) up to 8x in comparison.
        △ Less
",success exascal supercomput larg debat remain depend novel breakthrough technolog effect reduc power consumpt thermal dissip requir work consid integr co processor high perform comput hpc enabl low power seamless comput offload certain oper particular explor call vision process unit vpu highli parallel vector processor power envelop less w evalu chip infer use pre train googlenet convolut network model larg imag dataset imagenet ilsvrc challeng preliminari result indic multi vpu configur provid similar perform compar refer cpu gpu implement reduc thermal design power tdp x comparison less
3,1810.04147,"
        Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs). VAEs consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function. GANs, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data. The lack of having explicit probability models in GANs prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems. In this work, we show that an optimal transport GAN with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that VAEs are based on. In particular, our proof constructs an explicit probability model for GANs that can be used to compute likelihood statistics within GAN's framework. Our numerical results on several datasets demonstrate consistent trends with the proposed theory.
        △ Less
",build success deep learn two modern approach learn probabl model observ data gener adversari network gan variat autoencod vae vae consid explicit probabl model data comput gener distribut maxim variat lower bound log likelihood function gan howev comput gener model minim distanc observ gener probabl distribut without consid explicit model observ data lack explicit probabl model gan prohibit comput sampl likelihood framework limit use statist infer problem work show optim transport gan entropi regular view gener model maxim lower bound averag sampl likelihood approach vae base particular proof construct explicit probabl model gan use comput likelihood statist within gan framework numer result sever dataset demonstr consist trend propos theori less
4,1810.04146,"
        In this work, we consider the integration of MPI one-sided communication and non-blocking I/O in HPC-centric MapReduce frameworks. Using a decoupled strategy, we aim to overlap the Map and Reduce phases of the algorithm by allowing processes to communicate and synchronize using solely one-sided operations. Hence, we effectively increase the performance in situations where the workload per process is unexpectedly unbalanced. Using a Word-Count implementation and a large dataset from the Purdue MapReduce Benchmarks Suite (PUMA), we demonstrate that our approach can provide up to 23% performance improvement on average compared to a reference MapReduce implementation that uses state-of-the-art MPI collective communication and I/O.
        △ Less
",work consid integr mpi one side commun non block hpc centric mapreduc framework use decoupl strategi aim overlap map reduc phase algorithm allow process commun synchron use sole one side oper henc effect increas perform situat workload per process unexpectedli unbalanc use word count implement larg dataset purdu mapreduc benchmark suit puma demonstr approach provid perform improv averag compar refer mapreduc implement use state art mpi collect commun less
5,1810.04144,"
        Advanced driver assistance systems are advancing at a rapid pace and all major companies started investing in developing the autonomous vehicles. But the security and reliability is still uncertain and debatable. Imagine that a vehicle is compromised by the attackers and then what they can do. An attacker can control brake, accelerate and even steering which can lead to catastrophic consequences. This paper gives a very short and brief overview of most of the possible attacks on autonomous vehicle software and hardware and their potential implications.
        △ Less
",advanc driver assist system advanc rapid pace major compani start invest develop autonom vehicl secur reliabl still uncertain debat imagin vehicl compromis attack attack control brake acceler even steer lead catastroph consequ paper give short brief overview possibl attack autonom vehicl softwar hardwar potenti implic less
6,1810.04142,"
        We address fine-grained multilingual language identification: providing a language code for every token in a sentence, including codemixed text containing multiple languages. Such text is prevalent online, in documents, social media, and message boards. We show that a feed-forward network with a simple globally constrained decoder can accurately and rapidly label both codemixed and monolingual text in 100 languages and 100 language pairs. This model outperforms previously published multilingual approaches in terms of both accuracy and speed, yielding an 800x speed-up and a 19.5% averaged absolute gain on three codemixed datasets. It furthermore outperforms several benchmark systems on monolingual language identification.
        △ Less
",address fine grain multilingu languag identif provid languag code everi token sentenc includ codemix text contain multipl languag text preval onlin document social media messag board show feed forward network simpl global constrain decod accur rapidli label codemix monolingu text languag languag pair model outperform previous publish multilingu approach term accuraci speed yield x speed averag absolut gain three codemix dataset furthermor outperform sever benchmark system monolingu languag identif less
7,1810.04133,"
        Significant advances have been made recently on training neural networks, where the main challenge is in solving an optimization problem with abundant critical points. However, existing approaches to address this issue crucially rely on a restrictive assumption: the training data is drawn from a Gaussian distribution. In this paper, we provide a novel unified framework to design loss functions with desirable landscape properties for a wide range of general input distributions. On these loss functions, remarkably, stochastic gradient descent theoretically recovers the true parameters with global initializations and empirically outperforms the existing approaches. Our loss function design bridges the notion of score functions with the topic of neural network optimization. Central to our approach is the task of estimating the score function from samples, which is of basic and independent interest to theoretical statistics. Traditional estimation methods (example: kernel based) fail right at the outset; we bring statistical methods of local likelihood to design a novel estimator of score functions, that provably adapts to the local geometry of the unknown density.
        △ Less
",signific advanc made recent train neural network main challeng solv optim problem abund critic point howev exist approach address issu crucial reli restrict assumpt train data drawn gaussian distribut paper provid novel unifi framework design loss function desir landscap properti wide rang gener input distribut loss function remark stochast gradient descent theoret recov true paramet global initi empir outperform exist approach loss function design bridg notion score function topic neural network optim central approach task estim score function sampl basic independ interest theoret statist tradit estim method exampl kernel base fail right outset bring statist method local likelihood design novel estim score function provabl adapt local geometri unknown densiti less
8,1810.04125,"
        We present new algorithms for the randomized construction of hierarchically semi-separable matrices, addressing several practical issues. The HSS construction algorithms use a partially matrix-free, adaptive randomized projection scheme to determine the maximum off-diagonal block rank. We develop both relative and absolute stopping criteria to determine the minimum dimension of the random projection matrix that is sufficient for the desired accuracy. Two strategies are discussed to adaptively enlarge the random sample matrix: repeated doubling of the number of random vectors, and iteratively incrementing the number of random vectors by a fixed number. The relative and absolute stopping criteria are based on probabilistic bounds for the Frobenius norm of the random projection of the Hankel blocks of the input matrix. We discuss parallel implementation and computation and communication cost of both variants. Parallel numerical results for a range of applications, including boundary element method matrices and quantum chemistry Toeplitz matrices, show the effectiveness, scalability and numerical robustness of the proposed algorithms.
        △ Less
",present new algorithm random construct hierarch semi separ matric address sever practic issu hss construct algorithm use partial matrix free adapt random project scheme determin maximum diagon block rank develop rel absolut stop criteria determin minimum dimens random project matrix suffici desir accuraci two strategi discuss adapt enlarg random sampl matrix repeat doubl number random vector iter increment number random vector fix number rel absolut stop criteria base probabilist bound frobeniu norm random project hankel block input matrix discuss parallel implement comput commun cost variant parallel numer result rang applic includ boundari element method matric quantum chemistri toeplitz matric show effect scalabl numer robust propos algorithm less
9,1810.04119,"
        Cartesian Genetic Programming (CGP) has many modifications across a variety of implementations, such as recursive connections and node weights. Alternative genetic operators have also been proposed for CGP, but have not been fully studied. In this work, we present a new form of genetic programming based on a floating point representation. In this new form of CGP, called Positional CGP, node positions are evolved. This allows for the evaluation of many different genetic operators while allowing for previous CGP improvements like recurrency. Using nine benchmark problems from three different classes, we evaluate the optimal parameters for CGP and PCGP, including novel genetic operators.
        △ Less
",cartesian genet program cgp mani modif across varieti implement recurs connect node weight altern genet oper also propos cgp fulli studi work present new form genet program base float point represent new form cgp call posit cgp node posit evolv allow evalu mani differ genet oper allow previou cgp improv like recurr use nine benchmark problem three differ class evalu optim paramet cgp pcgp includ novel genet oper less
10,1810.04118,"
        Smart services are an important element of the smart cities and the Internet of Things (IoT) ecosystems where the intelligence behind the services is obtained and improved through the sensory data. Providing a large amount of training data is not always feasible; therefore, we need to consider alternative ways that incorporate unlabeled data as well. In recent years, Deep reinforcement learning (DRL) has gained great success in several application domains. It is an applicable method for IoT and smart city scenarios where auto-generated data can be partially labeled by users' feedback for training purposes. In this paper, we propose a semi-supervised deep reinforcement learning model that fits smart city applications as it consumes both labeled and unlabeled data to improve the performance and accuracy of the learning agent. The model utilizes Variational Autoencoders (VAE) as the inference engine for generalizing optimal policies. To the best of our knowledge, the proposed model is the first investigation that extends deep reinforcement learning to the semi-supervised paradigm. As a case study of smart city applications, we focus on smart buildings and apply the proposed model to the problem of indoor localization based on BLE signal strength. Indoor localization is the main component of smart city services since people spend significant time in indoor environments. Our model learns the best action policies that lead to a close estimation of the target locations with an improvement of 23% in terms of distance to the target and at least 67% more received rewards compared to the supervised DRL model.
        △ Less
",smart servic import element smart citi internet thing iot ecosystem intellig behind servic obtain improv sensori data provid larg amount train data alway feasibl therefor need consid altern way incorpor unlabel data well recent year deep reinforc learn drl gain great success sever applic domain applic method iot smart citi scenario auto gener data partial label user feedback train purpos paper propos semi supervis deep reinforc learn model fit smart citi applic consum label unlabel data improv perform accuraci learn agent model util variat autoencod vae infer engin gener optim polici best knowledg propos model first investig extend deep reinforc learn semi supervis paradigm case studi smart citi applic focu smart build appli propos model problem indoor local base ble signal strength indoor local main compon smart citi servic sinc peopl spend signific time indoor environ model learn best action polici lead close estim target locat improv term distanc target least receiv reward compar supervis drl model less
11,1810.04114,"
        We propose a general-purpose approach to discovering active learning (AL) strategies from data. These strategies are transferable from one domain to another and can be used in conjunction with many machine learning models. To this end, we formalize the annotation process as a Markov decision process, design universal state and action spaces and introduce a new reward function that precisely model the AL objective of minimizing the annotation cost We seek to find an optimal (non-myopic) AL strategy using reinforcement learning. We evaluate the learned strategies on multiple unrelated domains and show that they consistently outperform state-of-the-art baselines.
        △ Less
",propos gener purpos approach discov activ learn al strategi data strategi transfer one domain anoth use conjunct mani machin learn model end formal annot process markov decis process design univers state action space introduc new reward function precis model al object minim annot cost seek find optim non myopic al strategi use reinforc learn evalu learn strategi multipl unrel domain show consist outperform state art baselin less
12,1810.04111,"
        News editors need to find the photos that best illustrate a news piece and fulfill news-media quality standards, while being pressed to also find the most recent photos of live events. Recently, it became common to use social-media content in the context of news media for its unique value in terms of immediacy and quality. Consequently, the amount of images to be considered and filtered through is now too much to be handled by a person. To aid the news editor in this process, we propose a framework designed to deliver high-quality, news-press type photos to the user. The framework, composed of two parts, is based on a ranking algorithm tuned to rank professional media highly and a visual SPAM detection module designed to filter-out low-quality media. The core ranking algorithm is leveraged by aesthetic, social and deep-learning semantic features. Evaluation showed that the proposed framework is effective at finding high-quality photos (true-positive rate) achieving a retrieval MAP of 64.5% and a classification precision of 70%.
        △ Less
",news editor need find photo best illustr news piec fulfil news media qualiti standard press also find recent photo live event recent becam common use social media content context news media uniqu valu term immediaci qualiti consequ amount imag consid filter much handl person aid news editor process propos framework design deliv high qualiti news press type photo user framework compos two part base rank algorithm tune rank profession media highli visual spam detect modul design filter low qualiti media core rank algorithm leverag aesthet social deep learn semant featur evalu show propos framework effect find high qualiti photo true posit rate achiev retriev map classif precis less
13,1810.04110,"
        Upcoming HPC clusters will feature hybrid memories and storage devices per compute node. In this work, we propose to use the MPI one-sided communication model and MPI windows as unique interface for programming memory and storage. We describe the design and implementation of MPI storage windows, and present its benefits for out-of-core execution, parallel I/O and fault-tolerance. In addition, we explore the integration of heterogeneous window allocations, where memory and storage share a unified virtual address space. When performing large, irregular memory operations, we verify that MPI windows on local storage incurs a 55% performance penalty on average. When using a Lustre parallel file system, asymmetric performance is observed with over 90% degradation in writing operations. Nonetheless, experimental results of a Distributed Hash Table, the HACC I/O kernel mini-application, and a novel MapReduce implementation based on the use of MPI one-sided communication, indicate that the overall penalty of MPI windows on storage can be negligible in most cases in real-world applications.
        △ Less
",upcom hpc cluster featur hybrid memori storag devic per comput node work propos use mpi one side commun model mpi window uniqu interfac program memori storag describ design implement mpi storag window present benefit core execut parallel fault toler addit explor integr heterogen window alloc memori storag share unifi virtual address space perform larg irregular memori oper verifi mpi window local storag incur perform penalti averag use lustr parallel file system asymmetr perform observ degrad write oper nonetheless experiment result distribut hash tabl hacc kernel mini applic novel mapreduc implement base use mpi one side commun indic overal penalti mpi window storag neglig case real world applic less
14,1810.04109,"
        Consider a wireless network where each communication link has a minimum bandwidth quality-of-service requirement. Certain pairs of wireless links interfere with each other due to being in the same vicinity, and this interference is modeled by a conflict graph. Given the conflict graph and link bandwidth requirements, the objective is to determine, using only localized information, whether the demands of all the links can be satisfied. At one extreme, each node knows the demands of only its neighbors; at the other extreme, there exists an optimal, centralized scheduler that has global information. The present work interpolates between these two extremes by quantitatively characterizing the tradeoff between the degree of decentralization and the performance of the distributed algorithm. This open problem is resolved for the primary interference model, and the following general result is obtained: if each node knows the demands of all links in a ball of radius $d$ centered at the node, then there is a distributed algorithm whose performance is away from that of an optimal, centralized algorithm by a factor of at most $(2d+3)/(2d+2)$. It is shown that for line networks under the protocol interference model, the row constraints are a factor of at most $3$ away from optimal. Both bounds are best possible.
        △ Less
",consid wireless network commun link minimum bandwidth qualiti servic requir certain pair wireless link interfer due vicin interfer model conflict graph given conflict graph link bandwidth requir object determin use local inform whether demand link satisfi one extrem node know demand neighbor extrem exist optim central schedul global inform present work interpol two extrem quantit character tradeoff degre decentr perform distribut algorithm open problem resolv primari interfer model follow gener result obtain node know demand link ball radiu center node distribut algorithm whose perform away optim central algorithm factor shown line network protocol interfer model row constraint factor away optim bound best possibl less
15,1810.04108,"
        Aerator plays an important role in the regulation of dissolved oxygen in aquaculture. The development of computer vision technology provides an opportunity for realizing intelligent monitoring of aerator. Surveillance cameras have been widely used in aquaculture. Therefore, it is of great application value to detect the working state of the aerator with the existing surveillance cameras. In this paper, a method of object region detection and working state detection for aerator is presented. In the object region detection module, this paper proposes a method to detect the candidate region and then determine the object region, which combines the background modeling, the optical flow method and the maximum inter-class interval method. In the work state detection module, this paper proposes a novel method called reference frame Kanade-Lucas-Tomasi (RF-KLT) algorithm, and constructs a classification procedure for the unlabeled time series data. The results of this study show that the accuracy of detecting object region and working state of aerator in the complex background is 100% and 99.9% respectively, and the detection speed is 77-333 frames per second (FPS) according to the different types of surveillance camera. Compared with various foreground detection algorithms and machine learning algorithms, these methods can realize on-line, real-time and high-accuracy detection of the object region and working state of aerator.
        △ Less
",aerat play import role regul dissolv oxygen aquacultur develop comput vision technolog provid opportun realiz intellig monitor aerat surveil camera wide use aquacultur therefor great applic valu detect work state aerat exist surveil camera paper method object region detect work state detect aerat present object region detect modul paper propos method detect candid region determin object region combin background model optic flow method maximum inter class interv method work state detect modul paper propos novel method call refer frame kanad luca tomasi rf klt algorithm construct classif procedur unlabel time seri data result studi show accuraci detect object region work state aerat complex background respect detect speed frame per second fp accord differ type surveil camera compar variou foreground detect algorithm machin learn algorithm method realiz line real time high accuraci detect object region work state aerat less
16,1810.04107,"
        The development of smart cities and their fast-paced deployment is resulting in the generation of large quantities of data at unprecedented rates. Unfortunately, most of the generated data is wasted without extracting potentially useful information and knowledge because of the lack of established mechanisms and standards that benefit from the availability of such data. Moreover, the high dynamical nature of smart cities calls for new generation of machine learning approaches that are flexible and adaptable to cope with the dynamicity of data to perform analytics and learn from real-time data. In this article, we shed the light on the challenge of under utilizing the big data generated by smart cities from a machine learning perspective. Especially, we present the phenomenon of wasting unlabeled data. We argue that semi-supervision is a must for smart city to address this challenge. We also propose a three-level learning framework for smart cities that matches the hierarchical nature of big data generated by smart cities with a goal of providing different levels of knowledge abstractions. The proposed framework is scalable to meet the needs of smart city services. Fundamentally, the framework benefits from semi-supervised deep reinforcement learning where a small amount of data that has users' feedback serves as labeled data while a larger amount is without such users' feedback serves as unlabeled data. This paper also explores how deep reinforcement learning and its shift toward semi-supervision can handle the cognitive side of smart city services and improve their performance by providing several use cases spanning the different domains of smart cities. We also highlight several challenges as well as promising future research directions for incorporating machine learning and high-level intelligence into smart city services.
        △ Less
",develop smart citi fast pace deploy result gener larg quantiti data unpreced rate unfortun gener data wast without extract potenti use inform knowledg lack establish mechan standard benefit avail data moreov high dynam natur smart citi call new gener machin learn approach flexibl adapt cope dynam data perform analyt learn real time data articl shed light challeng util big data gener smart citi machin learn perspect especi present phenomenon wast unlabel data argu semi supervis must smart citi address challeng also propos three level learn framework smart citi match hierarch natur big data gener smart citi goal provid differ level knowledg abstract propos framework scalabl meet need smart citi servic fundament framework benefit semi supervis deep reinforc learn small amount data user feedback serv label data larger amount without user feedback serv unlabel data paper also explor deep reinforc learn shift toward semi supervis handl cognit side smart citi servic improv perform provid sever use case span differ domain smart citi also highlight sever challeng well promis futur research direct incorpor machin learn high level intellig smart citi servic less
17,1810.04101,"
        Image captioning is an interdisciplinary research problem that stands between computer vision and natural language processing. The task is to generate a textual description of the content of an image. The typical model used for image captioning is an encoder-decoder deep network, where the encoder captures the essence of an image while the decoder is responsible for generating a sentence describing the image. Attention mechanisms can be used to automatically focus the decoder on parts of the image which are relevant to predict the next word. In this paper, we explore different decoders and attentional models popular in neural machine translation, namely attentional recurrent neural networks, self-attentional transformers, and fully-convolutional networks, which represent the current state of the art of neural machine translation. We made the image captioning module available in SOCKEYE at https://github.com/awslabs/sockeye/tree/master/sockeye.
        △ Less
",imag caption interdisciplinari research problem stand comput vision natur languag process task gener textual descript content imag typic model use imag caption encod decod deep network encod captur essenc imag decod respons gener sentenc describ imag attent mechan use automat focu decod part imag relev predict next word paper explor differ decod attent model popular neural machin translat name attent recurr neural network self attent transform fulli convolut network repres current state art neural machin translat made imag caption modul avail sockey http github com awslab sockey tree master sockey less
18,1810.04093,"
        Depth estimation from a single image represents a very exciting challenge in computer vision. While other image-based depth sensing techniques leverage on the geometry between different viewpoints (e.g., stereo or structure from motion), the lack of these cues within a single image renders ill-posed the monocular depth estimation task. For inference, state-of-the-art encoder-decoder architectures for monocular depth estimation rely on effective feature representations learned at training time. For unsupervised training of these models, geometry has been effectively exploited by suitable images warping losses computed from views acquired by a stereo rig or a moving camera. In this paper, we make a further step forward showing that learning semantic information from images enables to improve effectively monocular depth estimation as well. In particular, by leveraging on semantically labeled images together with unsupervised signals gained by geometry through an image warping loss, we propose a deep learning approach aimed at joint semantic segmentation and depth estimation. Our overall learning framework is semi-supervised, as we deploy groundtruth data only in the semantic domain. At training time, our network learns a common feature representation for both tasks and a novel cross-task loss function is proposed. The experimental findings show how, jointly tackling depth prediction and semantic segmentation, allows to improve depth estimation accuracy. In particular, on the KITTI dataset our network outperforms state-of-the-art methods for monocular depth estimation.
        △ Less
",depth estim singl imag repres excit challeng comput vision imag base depth sens techniqu leverag geometri differ viewpoint e g stereo structur motion lack cue within singl imag render ill pose monocular depth estim task infer state art encod decod architectur monocular depth estim reli effect featur represent learn train time unsupervis train model geometri effect exploit suitabl imag warp loss comput view acquir stereo rig move camera paper make step forward show learn semant inform imag enabl improv effect monocular depth estim well particular leverag semant label imag togeth unsupervis signal gain geometri imag warp loss propos deep learn approach aim joint semant segment depth estim overal learn framework semi supervis deploy groundtruth data semant domain train time network learn common featur represent task novel cross task loss function propos experiment find show jointli tackl depth predict semant segment allow improv depth estim accuraci particular kitti dataset network outperform state art method monocular depth estim less
19,1810.04088,"
        State of the art online learning procedures focus either on selecting the best alternative (""best arm identification"") or on minimizing the cost (the ""regret""). We merge these two objectives by providing the theoretical analysis of cost minimizing algorithms that are also delta-PAC (with a proven guaranteed bound on the decision time), hence fulfilling at the same time regret minimization and best arm identification. This analysis sheds light on the common observation that ill-callibrated UCB-algorithms minimize regret while still identifying quickly the best arm.
  We also extend these results to the non-iid case faced by many practitioners. This provides a technique to make cost versus decision time compromise when doing adaptive tests with applications ranging from website A/B testing to clinical trials.
        △ Less
",state art onlin learn procedur focu either select best altern best arm identif minim cost regret merg two object provid theoret analysi cost minim algorithm also delta pac proven guarante bound decis time henc fulfil time regret minim best arm identif analysi shed light common observ ill callibr ucb algorithm minim regret still identifi quickli best arm also extend result non iid case face mani practition provid techniqu make cost versu decis time compromis adapt test applic rang websit b test clinic trial less
20,1810.04080,"
        This article presents a multiple sound source localization and tracking system, fed by the Eigenmike array. The First Order Ambisonics (FOA) format is used to build a pseudointensity-based spherical histogram, from which the source position estimates are deduced. These instantaneous estimates are processed by a wellknown tracking system relying on a set of particle filters. While the novelty within localization and tracking is incremental, the fully-functional, complete and real-time running system based on these algorithms is proposed for the first time. As such, it could serve as an additional baseline method of the LOCATA challenge.
        △ Less
",articl present multipl sound sourc local track system fed eigenmik array first order ambison foa format use build pseudointens base spheric histogram sourc posit estim deduc instantan estim process wellknown track system reli set particl filter novelti within local track increment fulli function complet real time run system base algorithm propos first time could serv addit baselin method locata challeng less
21,1810.04077,"
        Point set registration is a powerful method that enables robots to manipulate deformable objects. By mapping the point cloud of the current object to the pre-trained point cloud, a transformation function can be constructed. The manipulator's trajectory for pre-trained shapes can be warped with this transformation function, yielding a feasible trajectory for the new shape. However, usually this transformation function regards objects as discrete points, and dismisses the topological structures. Therefore, it risks over-stretching or over-compression during manipulation. To tackle this problem, this paper proposes a tangent space point set registration method. A tangent space representation of an object is constructed by defining an angle for each node on the object. Point set registration algorithm runs in this newly-constructed tangent space, yielding a tangent space trajectory. The trajectory is then converted back to Cartesian space and carried out by the robot. Compared to its counterpart in Cartesian space, tangent space point set registration is safer and more robust, succeeding in a series of experiments such as rope straightening, rope knotting, cloth folding and unfolding.
        △ Less
",point set registr power method enabl robot manipul deform object map point cloud current object pre train point cloud transform function construct manipul trajectori pre train shape warp transform function yield feasibl trajectori new shape howev usual transform function regard object discret point dismiss topolog structur therefor risk stretch compress manipul tackl problem paper propos tangent space point set registr method tangent space represent object construct defin angl node object point set registr algorithm run newli construct tangent space yield tangent space trajectori trajectori convert back cartesian space carri robot compar counterpart cartesian space tangent space point set registr safer robust succeed seri experi rope straighten rope knot cloth fold unfold less
22,1810.04066,"
        We propose a novel deep learning paradigm of differential flows that learn a stochastic differential equation transformations of inputs prior to a standard classification or regression function. The key property of differential Gaussian processes is the warping of inputs through infinitely deep, but infinitesimal, differential fields, that generalise discrete layers into a dynamical system. We demonstrate state-of-the-art results that exceed the performance of deep Gaussian processes and neural networks
        △ Less
",propos novel deep learn paradigm differenti flow learn stochast differenti equat transform input prior standard classif regress function key properti differenti gaussian process warp input infinit deep infinitesim differenti field generalis discret layer dynam system demonstr state art result exceed perform deep gaussian process neural network less
23,1810.04064,"
        In recent years, pattern analysis plays an important role in data mining and recognition, and many variants have been proposed to handle complicated scenarios. In the literature, it has been quite familiar with high dimensionality of data samples, but either such characteristics or large data have become usual sense in real-world applications. In this work, an improved maximum margin criterion (MMC) method is introduced firstly. With the new definition of MMC, several variants of MMC, including random MMC, layered MMC, 2D^2 MMC, are designed to make adaptive learning applicable. Particularly, the MMC network is developed to learn deep features of images in light of simple deep networks. Experimental results on a diversity of data sets demonstrate the discriminant ability of proposed MMC methods are compenent to be adopted in complicated application scenarios.
        △ Less
",recent year pattern analysi play import role data mine recognit mani variant propos handl complic scenario literatur quit familiar high dimension data sampl either characterist larg data becom usual sens real world applic work improv maximum margin criterion mmc method introduc firstli new definit mmc sever variant mmc includ random mmc layer mmc mmc design make adapt learn applic particularli mmc network develop learn deep featur imag light simpl deep network experiment result divers data set demonstr discrimin abil propos mmc method compen adopt complic applic scenario less
24,1810.04063,"
        General Purpose Graphic Processing Unit(GPGPU) is used widely for achieving high performance or high throughput in parallel programming. This capability of GPGPUs is very famous in the new era and mostly used for scientific computing which requires more processing power than normal personal computers. Therefore, most of the programmers, researchers and industry use this new concept for their work. However, achieving high-performance or high-throughput using GPGPUs are not an easy task compared with conventional programming concepts in the CPU side. In this research, the CPU's cache memory optimization techniques have been adopted to the GPGPU's cache memory to identify rare performance improvement techniques compared to GPGPU's best practices. The cache optimization techniques of blocking, loop fusion, array merging and array transpose were tested on GPGPUs for finding suitability of these techniques. Finally, we identified that some of the CPU cache optimization techniques go well with the cache memory system of the GPGPU and shows performance improvements while some others show the opposite effect on the GPGPUs compared with the CPUs.
        △ Less
",gener purpos graphic process unit gpgpu use wide achiev high perform high throughput parallel program capabl gpgpu famou new era mostli use scientif comput requir process power normal person comput therefor programm research industri use new concept work howev achiev high perform high throughput use gpgpu easi task compar convent program concept cpu side research cpu cach memori optim techniqu adopt gpgpu cach memori identifi rare perform improv techniqu compar gpgpu best practic cach optim techniqu block loop fusion array merg array transpos test gpgpu find suitabl techniqu final identifi cpu cach optim techniqu go well cach memori system gpgpu show perform improv other show opposit effect gpgpu compar cpu less
25,1810.04058,"
        Rebalancing is a critical service bottleneck for many transportation services, such as Citi Bike. Citi Bike relies on manual orchestrations of rebalancing bikes between dispatchers and field agents. Motivated by such problem and the lack of smart autonomous solutions in this area, this project explored a new RL architecture called Distributed RL (DiRL) with Transfer Learning (TL) capability. The DiRL solution is adaptive to changing traffic dynamics when keeping bike stock under control at the minimum cost. DiRL achieved a 350% improvement in bike rebalancing autonomously and TL offered a 62.4% performance boost in managing an entire bike network. Lastly, a field trip to the dispatch office of Chariot, a ride-sharing service, provided insights to overcome challenges of deploying an RL solution in the real world.
        △ Less
",rebalanc critic servic bottleneck mani transport servic citi bike citi bike reli manual orchestr rebalanc bike dispatch field agent motiv problem lack smart autonom solut area project explor new rl architectur call distribut rl dirl transfer learn tl capabl dirl solut adapt chang traffic dynam keep bike stock control minimum cost dirl achiev improv bike rebalanc autonom tl offer perform boost manag entir bike network lastli field trip dispatch offic chariot ride share servic provid insight overcom challeng deploy rl solut real world less
26,1810.04053,"
        In the last couple of years, the rise of Artificial Intelligence and the successes of academic breakthroughs in the field have been inescapable. Vast sums of money have been thrown at AI start-ups. Many existing tech companies -- including the giants like Google, Amazon, Facebook, and Microsoft -- have opened new research labs. The rapid changes in these everyday work and entertainment tools have fueled a rising interest in the underlying technology itself; journalists write about AI tirelessly, and companies -- of tech nature or not -- brand themselves with AI, Machine Learning or Deep Learning whenever they get a chance. Confronting squarely this media coverage, several analysts are starting to voice concerns about over-interpretation of AI's blazing successes and the sometimes poor public reporting on the topic. This paper reviews briefly the track-record in AI and Machine Learning and finds this pattern of early dramatic successes, followed by philosophical critique and unexpected difficulties, if not downright stagnation, returning almost to the clock in 30-year cycles since 1958.
        △ Less
",last coupl year rise artifici intellig success academ breakthrough field inescap vast sum money thrown ai start up mani exist tech compani includ giant like googl amazon facebook microsoft open new research lab rapid chang everyday work entertain tool fuel rise interest underli technolog journalist write ai tirelessli compani tech natur brand ai machin learn deep learn whenev get chanc confront squar media coverag sever analyst start voic concern interpret ai blaze success sometim poor public report topic paper review briefli track record ai machin learn find pattern earli dramat success follow philosoph critiqu unexpect difficulti downright stagnat return almost clock year cycl sinc less
27,1810.04047,"
        Models optimized for accuracy on single images are often prohibitively slow to run on each frame in a video. Recent work exploits the use of optical flow to warp image features forward from select keyframes, as a means to conserve computation on video. This approach, however, achieves only limited speedup, even when optimized, due to the accuracy degradation introduced by repeated forward warping, and the inference cost of optical flow estimation. To address these problems, we propose a new scheme that propagates features using the block motion vectors (BMV) present in compressed video (e.g. H.264 codecs), instead of optical flow, and bi-directionally warps and fuses features from enclosing keyframes to capture scene context on each video frame. Our technique, interpolation-BMV, enables us to accurately estimate the features of intermediate frames, while keeping inference costs low. We evaluate our system on the CamVid and Cityscapes datasets, comparing to both a strong single-frame baseline and related work. We find that we are able to substantially accelerate segmentation on video, achieving near real-time frame rates (20+ frames per second) on large images (e.g. 960 x 720 pixels), while maintaining competitive accuracy. This represents an improvement of almost 6x over the single-frame baseline and 2.5x over the fastest prior work.
        △ Less
",model optim accuraci singl imag often prohibit slow run frame video recent work exploit use optic flow warp imag featur forward select keyfram mean conserv comput video approach howev achiev limit speedup even optim due accuraci degrad introduc repeat forward warp infer cost optic flow estim address problem propos new scheme propag featur use block motion vector bmv present compress video e g h codec instead optic flow bi direct warp fuse featur enclos keyfram captur scene context video frame techniqu interpol bmv enabl us accur estim featur intermedi frame keep infer cost low evalu system camvid cityscap dataset compar strong singl frame baselin relat work find abl substanti acceler segment video achiev near real time frame rate frame per second larg imag e g x pixel maintain competit accuraci repres improv almost x singl frame baselin x fastest prior work less
28,1810.04041,"
        Capsule Networks (CapsNet) are recently proposed multi-stage computational models specialized for entity representation and discovery in image data. CapsNet employs iterative routing that shapes how the information cascades through different levels of interpretations. In this work, we investigate i) how the routing affects the CapsNet model fitting, ii) how the representation by capsules helps discover global structures in data distribution and iii) how learned data representation adapts and generalizes to new tasks. Our investigation shows: i) routing operation determines the certainty with which one layer of capsules pass information to the layer above, and the appropriate level of certainty is related to the model fitness, ii) in a designed experiment using data with a known 2D structure, capsule representations allow more meaningful 2D manifold embedding than neurons in a standard CNN do and iii) compared to neurons of standard CNN, capsules of successive layers are less coupled and more adaptive to new data distribution.
        △ Less
",capsul network capsnet recent propos multi stage comput model special entiti represent discoveri imag data capsnet employ iter rout shape inform cascad differ level interpret work investig rout affect capsnet model fit ii represent capsul help discov global structur data distribut iii learn data represent adapt gener new task investig show rout oper determin certainti one layer capsul pass inform layer appropri level certainti relat model fit ii design experi use data known structur capsul represent allow meaning manifold embed neuron standard cnn iii compar neuron standard cnn capsul success layer less coupl adapt new data distribut less
29,1810.04040,"
        Person-Job Fit is the process of matching the right talent for the right job by identifying talent competencies that are required for the job. While many qualitative efforts have been made in related fields, it still lacks of quantitative ways of measuring talent competencies as well as the job's talent requirements. To this end, in this paper, we propose a novel end-to-end data-driven model based on Convolutional Neural Network (CNN), namely Person-Job Fit Neural Network (PJFNN), for matching a talent qualification to the requirements of a job. To be specific, PJFNN is a bipartite neural network which can effectively learn the joint representation of Person-Job fitness from historical job applications. In particular, due to the design of a hierarchical representation structure, PJFNN can not only estimate whether a candidate fits a job, but also identify which specific requirement items in the job posting are satisfied by the candidate by measuring the distances between corresponding latent representations. Finally, the extensive experiments on a large-scale real-world dataset clearly validate the performance of PJFNN in terms of Person-Job Fit prediction. Also, we provide effective data visualization to show some job and talent benchmark insights obtained by PJFNN.
        △ Less
",person job fit process match right talent right job identifi talent compet requir job mani qualit effort made relat field still lack quantit way measur talent compet well job talent requir end paper propos novel end end data driven model base convolut neural network cnn name person job fit neural network pjfnn match talent qualif requir job specif pjfnn bipartit neural network effect learn joint represent person job fit histor job applic particular due design hierarch represent structur pjfnn estim whether candid fit job also identifi specif requir item job post satisfi candid measur distanc correspond latent represent final extens experi larg scale real world dataset clearli valid perform pjfnn term person job fit predict also provid effect data visual show job talent benchmark insight obtain pjfnn less
30,1810.04039,"
        Detection of interacting and conversational groups from images has applications in video surveillance and social robotics. In this paper we build on prior attempts to find conversational groups by detection of social gathering spaces called o-spaces used to assign people to groups. As our contributions to the task, we are the first paper to incorporate features extracted from the room layout image, and the first to incorporate a deep network to generate an image representation of the proposed o-spaces. Specifically, this novel network builds on the PointNet architecture which allows unordered inputs of variable sizes. We present accuracies which demonstrate the ability to rival and sometimes outperform the best models, but due to a data imbalance issue we do not yet outperform existing models in our test results.
        △ Less
",detect interact convers group imag applic video surveil social robot paper build prior attempt find convers group detect social gather space call space use assign peopl group contribut task first paper incorpor featur extract room layout imag first incorpor deep network gener imag represent propos space specif novel network build pointnet architectur allow unord input variabl size present accuraci demonstr abil rival sometim outperform best model due data imbal issu yet outperform exist model test result less
31,1810.04038,"
        Deep neural networks, including recurrent networks, have been successfully applied to human activity recognition. Unfortunately, the final representation learned by recurrent networks might encode some noise (irrelevant signal components, unimportant sensor modalities, etc.). Besides, it is difficult to interpret the recurrent networks to gain insight into the models' behavior. To address these issues, we propose two attention models for human activity recognition: temporal attention and sensor attention. These two mechanisms adaptively focus on important signals and sensor modalities. To further improve the understandability and mean F1 score, we add continuity constraints, considering that continuous sensor signals are more robust than discrete ones. We evaluate the approaches on three datasets and obtain state-of-the-art results. Furthermore, qualitative analysis shows that the attention learned by the models agree well with human intuition.
        △ Less
",deep neural network includ recurr network success appli human activ recognit unfortun final represent learn recurr network might encod nois irrelev signal compon unimport sensor modal etc besid difficult interpret recurr network gain insight model behavior address issu propos two attent model human activ recognit tempor attent sensor attent two mechan adapt focu import signal sensor modal improv understand mean f score add continu constraint consid continu sensor signal robust discret one evalu approach three dataset obtain state art result furthermor qualit analysi show attent learn model agre well human intuit less
32,1810.04033,"
        Simple stencil codes are and remain an important building block in scientific computing. On shared memory nodes, they are traditionally parallelised through colouring or (recursive) tiling. New OpenMP versions alternatively allow users to specify data dependencies explicitly and to outsource the decision how to distribute the work to the runtime system. We evaluate traditional multithreading strategies on both Broadwell and KNL, study the arising assignment of tasks to threads and, from there, derive two efficient ways to parallelise stencil codes on regular Cartesian grids that fuse colouring and task-based approaches.
        △ Less
",simpl stencil code remain import build block scientif comput share memori node tradit parallelis colour recurs tile new openmp version altern allow user specifi data depend explicitli outsourc decis distribut work runtim system evalu tradit multithread strategi broadwel knl studi aris assign task thread deriv two effici way parallelis stencil code regular cartesian grid fuse colour task base approach less
33,1810.04029,"
        This paper proposes an algorithm for recognizing slab identification numbers in factory scenes. In the development a deep-learning based system, manual labeling for preparing ground truth data (GTD) is an important but expensive task. Furthermore, the quality of GTD is closely related to the performance of a supervised learning algorithm. To reduce manual work in labeling process, we generated weakly annotated GTD by marking only character centroids. Whereas conventional GTD for scene text recognition, bounding-boxes, require at least a drag-and-drop operation or two clicks to annotate a character location, the weakly annotated GTD requires a single click to record a character location. The main contribution of this paper is on selective distillation to improve the quality of the weakly annotated GTD. Because manual GTD are usually generated by many people, it may contain personal bias or human error. To address this problem, the information in manual GTD is integrated and refined by selective distillation. In the process of selective distillation, a fully convolutional network (FCN) is trained using the weakly annotated GTD, and its prediction maps are selectively used to revise locations and boundaries of semantic regions of characters in the initial GTD. The modified GTD are used in main training stage, and a post-processing is conducted to retrieve text information. Experiments were thoroughly conducted on actual industry data collected at a steelworks to demonstrate the effectiveness of the proposed method.
        △ Less
",paper propos algorithm recogn slab identif number factori scene develop deep learn base system manual label prepar ground truth data gtd import expens task furthermor qualiti gtd close relat perform supervis learn algorithm reduc manual work label process gener weakli annot gtd mark charact centroid wherea convent gtd scene text recognit bound box requir least drag drop oper two click annot charact locat weakli annot gtd requir singl click record charact locat main contribut paper select distil improv qualiti weakli annot gtd manual gtd usual gener mani peopl may contain person bia human error address problem inform manual gtd integr refin select distil process select distil fulli convolut network fcn train use weakli annot gtd predict map select use revis locat boundari semant region charact initi gtd modifi gtd use main train stage post process conduct retriev text inform experi thoroughli conduct actual industri data collect steelwork demonstr effect propos method less
34,1810.04028,"
        In most convolution neural networks (CNNs), downsampling hidden layers is adopted for increasing computation efficiency and the receptive field size. Such operation is commonly so-called pooling. Maximation and averaging over sliding windows (max/average pooling), and plain downsampling in the form of strided convolution are popular pooling methods. Since the pooling is a lossy procedure, a motivation of our work is to design a new pooling approach for less lossy in the dimensionality reduction. Inspired by the Fourier spectral pooling(FSP) proposed by Rippel et. al. [1], we present the Hartley transform based spectral pooling method in CNNs. Compared with FSP, the proposed spectral pooling avoids the use of complex arithmetic for frequency representation and reduces the computation. Spectral pooling preserves more structure features for network's discriminability than max and average pooling. We empirically show that Hartley spectral pooling gives rise to the convergence of training CNNs on MNIST and CIFAR-10 datasets.
        △ Less
",convolut neural network cnn downsampl hidden layer adopt increas comput effici recept field size oper commonli call pool maxim averag slide window max averag pool plain downsampl form stride convolut popular pool method sinc pool lossi procedur motiv work design new pool approach less lossi dimension reduct inspir fourier spectral pool fsp propos rippel et al present hartley transform base spectral pool method cnn compar fsp propos spectral pool avoid use complex arithmet frequenc represent reduc comput spectral pool preserv structur featur network discrimin max averag pool empir show hartley spectral pool give rise converg train cnn mnist cifar dataset less
35,1810.04021,"
        In this paper, we propose a novel deep learning framework for anatomy segmentation and automatic landmark- ing. Specifically, we focus on the challenging problem of mandible segmentation from cone-beam computed tomography (CBCT) scans and identification of 9 anatomical landmarks of the mandible on the geodesic space. The overall approach employs three inter-related steps. In step 1, we propose a deep neu- ral network architecture with carefully designed regularization, and network hyper-parameters to perform image segmentation without the need for data augmentation and complex post- processing refinement. In step 2, we formulate the landmark localization problem directly on the geodesic space for sparsely- spaced anatomical landmarks. In step 3, we propose to use a long short-term memory (LSTM) network to identify closely- spaced landmarks, which is rather difficult to obtain using other standard detection networks. The proposed fully automated method showed superior efficacy compared to the state-of-the- art mandible segmentation and landmarking approaches in craniofacial anomalies and diseased states. We used a very challenging CBCT dataset of 50 patients with a high-degree of craniomaxillofacial (CMF) variability that is realistic in clinical practice. Complementary to the quantitative analysis, the qualitative visual inspection was conducted for distinct CBCT scans from 250 patients with high anatomical variability. We have also shown feasibility of the proposed work in an independent dataset from MICCAI Head-Neck Challenge (2015) achieving the state-of-the-art performance. Lastly, we present an in-depth analysis of the proposed deep networks with respect to the choice of hyper-parameters such as pooling and activation functions.
        △ Less
",paper propos novel deep learn framework anatomi segment automat landmark ing specif focu challeng problem mandibl segment cone beam comput tomographi cbct scan identif anatom landmark mandibl geodes space overal approach employ three inter relat step step propos deep neu ral network architectur care design regular network hyper paramet perform imag segment without need data augment complex post process refin step formul landmark local problem directli geodes space spars space anatom landmark step propos use long short term memori lstm network identifi close space landmark rather difficult obtain use standard detect network propos fulli autom method show superior efficaci compar state art mandibl segment landmark approach craniofaci anomali diseas state use challeng cbct dataset patient high degre craniomaxillofaci cmf variabl realist clinic practic complementari quantit analysi qualit visual inspect conduct distinct cbct scan patient high anatom variabl also shown feasibl propos work independ dataset miccai head neck challeng achiev state art perform lastli present depth analysi propos deep network respect choic hyper paramet pool activ function less
36,1810.04020,"
        Generating a description of an image is called image captioning. Image captioning requires to recognize the important objects, their attributes and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey paper, we aim to present a comprehensive review of existing deep learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep learning based automatic image captioning.
        △ Less
",gener descript imag call imag caption imag caption requir recogn import object attribut relationship imag also need gener syntact semant correct sentenc deep learn base techniqu capabl handl complex challeng imag caption survey paper aim present comprehens review exist deep learn base imag caption techniqu discuss foundat techniqu analyz perform strength limit also discuss dataset evalu metric popularli use deep learn base automat imag caption less
37,1810.04017,"
        Various approaches for liver segmentation in CT have been proposed: Besides statistical shape models, which played a major role in this research area, novel approaches on the basis of convolutional neural networks have been introduced recently. Using a set of 219 liver CT datasets with reference segmentations from liver surgery planning, we evaluate the performance of several neural network classifiers based on 2D and 3D U-net architectures. An interesting observation is that slice-wise approaches perform surprisingly well, with mean and median Dice coefficients above 0.97, and may be preferable over 3D approaches given current hardware and software limitations.
        △ Less
",variou approach liver segment ct propos besid statist shape model play major role research area novel approach basi convolut neural network introduc recent use set liver ct dataset refer segment liver surgeri plan evalu perform sever neural network classifi base u net architectur interest observ slice wise approach perform surprisingli well mean median dice coeffici may prefer approach given current hardwar softwar limit less
38,1810.04016,"
        We provide a framework for the assignment of multiple robots to goal locations, when robot travel times are uncertain. Our premise is that time is the most valuable asset in the system. Hence, we make use of redundant robots to counter the effect of uncertainty and minimize the average waiting time at destinations. We apply our framework to transport networks represented as graphs, and consider uncertainty in the edge costs (i.e., travel time). Since solving the redundant assignment problem is strongly NP-hard, we exploit structural properties of our problem to propose a polynomial-time solution with provable sub-optimality bounds. Our method uses distributive aggregate functions, which allow us to efficiently (i.e., incrementally) compute the effective cost of assigning redundant robots. Experimental results on random graphs show that the deployment of redundant robots through our method reduces waiting times at goal locations, when edge traversals are uncertain.
        △ Less
",provid framework assign multipl robot goal locat robot travel time uncertain premis time valuabl asset system henc make use redund robot counter effect uncertainti minim averag wait time destin appli framework transport network repres graph consid uncertainti edg cost e travel time sinc solv redund assign problem strongli np hard exploit structur properti problem propos polynomi time solut provabl sub optim bound method use distribut aggreg function allow us effici e increment comput effect cost assign redund robot experiment result random graph show deploy redund robot method reduc wait time goal locat edg travers uncertain less
39,1810.04012,"
        Enhancing visual qualities of images plays very important roles in various vision and learning applications. In the past few years, both knowledge-driven maximum a posterior (MAP) with prior modelings and fully data-dependent convolutional neural network (CNN) techniques have been investigated to address specific enhancement tasks. In this paper, by exploiting the advantages of these two types of mechanisms within a complementary propagation perspective, we propose a unified framework, named deep prior ensemble (DPE), for solving various image enhancement tasks. Specifically, we first establish the basic propagation scheme based on the fundamental image modeling cues and then introduce residual CNNs to help predicting the propagation direction at each stage. By designing prior projections to perform feedback control, we theoretically prove that even with experience-inspired CNNs, DPE is definitely converged and the output will always satisfy our fundamental task constraints. The main advantage against conventional optimization-based MAP approaches is that our descent directions are learned from collected training data, thus are much more robust to unwanted local minimums. While, compared with existing CNN type networks, which are often designed in heuristic manners without theoretical guarantees, DPE is able to gain advantages from rich task cues investigated on the bases of domain knowledges. Therefore, DPE actually provides a generic ensemble methodology to integrate both knowledge and data-based cues for different image enhancement tasks. More importantly, our theoretical investigations verify that the feedforward propagations of DPE are properly controlled toward our desired solution. Experimental results demonstrate that the proposed DPE outperforms state-of-the-arts on a variety of image enhancement tasks in terms of both quantitative measure and visual perception quality.
        △ Less
",enhanc visual qualiti imag play import role variou vision learn applic past year knowledg driven maximum posterior map prior model fulli data depend convolut neural network cnn techniqu investig address specif enhanc task paper exploit advantag two type mechan within complementari propag perspect propos unifi framework name deep prior ensembl dpe solv variou imag enhanc task specif first establish basic propag scheme base fundament imag model cue introduc residu cnn help predict propag direct stage design prior project perform feedback control theoret prove even experi inspir cnn dpe definit converg output alway satisfi fundament task constraint main advantag convent optim base map approach descent direct learn collect train data thu much robust unwant local minimum compar exist cnn type network often design heurist manner without theoret guarante dpe abl gain advantag rich task cue investig base domain knowledg therefor dpe actual provid gener ensembl methodolog integr knowledg data base cue differ imag enhanc task importantli theoret investig verifi feedforward propag dpe properli control toward desir solut experiment result demonstr propos dpe outperform state art varieti imag enhanc task term quantit measur visual percept qualiti less
40,1810.04008,"
        MRI analysis takes central position in brain tumor diagnosis and treatment, thus it's precise evaluation is crucially important. However, it's 3D nature imposes several challenges, so the analysis is often performed on 2D projections that reduces the complexity, but increases bias. On the other hand, time consuming 3D evaluation, like, segmentation, is able to provide precise estimation of a number of valuable spatial characteristics, giving us understanding about the course of the disease.\newline Recent studies, focusing on the segmentation task, report superior performance of Deep Learning methods compared to classical computer vision algorithms. But still, it remains a challenging problem. In this paper we present deep cascaded approach for automatic brain tumor segmentation. Similar to recent methods for object detection, our implementation is based on neural networks; we propose modifications to the 3D UNet architecture and augmentation strategy to efficiently handle multimodal MRI input, besides this we introduce approach to enhance segmentation quality with context obtained from models of the same topology operating on downscaled data. We evaluate presented approach on BraTS 2018 dataset and discuss results.
        △ Less
",mri analysi take central posit brain tumor diagnosi treatment thu precis evalu crucial import howev natur impos sever challeng analysi often perform project reduc complex increas bia hand time consum evalu like segment abl provid precis estim number valuabl spatial characterist give us understand cours diseas newlin recent studi focus segment task report superior perform deep learn method compar classic comput vision algorithm still remain challeng problem paper present deep cascad approach automat brain tumor segment similar recent method object detect implement base neural network propos modif unet architectur augment strategi effici handl multimod mri input besid introduc approach enhanc segment qualiti context obtain model topolog oper downscal data evalu present approach brat dataset discuss result less
41,1810.04006,"
        In this article, we study the problem of enumerating the models of DNF formulas. The aim is to provide enumeration algorithms with a delay that depends polynomially on the size of each model and not on the size of the formula. We succeed for two subclasses of DNF formulas: we provide a constant delay algorithm for $k$-DNF with fixed $k$ by an appropriate amortization method and we give a polynomial delay algorithm for monotone formulas. We then focus on the average delay of enumeration algorithms and show that we can bring down the dependency of the average delay to the square root of the formula size and even to a logarithmic dependency for monotone formulas.
        △ Less
",articl studi problem enumer model dnf formula aim provid enumer algorithm delay depend polynomi size model size formula succeed two subclass dnf formula provid constant delay algorithm k dnf fix k appropri amort method give polynomi delay algorithm monoton formula focu averag delay enumer algorithm show bring depend averag delay squar root formula size even logarithm depend monoton formula less
42,1810.04002,"
        In this paper, we analyze failure cases of state-of-the-art detectors and observe that most hard false positives result from classification instead of localization and they have a large negative impact on the performance of object detectors. We conjecture there are three factors: (1) Shared feature representation is not optimal due to the mismatched goals of feature learning for classification and localization; (2) multi-task learning helps, yet optimization of the multi-task loss may result in sub-optimal for individual tasks; (3) large receptive field for different scales leads to redundant context information for small objects. We demonstrate the potential of detector classification power by a simple, effective, and widely-applicable Decoupled Classification Refinement (DCR) network. In particular, DCR places a separate classification network in parallel with the localization network (base detector). With ROI Pooling placed on the early stage of the classification network, we enforce an adaptive receptive field in DCR. During training, DCR samples hard false positives from the base detector and trains a strong classifier to refine classification results. During testing, DCR refines all boxes from the base detector. Experiments show competitive results on PASCAL VOC and COCO without any bells and whistles. Our codes are available at: https://github.com/bowenc0221/Decoupled-Classification-Refinement.
        △ Less
",paper analyz failur case state art detector observ hard fals posit result classif instead local larg neg impact perform object detector conjectur three factor share featur represent optim due mismatch goal featur learn classif local multi task learn help yet optim multi task loss may result sub optim individu task larg recept field differ scale lead redund context inform small object demonstr potenti detector classif power simpl effect wide applic decoupl classif refin dcr network particular dcr place separ classif network parallel local network base detector roi pool place earli stage classif network enforc adapt recept field dcr train dcr sampl hard fals posit base detector train strong classifi refin classif result test dcr refin box base detector experi show competit result pascal voc coco without bell whistl code avail http github com bowenc decoupl classif refin less
43,1810.04000,"
        With the rapid development of knowledge base,question answering based on knowledge base has been a hot research issue. In this paper, we focus on answering singlerelation factoid questions based on knowledge base. We build a question answering system and study the effect of context information on fact selection, such as entity's notable type,outdegree. Experimental results show that context information can improve the result of simple question answering.
        △ Less
",rapid develop knowledg base question answer base knowledg base hot research issu paper focu answer singlerel factoid question base knowledg base build question answer system studi effect context inform fact select entiti notabl type outdegre experiment result show context inform improv result simpl question answer less
44,1810.03999,"
        Dose reduction in computed tomography (CT) has been of great research interest for decades with the endeavor to reduce the health risk related to radiation. Promising results have been achieved by the recent application of deep learning to image reconstruction algorithms. Unrolled neural networks have reached state-of-the-art performance by learning the image reconstruction algorithm end-to-end. However, it suffers from huge memory consumption and long training time, which made it hard to scale to 3D data with current hardware. In this paper, we proposed an unrolled neural network for image reconstruction which can be trained step-by-step instead of end-to-end. Multiple cascades of image domain network were trained sequentially and connected with iterations which enforced data fidelity. Local image patches could be utilized for the neural network training, which made it fully scalable to 3D CT data. The proposed method was validated with both simulated and real data and demonstrated competing performance against the end-to-end networks.
        △ Less
",dose reduct comput tomographi ct great research interest decad endeavor reduc health risk relat radiat promis result achiev recent applic deep learn imag reconstruct algorithm unrol neural network reach state art perform learn imag reconstruct algorithm end end howev suffer huge memori consumpt long train time made hard scale data current hardwar paper propos unrol neural network imag reconstruct train step step instead end end multipl cascad imag domain network train sequenti connect iter enforc data fidel local imag patch could util neural network train made fulli scalabl ct data propos method valid simul real data demonstr compet perform end end network less
45,1810.03996,"
        Morphological declension, which aims to inflect nouns to indicate number, case and gender, is an important task in natural language processing (NLP). This research proposal seeks to address the degree to which Recurrent Neural Networks (RNNs) are efficient in learning to decline noun cases. Given the challenge of data sparsity in processing morphologically rich languages and also, the flexibility of sentence structures in such languages, we believe that modeling morphological dependencies can improve the performance of neural network models. It is suggested to carry out various experiments to understand the interpretable features that may lead to a better generalization of the learned models on cross-lingual tasks.
        △ Less
",morpholog declens aim inflect noun indic number case gender import task natur languag process nlp research propos seek address degre recurr neural network rnn effici learn declin noun case given challeng data sparsiti process morpholog rich languag also flexibl sentenc structur languag believ model morpholog depend improv perform neural network model suggest carri variou experi understand interpret featur may lead better gener learn model cross lingual task less
46,1810.03993,"
        Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related AI technology, increasing transparency into how well AI technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.
        △ Less
",train machin learn model increasingli use perform high impact task area law enforc medicin educ employ order clarifi intend use case machin learn model minim usag context well suit recommend releas model accompani document detail perform characterist paper propos framework call model card encourag transpar model report model card short document accompani train machin learn model provid benchmark evalu varieti condit across differ cultur demograph phenotyp group e g race geograph locat sex fitzpatrick skin type intersect group e g age race sex fitzpatrick skin type relev intend applic domain model card also disclos context model intend use detail perform evalu procedur relev inform focu primarili human center machin learn model applic field comput vision natur languag process framework use document train machin learn model solidifi concept provid card two supervis model one train detect smile face imag one train detect toxic comment text propos model card step toward respons democrat machin learn relat ai technolog increas transpar well ai technolog work hope work encourag releas train machin learn model accompani model releas similar detail evalu number relev document less
47,1810.03990,"
        Nonlinear electromagnetic (EM) inverse scattering is a quantitative and super-resolution imaging technique, in which more realistic interactions between the internal structure of scene and EM wavefield are taken into account in the imaging procedure, in contrast to conventional tomography. However, it poses important challenges arising from its intrinsic strong nonlinearity, ill-posedness, and expensive computation costs. To tackle these difficulties, we, for the first time to our best knowledge, exploit a connection between the deep neural network (DNN) architecture and the iterative method of nonlinear EM inverse scattering. This enables the development of a novel DNN-based methodology for nonlinear EM inverse problems (termed here DeepNIS). The proposed DeepNIS consists of a cascade of multi-layer complexvalued residual convolutional neural network (CNN) modules. We numerically and experimentally demonstrate that the DeepNIS outperforms remarkably conventional nonlinear inverse scattering methods in terms of both the image quality and computational time. We show that DeepNIS can learn a general model approximating the underlying EM inverse scattering system. It is expected that the DeepNIS will serve as powerful tool in treating highly nonlinear EM inverse scattering problems over different frequency bands, involving large-scale and high-contrast objects, which are extremely hard and impractical to solve using conventional inverse scattering methods.
        △ Less
",nonlinear electromagnet em invers scatter quantit super resolut imag techniqu realist interact intern structur scene em wavefield taken account imag procedur contrast convent tomographi howev pose import challeng aris intrins strong nonlinear ill posed expens comput cost tackl difficulti first time best knowledg exploit connect deep neural network dnn architectur iter method nonlinear em invers scatter enabl develop novel dnn base methodolog nonlinear em invers problem term deepni propos deepni consist cascad multi layer complexvalu residu convolut neural network cnn modul numer experiment demonstr deepni outperform remark convent nonlinear invers scatter method term imag qualiti comput time show deepni learn gener model approxim underli em invers scatter system expect deepni serv power tool treat highli nonlinear em invers scatter problem differ frequenc band involv larg scale high contrast object extrem hard impract solv use convent invers scatter method less
48,1810.03989,"
        Image-to-video person re-identification identifies a target person by a probe image from quantities of pedestrian videos captured by non-overlapping cameras. Despite the great progress achieved,it's still challenging to match in the multimodal scenario,i.e. between image and video. Currently,state-of-the-art approaches mainly focus on the task-specific data,neglecting the extra information on the different but related tasks. In this paper,we propose an end-to-end neural network framework for image-to-video person reidentification by leveraging cross-modal embeddings learned from extra information.Concretely speaking,cross-modal embeddings from image captioning and video captioning models are reused to help learned features be projected into a coordinated space,where similarity can be directly computed. Besides,training steps from fixed model reuse approach are integrated into our framework,which can incorporate beneficial information and eventually make the target networks independent of existing models. Apart from that,our proposed framework resorts to CNNs and LSTMs for extracting visual and spatiotemporal features,and combines the strengths of identification and verification model to improve the discriminative ability of the learned feature. The experimental results demonstrate the effectiveness of our framework on narrowing down the gap between heterogeneous data and obtaining observable improvement in image-to-video person re-identification.
        △ Less
",imag video person identif identifi target person probe imag quantiti pedestrian video captur non overlap camera despit great progress achiev still challeng match multimod scenario e imag video current state art approach mainli focu task specif data neglect extra inform differ relat task paper propos end end neural network framework imag video person reidentif leverag cross modal embed learn extra inform concret speak cross modal embed imag caption video caption model reus help learn featur project coordin space similar directli comput besid train step fix model reus approach integr framework incorpor benefici inform eventu make target network independ exist model apart propos framework resort cnn lstm extract visual spatiotempor featur combin strength identif verif model improv discrimin abil learn featur experiment result demonstr effect framework narrow gap heterogen data obtain observ improv imag video person identif less
49,1810.03988,"
        Panoramic video is a sort of video recorded at the same point of view to record the full scene. With the development of video surveillance and the requirement for 3D converged video surveillance in smart cities, CPU and GPU are required to possess strong processing abilities to make panoramic video. The traditional panoramic products depend on post processing, which results in high power consumption, low stability and unsatisfying performance in real time. In order to solve these problems,we propose a real-time panoramic video stitching framework.The framework we propose mainly consists of three algorithms, LORB image feature extraction algorithm, feature point matching algorithm based on LSH and GPU parallel video stitching algorithm based on CUDA.The experiment results show that the algorithm mentioned can improve the performance in the stages of feature extraction of images stitching and matching, the running speed of which is 11 times than that of the traditional ORB algorithm and 639 times than that of the traditional SIFT algorithm. Based on analyzing the GPU resources occupancy rate of each resolution image stitching, we further propose a stream parallel strategy to maximize the utilization of GPU resources. Compared with the L-ORB algorithm, the efficiency of this strategy is improved by 1.6-2.5 times, and it can make full use of GPU resources. The performance of the system accomplished in the paper is 29.2 times than that of the former embedded one, while the power dissipation is reduced to 10W.
        △ Less
",panoram video sort video record point view record full scene develop video surveil requir converg video surveil smart citi cpu gpu requir possess strong process abil make panoram video tradit panoram product depend post process result high power consumpt low stabil unsatisfi perform real time order solv problem propos real time panoram video stitch framework framework propos mainli consist three algorithm lorb imag featur extract algorithm featur point match algorithm base lsh gpu parallel video stitch algorithm base cuda experi result show algorithm mention improv perform stage featur extract imag stitch match run speed time tradit orb algorithm time tradit sift algorithm base analyz gpu resourc occup rate resolut imag stitch propos stream parallel strategi maxim util gpu resourc compar l orb algorithm effici strategi improv time make full use gpu resourc perform system accomplish paper time former embed one power dissip reduc w less
50,1810.03987,"
        Statistical shape modeling (SSM) has proven useful in many areas of biology and medicine as a new generation of morphometric approaches for the quantitative analysis of anatomical shapes. Recently, the increased availability of high-resolution in vivo images of anatomy has led to the development and distribution of open-source computational tools to model anatomical shapes and their variability within populations with unprecedented detail and statistical power. Nonetheless, there is little work on the evaluation and validation of such tools as related to clinical applications that rely on morphometric quantifications for treatment planning. To address this lack of validation, we systematically assess the outcome of widely used off-the-shelf SSM tools, namely ShapeWorks, SPHARM-PDM, and Deformetrica, in the context of designing closure devices for left atrium appendage (LAA) in atrial fibrillation (AF) patients to prevent stroke, where an incomplete LAA closure may be worse than no closure. This study is motivated by the potential role of SSM in the geometric design of closure devices, which could be informed by population-level statistics, and patient-specific device selection, which is driven by anatomical measurements that could be automated by relating patient-level anatomy to population-level morphometrics. Hence, understanding the consequences of different SSM tools for the final analysis is critical for the careful choice of the tool to be deployed in real clinical scenarios. Results demonstrate that estimated measurements from ShapeWorks model are more consistent compared to models from Deformetrica and SPHARM-PDM. Furthermore, ShapeWorks and Deformetrica shape models capture clinically relevant population-level variability compared to SPHARM-PDM models.
        △ Less
",statist shape model ssm proven use mani area biolog medicin new gener morphometr approach quantit analysi anatom shape recent increas avail high resolut vivo imag anatomi led develop distribut open sourc comput tool model anatom shape variabl within popul unpreced detail statist power nonetheless littl work evalu valid tool relat clinic applic reli morphometr quantif treatment plan address lack valid systemat assess outcom wide use shelf ssm tool name shapework spharm pdm deformetrica context design closur devic left atrium appendag laa atrial fibril af patient prevent stroke incomplet laa closur may wors closur studi motiv potenti role ssm geometr design closur devic could inform popul level statist patient specif devic select driven anatom measur could autom relat patient level anatomi popul level morphometr henc understand consequ differ ssm tool final analysi critic care choic tool deploy real clinic scenario result demonstr estim measur shapework model consist compar model deformetrica spharm pdm furthermor shapework deformetrica shape model captur clinic relev popul level variabl compar spharm pdm model less
51,1810.03986,"
        In this paper, we propose a method for home activity monitoring. We demonstrate our model on dataset of Detection and Classification of Acoustic Scenes and Events (DCASE) 2018 Challenge Task 5. This task aims to classify multi-channel audios into one of the provided pre-defined classes. All of these classes are daily activities performed in a home environment. To tackle this task, we propose a gated convolutional neural network with segment-level attention mechanism (SAM-GCNN). The proposed framework is a convolutional model with two auxiliary modules: a gated convolutional neural network and a segment-level attention mechanism. Furthermore, we adopted model ensemble to enhance the capability of generalization of our model. We evaluated our work on the development dataset of DCASE 2018 Task 5 and achieved competitive performance, with a macro-averaged F-1 score increasing from 83.76% to 89.33%, compared with the convolutional baseline system.
        △ Less
",paper propos method home activ monitor demonstr model dataset detect classif acoust scene event dcase challeng task task aim classifi multi channel audio one provid pre defin class class daili activ perform home environ tackl task propos gate convolut neural network segment level attent mechan sam gcnn propos framework convolut model two auxiliari modul gate convolut neural network segment level attent mechan furthermor adopt model ensembl enhanc capabl gener model evalu work develop dataset dcase task achiev competit perform macro averag f score increas compar convolut baselin system less
52,1810.03982,"
        Deep neural networks, in particular convolutional neural networks, have become highly effective tools for compressing images and solving inverse problems including denoising, inpainting, and reconstruction from few and noisy measurements. This success can be attributed in part to their ability to represent and generate natural images well. Contrary to classical tools such as wavelets, image-generating deep neural networks have a large number of parameters---typically a multiple of their output dimension---and need to be trained on large datasets. In this paper, we propose an untrained simple image model, called the deep decoder, which is a deep neural network that can generate natural images from very few weight parameters. The deep decoder has a simple architecture with no convolutions and fewer weight parameters than the output dimensionality. This underparameterization enables the deep decoder to compress images into a concise set of network weights, which we show is on par with wavelet-based thresholding. Further, underparameterization provides a barrier to overfitting, allowing the deep decoder to have state-of-the-art performance for denoising. The deep decoder is simple in the sense that each layer has an identical structure that consists of only one upsampling unit, pixel-wise linear combination of channels, ReLU activation, and channelwise normalization. This simplicity makes the network amenable to theoretical analysis, and it sheds light on the aspects of neural networks that enable them to form effective signal representations.
        △ Less
",deep neural network particular convolut neural network becom highli effect tool compress imag solv invers problem includ denois inpaint reconstruct noisi measur success attribut part abil repres gener natur imag well contrari classic tool wavelet imag gener deep neural network larg number paramet typic multipl output dimens need train larg dataset paper propos untrain simpl imag model call deep decod deep neural network gener natur imag weight paramet deep decod simpl architectur convolut fewer weight paramet output dimension underparameter enabl deep decod compress imag concis set network weight show par wavelet base threshold underparameter provid barrier overfit allow deep decod state art perform denois deep decod simpl sens layer ident structur consist one upsampl unit pixel wise linear combin channel relu activ channelwis normal simplic make network amen theoret analysi shed light aspect neural network enabl form effect signal represent less
53,1810.03981,"
        The Clustered Traveling Salesman Problem with a Prespecified Order on the Clusters, a variant of the well-known traveling salesman problem is studied in literature. In this problem, delivery locations are divided into clusters with different urgency levels and more urgent locations must be visited before less urgent ones. However, this could lead to an inefficient route in terms of traveling cost. This priority-oriented constraint can be relaxed by a rule called d-relaxed priority that provides a trade-off between transportation cost and emergency level. Our research proposes two approaches to solve the problem with d-relaxed priority rule. We improve the mathematical formulation proposed in the literature to construct an exact solution method. A meta-heuristic method based on the framework of Iterated Local Search with problem-tailored operators is also introduced to find approximate solutions. Experimental results show the effectiveness of our methods.
        △ Less
",cluster travel salesman problem prespecifi order cluster variant well known travel salesman problem studi literatur problem deliveri locat divid cluster differ urgenc level urgent locat must visit less urgent one howev could lead ineffici rout term travel cost prioriti orient constraint relax rule call relax prioriti provid trade transport cost emerg level research propos two approach solv problem relax prioriti rule improv mathemat formul propos literatur construct exact solut method meta heurist method base framework iter local search problem tailor oper also introduc find approxim solut experiment result show effect method less
54,1810.03980,"
        Locally repairable codes (LRCs) have received significant recent attention as a method of designing data storage systems robust to server failure. Optimal LRCs offer the ideal trade-off between minimum distance and locality, a measure of the cost of repairing a single codeword symbol. For optimal LRCs with minimum distance greater than or equal to 5, block length is bounded by a polynomial function of alphabet size. In this paper, we give explicit constructions of optimal-length (in terms of alphabet size), optimal LRCs with minimum distance equal to 5.
        △ Less
",local repair code lrc receiv signific recent attent method design data storag system robust server failur optim lrc offer ideal trade minimum distanc local measur cost repair singl codeword symbol optim lrc minimum distanc greater equal block length bound polynomi function alphabet size paper give explicit construct optim length term alphabet size optim lrc minimum distanc equal less
55,1810.03979,"
        After the tremendous success of convolutional neural networks in image classification, object detection, speech recognition, etc., there is now rising demand for deployment of these compute-intensive ML models on tightly power constrained embedded and mobile systems at low cost as well as for pushing the throughput in data centers. This has triggered a wave of research towards specialized hardware accelerators. Their performance is often constrained by I/O bandwidth and the energy consumption is dominated by I/O transfers to off-chip memory. We introduce and evaluate a novel, hardware-friendly compression scheme for the feature maps present within convolutional neural networks. We show that an average compression ratio of 4.4x relative to uncompressed data and a gain of 60% over existing method can be achieved for ResNet-34 with a compression block requiring <300 bit of sequential cells and minimal combinational logic.
        △ Less
",tremend success convolut neural network imag classif object detect speech recognit etc rise demand deploy comput intens ml model tightli power constrain embed mobil system low cost well push throughput data center trigger wave research toward special hardwar acceler perform often constrain bandwidth energi consumpt domin transfer chip memori introduc evalu novel hardwar friendli compress scheme featur map present within convolut neural network show averag compress ratio x rel uncompress data gain exist method achiev resnet compress block requir bit sequenti cell minim combin logic less
56,1810.03977,"
        Hackers and spammers are employing innovative and novel techniques to deceive novice and even knowledgeable internet users. Image spam is one of such technique where the spammer varies and changes some portion of the image such that it is indistinguishable from the original image fooling the users. This paper proposes a deep learning based approach for image spam detection using the convolutional neural networks which uses a dataset with 810 natural images and 928 spam images for classification achieving an accuracy of 91.7% outperforming the existing image processing and machine learning techniques
        △ Less
",hacker spammer employ innov novel techniqu deceiv novic even knowledg internet user imag spam one techniqu spammer vari chang portion imag indistinguish origin imag fool user paper propos deep learn base approach imag spam detect use convolut neural network use dataset natur imag spam imag classif achiev accuraci outperform exist imag process machin learn techniqu less
57,1810.03975,"
        This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modeling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German$\leftrightarrow$English.
        △ Less
",work investig altern model neural machin translat nmt propos novel architectur employ multi dimension long short term memori mdlstm translat model state art method sourc target sentenc treat one dimension sequenc time view translat two dimension map use mdlstm layer defin correspond sourc target word extend beyond current sequenc sequenc backbon nmt model structur sourc target sentenc align grid propos topolog show consist improv attent base sequenc sequenc model two wmt task german leftrightarrow english less
58,1810.03974,"
        We derive a nonlinear integro-differential transport equation describing collective evolution of weights under gradient descent in large-width neural-network-like models. We characterize stationary points of the evolution and analyze several scenarios where the transport equation can be solved approximately. We test our general method in the special case of linear free-knot splines, and find good agreement between theory and experiment in observations of global optima, stability of stationary points, and convergence rates.
        △ Less
",deriv nonlinear integro differenti transport equat describ collect evolut weight gradient descent larg width neural network like model character stationari point evolut analyz sever scenario transport equat solv approxim test gener method special case linear free knot spline find good agreement theori experi observ global optima stabil stationari point converg rate less
59,1810.03973,"
        As 3D scanning devices and depth sensors mature, point clouds have attracted increasing attention as a format for 3D object representation, with applications in various fields such as tele-presence, navigation and heritage reconstruction. However, point clouds usually exhibit holes of missing data, mainly due to the limitation of acquisition techniques and complicated structure. Further, point clouds are defined on irregular non-Euclidean domains, which is challenging to address especially with conventional signal processing tools. Hence, leveraging on recent advances in graph signal processing, we propose an efficient point cloud inpainting method, exploiting both the local smoothness and the non-local self-similarity in point clouds. Specifically, we first propose a frequency interpretation in graph nodal domain, based on which we introduce the local graph-signal smoothness prior in order to describe the local smoothness of point clouds. Secondly, we explore the characteristics of non-local self-similarity, by globally searching for the most similar area to the missing region. The similarity metric between two areas is defined based on the direct component and the anisotropic graph total variation of normals in each area. Finally, we formulate the hole-filling step as an optimization problem based on the selected most similar area and regularized by the graph-signal smoothness prior. Besides, we propose voxelization and automatic hole detection methods for the point cloud prior to inpainting. Experimental results show that the proposed approach outperforms four competing methods significantly, both in objective and subjective quality.
        △ Less
",scan devic depth sensor matur point cloud attract increas attent format object represent applic variou field tele presenc navig heritag reconstruct howev point cloud usual exhibit hole miss data mainli due limit acquisit techniqu complic structur point cloud defin irregular non euclidean domain challeng address especi convent signal process tool henc leverag recent advanc graph signal process propos effici point cloud inpaint method exploit local smooth non local self similar point cloud specif first propos frequenc interpret graph nodal domain base introduc local graph signal smooth prior order describ local smooth point cloud secondli explor characterist non local self similar global search similar area miss region similar metric two area defin base direct compon anisotrop graph total variat normal area final formul hole fill step optim problem base select similar area regular graph signal smooth prior besid propos voxel automat hole detect method point cloud prior inpaint experiment result show propos approach outperform four compet method significantli object subject qualiti less
60,1810.03970,"
        In this paper we provide a categorisation and implementation of digital ink features for behaviour characterisation. Based on four feature sets taken from literature, we provide a categorisation in different classes of syntactic and semantic features. We implemented a publicly available framework to calculate these features and show its deployment in the use case of analysing cognitive assessments performed using a digital pen.
        △ Less
",paper provid categoris implement digit ink featur behaviour characteris base four featur set taken literatur provid categoris differ class syntact semant featur implement publicli avail framework calcul featur show deploy use case analys cognit assess perform use digit pen less
61,1810.03969,"
        The clinical management of several cardiovascular conditions, such as pulmonary hypertension, require the assessment of the right ventricular (RV) function. This work addresses the fully automatic and robust access to one of the key RV biomarkers, its ejection fraction, from the gold standard imaging modality, MRI. The problem becomes the accurate segmentation of the RV blood pool from cine MRI sequences. This work proposes a solution based on Fully Convolutional Neural Networks (FCNN), where our first contribution is the optimal combination of three concepts (the convolution Gated Recurrent Units (GRU), the Generative Adversarial Networks (GAN), and the L1 loss function) that achieves an improvement of 0.05 and 3.49 mm in Dice Index and Hausdorff Distance respectively with respect to the baseline FCNN. This improvement is then doubled by our second contribution, the ROI-GAN, that sets two GANs to cooperate working at two fields of view of the image, its full resolution and the region of interest (ROI). Our rationale here is to better guide the FCNN learning by combining global (full resolution) and local Region Of Interest (ROI) features. The study is conducted in a large in-house dataset of $\sim$ 23.000 segmented MRI slices, and its generality is verified in a publicly available dataset.
        △ Less
",clinic manag sever cardiovascular condit pulmonari hypertens requir assess right ventricular rv function work address fulli automat robust access one key rv biomark eject fraction gold standard imag modal mri problem becom accur segment rv blood pool cine mri sequenc work propos solut base fulli convolut neural network fcnn first contribut optim combin three concept convolut gate recurr unit gru gener adversari network gan l loss function achiev improv mm dice index hausdorff distanc respect respect baselin fcnn improv doubl second contribut roi gan set two gan cooper work two field view imag full resolut region interest roi rational better guid fcnn learn combin global full resolut local region interest roi featur studi conduct larg hous dataset sim segment mri slice gener verifi publicli avail dataset less
62,1810.03968,"
        Accurate segmentation of the left ventricle myocardium in cardiac CT angiography (CCTA) is essential for e.g. the assessment of myocardial perfusion. Automatic deep learning methods for segmentation in CCTA might suffer from differences in contrast-agent attenuation between training and test data due to non-standardized contrast administration protocols and varying cardiac output. We propose augmentation of the training data with virtual mono-energetic reconstructions from a spectral CT scanner which show different attenuation levels of the contrast agent. We compare this to an augmentation by linear scaling of all intensity values, and combine both types of augmentation. We train a 3D fully convolutional network (FCN) with 10 conventional CCTA images and corresponding virtual mono-energetic reconstructions acquired on a spectral CT scanner, and evaluate on 40 CCTA scans acquired on a conventional CT scanner. We show that training with data augmentation using virtual mono-energetic images improves upon training with only conventional images (Dice similarity coefficient (DSC) 0.895 $\pm$ 0.039 vs. 0.846 $\pm$ 0.125). In comparison, training with data augmentation using linear scaling improves the DSC to 0.890 $\pm$ 0.039. Moreover, combining the results of both augmentation methods leads to a DSC of 0.901 $\pm$ 0.036, showing that both augmentations lead to different local improvements of the segmentations. Our results indicate that virtual mono-energetic images improve the generalization of an FCN used for myocardium segmentation in CCTA images.
        △ Less
",accur segment left ventricl myocardium cardiac ct angiographi ccta essenti e g assess myocardi perfus automat deep learn method segment ccta might suffer differ contrast agent attenu train test data due non standard contrast administr protocol vari cardiac output propos augment train data virtual mono energet reconstruct spectral ct scanner show differ attenu level contrast agent compar augment linear scale intens valu combin type augment train fulli convolut network fcn convent ccta imag correspond virtual mono energet reconstruct acquir spectral ct scanner evalu ccta scan acquir convent ct scanner show train data augment use virtual mono energet imag improv upon train convent imag dice similar coeffici dsc pm vs pm comparison train data augment use linear scale improv dsc pm moreov combin result augment method lead dsc pm show augment lead differ local improv segment result indic virtual mono energet imag improv gener fcn use myocardium segment ccta imag less
63,1810.03967,"
        Vision-based navigation of modern autonomous vehicles primarily depends on Deep Neural Network (DNN) based systems in which the controller obtains input from sensors/detectors such as cameras, and produces an output such as a steering wheel angle to navigate the vehicle safely in roadway traffic. Typically, these DNN-based systems are trained through supervised and/or transfer learning; however, recent studies show that these systems can be compromised by perturbation or adversarial input features on the trained DNN-based models. Similarly, this perturbation can be introduced into the autonomous vehicle DNN-based system by roadway hazards such as debris and roadblocks. In this study, we first introduce a roadway hazardous environment (both intentional and unintentional) that can compromise the DNN-based system of an autonomous vehicle, producing an incorrect vehicle navigational output such as a steering wheel angle, which can cause crashes resulting in fatality and injury. Then, we develop an approach based on object detection and semantic segmentation to mitigate the adverse effect of this hazardous environment, one that helps the autonomous vehicle to navigate safely around such hazards. This study finds the DNN-based model with hazardous object detection, and semantic segmentation improves the ability of an autonomous vehicle to avoid potential crashes by 21% compared to the traditional DNN-based autonomous driving system.
        △ Less
",vision base navig modern autonom vehicl primarili depend deep neural network dnn base system control obtain input sensor detector camera produc output steer wheel angl navig vehicl safe roadway traffic typic dnn base system train supervis transfer learn howev recent studi show system compromis perturb adversari input featur train dnn base model similarli perturb introduc autonom vehicl dnn base system roadway hazard debri roadblock studi first introduc roadway hazard environ intent unintent compromis dnn base system autonom vehicl produc incorrect vehicl navig output steer wheel angl caus crash result fatal injuri develop approach base object detect semant segment mitig advers effect hazard environ one help autonom vehicl navig safe around hazard studi find dnn base model hazard object detect semant segment improv abil autonom vehicl avoid potenti crash compar tradit dnn base autonom drive system less
64,1810.03966,"
        When performing data classification over a stream of continuously occurring instances, a key challenge is to develop an open-world classifier that anticipates instances from an unknown class. Studies addressing this problem, typically called novel class detection, have considered classification methods that reactively adapt to such changes along the stream. Importantly, they rely on the property of cohesion and separation among instances in feature space. Instances belonging to the same class are assumed to be closer to each other (cohesion) than those belonging to different classes (separation). Unfortunately, this assumption may not have large support when dealing with high dimensional data such as images. In this paper, we address this key challenge by proposing a semisupervised multi-task learning framework called CSIM which aims to intrinsically search for a latent space suitable for detecting labels of instances from both known and unknown classes. Particularly, we utilize a convolution neural network layer that aids in the learning of a latent feature space suitable for novel class detection. We empirically measure the performance of CSIM over multiple realworld image datasets and demonstrate its superiority by comparing its performance with existing semi-supervised methods.
        △ Less
",perform data classif stream continu occur instanc key challeng develop open world classifi anticip instanc unknown class studi address problem typic call novel class detect consid classif method reactiv adapt chang along stream importantli reli properti cohes separ among instanc featur space instanc belong class assum closer cohes belong differ class separ unfortun assumpt may larg support deal high dimension data imag paper address key challeng propos semisupervis multi task learn framework call csim aim intrins search latent space suitabl detect label instanc known unknown class particularli util convolut neural network layer aid learn latent featur space suitabl novel class detect empir measur perform csim multipl realworld imag dataset demonstr superior compar perform exist semi supervis method less
65,1810.03965,"
        We present an algorithm for realtime anomaly detection in low to medium density crowd videos using trajectory-level behavior learning. Our formulation combines online tracking algorithms from computer vision, non-linear pedestrian motion models from crowd simulation, and Bayesian learning techniques to automatically compute the trajectory-level pedestrian behaviors for each agent in the video. These learned behaviors are used to segment the trajectories and motions of different pedestrians or agents and detect anomalies. We demonstrate the interactive performance on the PETS ARENA dataset as well as indoor and outdoor crowd video benchmarks consisting of tens of human agents. We also discuss the implications of recent public policy and law enforcement issues relating to surveillance and our research.
        △ Less
",present algorithm realtim anomali detect low medium densiti crowd video use trajectori level behavior learn formul combin onlin track algorithm comput vision non linear pedestrian motion model crowd simul bayesian learn techniqu automat comput trajectori level pedestrian behavior agent video learn behavior use segment trajectori motion differ pedestrian agent detect anomali demonstr interact perform pet arena dataset well indoor outdoor crowd video benchmark consist ten human agent also discuss implic recent public polici law enforc issu relat surveil research less
66,1810.03964,"
        Advanced video classification systems decode video frames to derive the necessary texture and motion representations for ingestion and analysis by spatio-temporal deep convolutional neural networks (CNNs). However, when considering visual Internet-of-Things applications, surveillance systems and semantic crawlers of large video repositories, the video capture and the CNN-based semantic analysis parts do not tend to be co-located. This necessitates the transport of compressed video over networks and incurs significant overhead in bandwidth and energy consumption, thereby significantly undermining the deployment potential of such systems. In this paper, we investigate the trade-off between the encoding bitrate and the achievable accuracy of CNN-based video classification models that directly ingest AVC/H.264 and HEVC encoded videos. Instead of retaining entire compressed video bitstreams and applying complex optical flow calculations prior to CNN processing, we only retain motion vector and select texture information at significantly-reduced bitrates and apply no additional processing prior to CNN ingestion. Based on three CNN architectures and two action recognition datasets, we achieve 11%-94% saving in bitrate with marginal effect on classification accuracy. A model-based selection between multiple CNNs increases these savings further, to the point where, if up to 7% loss of accuracy can be tolerated, video classification can take place with as little as 3 kbps for the transport of the required compressed video information to the system implementing the CNN models.
        △ Less
",advanc video classif system decod video frame deriv necessari textur motion represent ingest analysi spatio tempor deep convolut neural network cnn howev consid visual internet thing applic surveil system semant crawler larg video repositori video captur cnn base semant analysi part tend co locat necessit transport compress video network incur signific overhead bandwidth energi consumpt therebi significantli undermin deploy potenti system paper investig trade encod bitrat achiev accuraci cnn base video classif model directli ingest avc h hevc encod video instead retain entir compress video bitstream appli complex optic flow calcul prior cnn process retain motion vector select textur inform significantli reduc bitrat appli addit process prior cnn ingest base three cnn architectur two action recognit dataset achiev save bitrat margin effect classif accuraci model base select multipl cnn increas save point loss accuraci toler video classif take place littl kbp transport requir compress video inform system implement cnn model less
67,1810.03963,"
        This paper proposes a novel approach to stereo visual odometry without stereo matching. It is particularly robust in scenes of repetitive high-frequency textures. Referred to as DSVO (Direct Stereo Visual Odometry), it operates directly on pixel intensities, without any explicit feature matching, and is thus efficient and more accurate than the state-of-the-art stereo-matching-based methods. It applies a semi-direct monocular visual odometry running on one camera of the stereo pair, tracking the camera pose and mapping the environment simultaneously; the other camera is used to optimize the scale of monocular visual odometry. We evaluate DSVO in a number of challenging scenes to evaluate its performance and present comparisons with the state-of-the-art stereo visual odometry algorithms.
        △ Less
",paper propos novel approach stereo visual odometri without stereo match particularli robust scene repetit high frequenc textur refer dsvo direct stereo visual odometri oper directli pixel intens without explicit featur match thu effici accur state art stereo match base method appli semi direct monocular visual odometri run one camera stereo pair track camera pose map environ simultan camera use optim scale monocular visual odometri evalu dsvo number challeng scene evalu perform present comparison state art stereo visual odometri algorithm less
68,1810.03962,"
        This paper presents Densely Supervised Grasp Detector (DSGD), a deep learning framework which combines CNN structures with layer-wise feature fusion and produces grasps and their confidence scores at different levels of the image hierarchy (i.e., global-, region-, and pixel-levels). Specifically, at the global-level, DSGD uses the entire image information to predict a grasp and its confidence score. At the region-level, DSGD uses a region proposal network to identify salient regions in the image and predicts a grasp for each salient region. At the pixel-level, DSGD uses a fully convolutional network and predicts a grasp and its confidence at every pixel. The grasp with the highest confidence score is selected as the output of DSGD. This selection from hierarchically generated grasp candidates overcomes limitations of the individual models. DSGD outperforms state-of-the-art methods on the Cornell grasp dataset in terms of grasp accuracy. Evaluation on a multi-object dataset and real-world robotic grasping experiments show that DSGD produces highly stable grasps on a set of unseen objects in new environments. It achieves 96% grasp detection accuracy and 90% robotic grasping success rate with real-time inference speed.
        △ Less
",paper present dens supervis grasp detector dsgd deep learn framework combin cnn structur layer wise featur fusion produc grasp confid score differ level imag hierarchi e global region pixel level specif global level dsgd use entir imag inform predict grasp confid score region level dsgd use region propos network identifi salient region imag predict grasp salient region pixel level dsgd use fulli convolut network predict grasp confid everi pixel grasp highest confid score select output dsgd select hierarch gener grasp candid overcom limit individu model dsgd outperform state art method cornel grasp dataset term grasp accuraci evalu multi object dataset real world robot grasp experi show dsgd produc highli stabl grasp set unseen object new environ achiev grasp detect accuraci robot grasp success rate real time infer speed less
69,1810.03961,"
        Intelligent reflecting surface (IRS) is envisioned to be a new and revolutionizing technology for achieving spectrum and energy efficient wireless communication networks cost-effectively in the future. Specifically, an IRS consists of a large number of low-cost passive elements each reflecting the incident signal with a certain phase shift to collaboratively achieve beamforming and/or interference suppression at designated receivers. In this paper, we study an IRS-aided multiuser multiple-input single-output (MISO) wireless system where one IRS is deployed to assist in the communication from a multi- antenna access point (AP) to multiple single-antenna users. As such, each user receives the superposed signals from the AP as well as the IRS via its reflection. We aim to minimize the total transmit power at the AP by jointly optimizing the transmit beamforming by active antenna array at the AP and reflect beamforming by passive phase shifters at the IRS, subject to users' individual signal-to-interference-plus-noise ratio (SINR) constraints. However, the formulated problem is non-convex and difficult to be solved optimally.
        △ Less
",intellig reflect surfac ir envis new revolution technolog achiev spectrum energi effici wireless commun network cost effect futur specif ir consist larg number low cost passiv element reflect incid signal certain phase shift collabor achiev beamform interfer suppress design receiv paper studi ir aid multius multipl input singl output miso wireless system one ir deploy assist commun multi antenna access point ap multipl singl antenna user user receiv superpos signal ap well ir via reflect aim minim total transmit power ap jointli optim transmit beamform activ antenna array ap reflect beamform passiv phase shifter ir subject user individu signal interfer plu nois ratio sinr constraint howev formul problem non convex difficult solv optim less
70,1810.03958,"
        Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We fix VB and turn it into a robust inference tool for Bayesian neural networks. We achieve this with two innovations: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate strong predictive performance over alternative approaches.
        △ Less
",bayesian neural network bnn hold great promis flexibl principl solut deal uncertainti learn finit data among approach realiz probabilist infer deep neural network variat bay vb theoret ground gener applic comput effici wide recognit potenti advantag variat bay seen limit practic use bnn real applic argu variat infer neural network fragil success implement requir care initi tune prior varianc well control varianc mont carlo gradient estim fix vb turn robust infer tool bayesian neural network achiev two innov first introduc novel determinist method approxim moment neural network elimin gradient varianc second introduc hierarch prior paramet novel empir bay procedur automat select prior varianc combin two innov result method highli effici robust applic heteroscedast regress demonstr strong predict perform altern approach less
71,1810.03956,"
        Video-mapping is the process of coherent video-projection of images, animations or movies on static objects or buildings for shows. This paper focuses on the dynamic video-mapping of the suit of a puppet being moved by its puppeteer on the theater stage. This may allow changing the costume dynamically and simulate light interaction and more.
  Contrary to common video-mapping, the image warping cannot be done once, offline, before the show. It must be done in real-time, and considering a non-flat projection surface, so that the video-projected suit always maps perfectly the puppet, automatically.
  Hence, we propose a new visual tracking method of articulated object, for the puppet tracking, exploiting the silhouette of a 3D model of it, in the depth images of a Kinect v2. Then, considering the precise calibration between the latter and the video-projector, that we propose, coherent dynamic video-mapping is made possible as the presented results show.
        △ Less
",video map process coher video project imag anim movi static object build show paper focus dynam video map suit puppet move puppet theater stage may allow chang costum dynam simul light interact contrari common video map imag warp cannot done offlin show must done real time consid non flat project surfac video project suit alway map perfectli puppet automat henc propos new visual track method articul object puppet track exploit silhouett model depth imag kinect v consid precis calibr latter video projector propos coher dynam video map made possibl present result show less
72,1810.03955,"
        We propose a unified architecture for next generation cognitive, low cost, mobile internet. The end user platform is able to scale as per the application and network requirements. It takes computing out of the data center and into end user platform. Internet enables open standards, accessible computing and applications programmability on a commodity platform. The architecture is a super-set to present day infrastructure web computing. The Java virtual machine (JVM) derives from the stack architecture. Applications can be developed and deployed on a multitude of host platforms. O(1) -> O(N). Computing and the internet today are more accessible and available to the larger community. Machine learning has made extensive advances with the availability of modern computing. It is used widely in NLP, Computer Vision, Deep learning and AI. A prototype device for mobile could contain N compute and N MB of memory.
        △ Less
",propos unifi architectur next gener cognit low cost mobil internet end user platform abl scale per applic network requir take comput data center end user platform internet enabl open standard access comput applic programm commod platform architectur super set present day infrastructur web comput java virtual machin jvm deriv stack architectur applic develop deploy multitud host platform n comput internet today access avail larger commun machin learn made extens advanc avail modern comput use wide nlp comput vision deep learn ai prototyp devic mobil could contain n comput n mb memori less
73,1810.03947,"
        We address two challenges of probabilistic topic modelling in order to better estimate the probability of a word in a given context, i.e., P(word|context): (1) No Language Structure in Context: Probabilistic topic models ignore word order by summarizing a given context as a ""bag-of-word"" and consequently the semantics of words in the context is lost. The LSTM-LM learns a vector-space representation of each word by accounting for word order in local collocation patterns and models complex characteristics of language (e.g., syntax and semantics), while the TM simultaneously learns a latent representation from the entire document and discovers the underlying thematic structure. We unite two complementary paradigms of learning the meaning of word occurrences by combining a TM and a LM in a unified probabilistic framework, named as ctx-DocNADE. (2) Limited Context and/or Smaller training corpus of documents: In settings with a small number of word occurrences (i.e., lack of context) in short text or data sparsity in a corpus of few documents, the application of TMs is challenging. We address this challenge by incorporating external knowledge into neural autoregressive topic models via a language modelling approach: we use word embeddings as input of a LSTM-LM with the aim to improve the word-topic mapping on a smaller and/or short-text corpus. The proposed DocNADE extension is named as ctx-DocNADEe.
  We present novel neural autoregressive topic model variants coupled with neural LMs and embeddings priors that consistently outperform state-of-the-art generative TMs in terms of generalization (perplexity), interpretability (topic coherence) and applicability (retrieval and classification) over 6 long-text and 8 short-text datasets from diverse domains.
        △ Less
",address two challeng probabilist topic model order better estim probabl word given context e p word context languag structur context probabilist topic model ignor word order summar given context bag word consequ semant word context lost lstm lm learn vector space represent word account word order local colloc pattern model complex characterist languag e g syntax semant tm simultan learn latent represent entir document discov underli themat structur unit two complementari paradigm learn mean word occurr combin tm lm unifi probabilist framework name ctx docnad limit context smaller train corpu document set small number word occurr e lack context short text data sparsiti corpu document applic tm challeng address challeng incorpor extern knowledg neural autoregress topic model via languag model approach use word embed input lstm lm aim improv word topic map smaller short text corpu propos docnad extens name ctx docnade present novel neural autoregress topic model variant coupl neural lm embed prior consist outperform state art gener tm term gener perplex interpret topic coher applic retriev classif long text short text dataset divers domain less
74,1810.03946,"
        Currently, increasingly deeper neural networks have been applied to improve their accuracy. In contrast, We propose a novel wider Convolutional Neural Networks (CNN) architecture, motivated by the Multi-column Deep Neural Networks and the Network In Network(NIN), aiming for higher accuracy without input data transmutation. In our architecture, namely ""CNN In Convolution""(CNNIC), a small CNN, instead of the original generalized liner model(GLM) based filters, is convoluted as kernel on the original image, serving as feature extracting layer of this networks. And further classifications are then carried out by a global average pooling layer and a softmax layer. Dropout and orthonormal initialization are applied to overcome training difficulties including slow convergence and over-fitting. Persuasive classification performance is demonstrated on MNIST.
        △ Less
",current increasingli deeper neural network appli improv accuraci contrast propos novel wider convolut neural network cnn architectur motiv multi column deep neural network network network nin aim higher accuraci without input data transmut architectur name cnn convolut cnnic small cnn instead origin gener liner model glm base filter convolut kernel origin imag serv featur extract layer network classif carri global averag pool layer softmax layer dropout orthonorm initi appli overcom train difficulti includ slow converg fit persuas classif perform demonstr mnist less
75,1810.03945,"
        This paper presents an interconnected control-planning strategy for redundant manipulators, subject to system and environmental constraints. The method incorporates low-level control characteristics and high-level planning components into a robust strategy for manipulators acting in complex environments, subject to joint limits. This strategy is formulated using an adaptive control rule, the estimated dynamic model of the robotic system and the nullspace of the linearized constraints. A path is generated that takes into account the capabilities of the platform. The proposed method is computationally efficient, enabling its implementation on a real multi-body robotic system. Through experimental results with a 7 DOF manipulator, we demonstrate the performance of the method in real-world scenarios.
        △ Less
",paper present interconnect control plan strategi redund manipul subject system environment constraint method incorpor low level control characterist high level plan compon robust strategi manipul act complex environ subject joint limit strategi formul use adapt control rule estim dynam model robot system nullspac linear constraint path gener take account capabl platform propos method comput effici enabl implement real multi bodi robot system experiment result dof manipul demonstr perform method real world scenario less
76,1810.03943,"
        OpenAI Gym is a toolkit for reinforcement learning (RL) research. It includes a large number of well-known problems that expose a common interface allowing to directly compare the performance results of different RL algorithms. Since many years, the ns-3 network simulation tool is the de-facto standard for academic and industry research into networking protocols and communications technology. Numerous scientific papers were written reporting results obtained using ns-3, and hundreds of models and modules were written and contributed to the ns-3 code base. Today as a major trend in network research we see the use of machine learning tools like RL. What is missing is the integration of a RL framework like OpenAI Gym into the network simulator ns-3. This paper presents the ns3-gym framework. First, we discuss design decisions that went into the software. Second, two illustrative examples implemented using ns3-gym are presented. Our software package is provided to the community as open source under a GPL license and hence can be easily extended.
        △ Less
",openai gym toolkit reinforc learn rl research includ larg number well known problem expos common interfac allow directli compar perform result differ rl algorithm sinc mani year ns network simul tool de facto standard academ industri research network protocol commun technolog numer scientif paper written report result obtain use ns hundr model modul written contribut ns code base today major trend network research see use machin learn tool like rl miss integr rl framework like openai gym network simul ns paper present ns gym framework first discuss design decis went softwar second two illustr exampl implement use ns gym present softwar packag provid commun open sourc gpl licens henc easili extend less
77,1810.03940,"
        We study the behaviour of a seismic simulation on a server with an Intel Optane memory subsystem. The simulation uses the ExaHyPE engine. ExaHyPE combines dynamically adaptive meshes (AMR) with ADER-DG. It is parallelised using tasks, and it is cache efficient. AMR plus ADER-DG yields a task graph which is highly dynamic in nature and comprises both arithmetically expensive and bandwidth-intense tasks. Despite this inhomogeneous character, our code benefits from AVX vectorization, though it suffers from memory access bursts. We show that a frequency reduction of the chip does not mitigate burst effects although it improves the code's energy-to-solution. Even worse, the burst's latency dependence becomes worse once we add Optane. However, in cases where memory-intense and computationally expensive tasks overlap, ExaHype's cache-oblivious implementation can exploit the Intel Memory Drive Technology (IMDT) virtualizing the extra memory layer, and it suffers only from a less than 3x performance decline per degree of freedom versus pure DDR4 memory. We thus propose that upcoming supercomputing simulation codes with dynamic, inhomogeneous task graphs require algorithms and task schedulers which actively prefetch data, intermix tasks of different character, and apply frequency control to or switch off the memory if appropriate.
        △ Less
",studi behaviour seismic simul server intel optan memori subsystem simul use exahyp engin exahyp combin dynam adapt mesh amr ader dg parallelis use task cach effici amr plu ader dg yield task graph highli dynam natur compris arithmet expens bandwidth intens task despit inhomogen charact code benefit avx vector though suffer memori access burst show frequenc reduct chip mitig burst effect although improv code energi solut even wors burst latenc depend becom wors add optan howev case memori intens comput expens task overlap exahyp cach oblivi implement exploit intel memori drive technolog imdt virtual extra memori layer suffer less x perform declin per degre freedom versu pure ddr memori thu propos upcom supercomput simul code dynam inhomogen task graph requir algorithm task schedul activ prefetch data intermix task differ charact appli frequenc control switch memori appropri less
78,1810.03931,"
        A general purpose, modular program package for the integration of large number of independent ordinary differential equation systems capable of using professional graphics cards is presented. The available numerical schemes are the explicit and adaptive Runge--Kutta--Cash--Karp algorithm and the explicit fourth order Runge--Kutta method with fixed time step. In order to harness the huge processing power of graphics cards, the intermediate points of the computed trajectories are not stored. As a compensate, with pre-declared device functions, the required special features or properties of a solution can be easily extracted and stored each into a dedicated variable. For instance, the maximum and minimum values and/or their time instances. Event handling is also incorporated into the package in order to detect special points which can be stored as well. Moreover, again with pre-declared device function calls at such special points, the efficient handling of non-smooth dynamics---e.g. impact dynamics---is possible. Several test cases are presented to demonstrate the flexibility of the pre-declared device functions and the strength of the program package. The applied models are the simple Duffing oscillator, the more complex Keller--Miksis equation known in bubble dynamics, and a system describing the behaviour of a pressure relief valve that can exhibit impact dynamics.
        △ Less
",gener purpos modular program packag integr larg number independ ordinari differenti equat system capabl use profession graphic card present avail numer scheme explicit adapt rung kutta cash karp algorithm explicit fourth order rung kutta method fix time step order har huge process power graphic card intermedi point comput trajectori store compens pre declar devic function requir special featur properti solut easili extract store dedic variabl instanc maximum minimum valu time instanc event handl also incorpor packag order detect special point store well moreov pre declar devic function call special point effici handl non smooth dynam e g impact dynam possibl sever test case present demonstr flexibl pre declar devic function strength program packag appli model simpl duf oscil complex keller miksi equat known bubbl dynam system describ behaviour pressur relief valv exhibit impact dynam less
79,1810.03918,"
        Question Answering (QA) research is a significant and challenging task in Natural Language Processing. QA aims to extract an exact answer from a relevant text snippet or a document. The motivation behind QA research is the need of user who is using state-of-the-art search engines. The user expects an exact answer rather than a list of documents that probably contain the answer. In this paper, for a successful answer extraction from relevant documents several efficient features and relations are required to extract. The features include various lexical, syntactic, semantic and structural features. The proposed structural features are extracted from the dependency features of the question and supported document. Experimental results show that structural features improve the accuracy of answer extraction when combined with the basic features and designed using dependency principles. Proposed structural features use new design principles which extract the long-distance relations. This addition is a possible reason behind the improvement in overall answer extraction accuracy.
        △ Less
",question answer qa research signific challeng task natur languag process qa aim extract exact answer relev text snippet document motiv behind qa research need user use state art search engin user expect exact answer rather list document probabl contain answer paper success answer extract relev document sever effici featur relat requir extract featur includ variou lexic syntact semant structur featur propos structur featur extract depend featur question support document experiment result show structur featur improv accuraci answer extract combin basic featur design use depend principl propos structur featur use new design principl extract long distanc relat addit possibl reason behind improv overal answer extract accuraci less
80,1810.03916,"
        The growing interest in archaeology has enabled the discovery of an immense number of cultural heritage assets and historical sites. Hence, preservation of CH through digitalisation is becoming a primordial requirement for many countries as a part of national cultural programs. However, CH digitalisation is still posing serious challenges such as cost and time-consumption. In this manuscript, 3D holoscopic (H3D) technology is applied to capture small sized CH assets. The H3D camera utilises micro lens array within a single aperture lens and typical 2D sensor to acquire 3D information. This technology allows 3D autostereoscopic visualisation with full motion parallax if convenient Microlens Array (MLA)is used on the display side. Experimental works have shown easiness and simplicity of H3D acquisition compared to existing technologies. In fact, H3D capture process took an equal time of shooting a standard 2D image. These advantages qualify H3D technology to be cost effective and time-saving technology for cultural heritage 3D digitisation.
        △ Less
",grow interest archaeolog enabl discoveri immens number cultur heritag asset histor site henc preserv ch digitalis becom primordi requir mani countri part nation cultur program howev ch digitalis still pose seriou challeng cost time consumpt manuscript holoscop h technolog appli captur small size ch asset h camera utilis micro len array within singl apertur len typic sensor acquir inform technolog allow autostereoscop visualis full motion parallax conveni microlen array mla use display side experiment work shown easi simplic h acquisit compar exist technolog fact h captur process took equal time shoot standard imag advantag qualifi h technolog cost effect time save technolog cultur heritag digitis less
81,1810.03913,"
        Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.
        △ Less
",deep neural network dnn vulner malici gener adversari exampl exampl intent design make impercept perturb often mislead dnn make incorrect predict phenomenon mean signific risk appli dnn safeti critic applic driverless car address issu present visual analyt approach explain primari caus wrong predict introduc adversari exampl key analyz datapath adversari exampl compar normal exampl datapath group critic neuron connect end formul datapath extract subset select problem approxim solv base back propag multi level visual consist segment dag layer level euler diagram featur map level heat map neuron level design help expert investig datapath high level layer detail neuron activ two case studi conduct demonstr promis approach support explain work mechan adversari exampl less
82,1810.03908,"
        Image segmentation is the process of partitioning an image into meaningful segments. The meaning of the segments is subjective due to the definition of homogeneity is varied based on the users perspective hence the automation of the segmentation is challenging. Watershed is a popular segmentation technique which assumes topographic map in an image, with the brightness of each pixel representing its height, and finds the lines that run along the tops of ridges. The results from the algorithm typically suffer from over segmentation due to the lack of knowledge of the objects being classified. This paper presents an approach to reduce the over segmentation of watershed algorithm by assuming that the different adjacent segments of an object have similar color distribution. The approach demonstrates an improvement over conventional watershed algorithm.
        △ Less
",imag segment process partit imag meaning segment mean segment subject due definit homogen vari base user perspect henc autom segment challeng watersh popular segment techniqu assum topograph map imag bright pixel repres height find line run along top ridg result algorithm typic suffer segment due lack knowledg object classifi paper present approach reduc segment watersh algorithm assum differ adjac segment object similar color distribut approach demonstr improv convent watersh algorithm less
83,1810.03884,"
        Over the years, the Internet has been enriched with new available communication technologies, for both fixed and mobile networks and devices, exhibiting an impressive growth in terms of performance, with steadily increasing available data rates. The Internet research community has kept trying to evolve the transport layer protocols to match the capabilities of modern networks, in order to fully reap the benefits of the new communication technologies. This paper surveys the main novelties related to transport protocols that have been recently proposed, identifying three main research trends: (i) the evolution of congestion control algorithms, to target optimal performance in challenging scenarios, possibly with the application of machine learning techniques; (ii) the proposal of brand new transport protocols, alternative to TCP and implemented in the user-space; and (iii) the introduction of multi-path capabilities at the transport layer.
        △ Less
",year internet enrich new avail commun technolog fix mobil network devic exhibit impress growth term perform steadili increas avail data rate internet research commun kept tri evolv transport layer protocol match capabl modern network order fulli reap benefit new commun technolog paper survey main novelti relat transport protocol recent propos identifi three main research trend evolut congest control algorithm target optim perform challeng scenario possibl applic machin learn techniqu ii propos brand new transport protocol altern tcp implement user space iii introduct multi path capabl transport layer less
84,1810.03880,"
        We consider the problem of building a state representation model in a continual fashion. As the environment changes, the aim is to efficiently compress the state's information without losing past knowledge. The learned features are then fed to a Reinforcement Learning algorithm to learn a policy. We propose to use Variational Auto-Encoders for state representation, and Generative Replay, e.g the use of generated samples, to maintain past knowledge. We also provide a general and statistically sound method for automatic environment change detection. Our method provides efficient state representation as well as forward transfer, and avoids catastrophic forgetting. The resulting model is capable of incrementally learning information without using past data and with a bounded system size.
        △ Less
",consid problem build state represent model continu fashion environ chang aim effici compress state inform without lose past knowledg learn featur fed reinforc learn algorithm learn polici propos use variat auto encod state represent gener replay e g use gener sampl maintain past knowledg also provid gener statist sound method automat environ chang detect method provid effici state represent well forward transfer avoid catastroph forget result model capabl increment learn inform without use past data bound system size less
85,1810.03879,"
        Semantic role theory is a widely used approach for event representation. Yet, there are multiple indications that semantic role paradigm is necessary but not sufficient to cover all elements of event structure. We conducted an analysis of semantic role representation for events to provide an empirical evidence of insufficiency. The consequence of that is a hybrid role-scalar approach. The results are considered as preliminary in investigation of semantic roles coverage for event representation.
        △ Less
",semant role theori wide use approach event represent yet multipl indic semant role paradigm necessari suffici cover element event structur conduct analysi semant role represent event provid empir evid insuffici consequ hybrid role scalar approach result consid preliminari investig semant role coverag event represent less
86,1810.03875,"
        Semantic role theory considers roles as a small universal set of unanalyzed entities. It means that formally there are no restrictions on role combinations. We argue that the semantic roles co-occur in verb representations. It means that there are hidden restrictions on role combinations. To demonstrate that a practical and evidence-based approach has been built on in-depth analysis of the largest verb database VerbNet. The consequences of this approach are considered.
        △ Less
",semant role theori consid role small univers set unanalyz entiti mean formal restrict role combin argu semant role co occur verb represent mean hidden restrict role combin demonstr practic evid base approach built depth analysi largest verb databas verbnet consequ approach consid less
87,1810.03873,"
        For robots acting in the presence of observers, we examine the information that is divulged if the observer is party to the robot's plan. Privacy constraints are specified as the stipulations on what can be inferred during plan execution. We imagine a case in which the robot's plan is divulged beforehand, so that the observer can use this {\em a priori} information along with the disclosed executions. The divulged plan, which can be represented by a procrustean graph, is shown to undermine privacy precisely to the extent that it can eliminate action-observation sequences that will never appear in the plan. Future work will consider how the divulged plan might be sought as the output of a planning procedure.
        △ Less
",robot act presenc observ examin inform divulg observ parti robot plan privaci constraint specifi stipul infer plan execut imagin case robot plan divulg beforehand observ use em priori inform along disclos execut divulg plan repres procrustean graph shown undermin privaci precis extent elimin action observ sequenc never appear plan futur work consid divulg plan might sought output plan procedur less
88,1810.03871,"
        We propose a new generative adversarial architecture to mitigate imbalance data problem in medical image semantic segmentation where the majority of pixels belongs to a healthy region and few belong to lesion or non-health region. A model trained with imbalanced data tends to bias toward healthy data which is not desired in clinical applications and predicted outputs by these networks have high precision and low sensitivity. We propose a new conditional generative refinement network with three components: a generative, a discriminative, and a refinement network to mitigate unbalanced data problem through ensemble learning. The generative network learns to a segment at the pixel level by getting feedback from the discriminative network according to the true positive and true negative maps. On the other hand, the refinement network learns to predict the false positive and the false negative masks produced by the generative network that has significant value, especially in medical application. The final semantic segmentation masks are then composed by the output of the three networks. The proposed architecture shows state-of-the-art results on LiTS-2017 for liver lesion segmentation, and two microscopic cell segmentation datasets MDA231, PhC-HeLa. We have achieved competitive results on BraTS-2017 for brain tumour segmentation.
        △ Less
",propos new gener adversari architectur mitig imbal data problem medic imag semant segment major pixel belong healthi region belong lesion non health region model train imbalanc data tend bia toward healthi data desir clinic applic predict output network high precis low sensit propos new condit gener refin network three compon gener discrimin refin network mitig unbalanc data problem ensembl learn gener network learn segment pixel level get feedback discrimin network accord true posit true neg map hand refin network learn predict fals posit fals neg mask produc gener network signific valu especi medic applic final semant segment mask compos output three network propos architectur show state art result lit liver lesion segment two microscop cell segment dataset mda phc hela achiev competit result brat brain tumour segment less
89,1810.03868,"
        Numerous problems consisting in identifying vertices in graphs using distances are useful in domains such as network verification and graph isomorphism. Unifying them into a meta-problem may be of main interest. We introduce here a promising solution named Distance Identifying Set. The model contains Identifying Code (IC), Locating Dominating Set (LD) and their generalizations $r$-IC and $r$-LD where the closed neighborhood is considered up to distance $r$. It also contains Metric Dimension (MD) and its refinement $r$-MD in which the distance between two vertices is considered as infinite if the real distance exceeds $r$. Note that while IC = 1-IC and LD = 1-LD, we have MD = $\infty$-MD; we say that MD is not local
  In this article, we prove computational lower bounds for several problems included in Distance Identifying Set by providing generic reductions from (Planar) Hitting Set to the meta-problem. We mainly focus on two families of problem from the meta-problem: the first one, called bipartite gifted local, contains $r$-IC, $r$-LD and $r$-MD for each positive integer $r$ while the second one, called 1-layered, contains LD, MD and $r$-MD for each positive integer $r$. We have:
  - the 1-layered problems are NP-hard even in bipartite apex graphs,
  - the bipartite gifted local problems are NP-hard even in bipartite planar graphs,
  - assuming ETH, all these problems cannot be solved in $2^{o(\sqrt{n})}$ when restricted to bipartite planar or apex graph, respectively, and they cannot be solved in $2^{o(n)}$ on bipartite graphs,
  - even restricted to bipartite graphs, they do not admit parameterized algorithms in $2^{O(k)}.n^{O(1)}$ except if W[0] = W[2]. Here $k$ is the solution size of a relevant identifying set.
  In particular, Metric Dimension cannot be solved in $2^{o(n)}$ under ETH, answering a question of Hartung in 2013.
        △ Less
",numer problem consist identifi vertic graph use distanc use domain network verif graph isomorph unifi meta problem may main interest introduc promis solut name distanc identifi set model contain identifi code ic locat domin set ld gener r ic r ld close neighborhood consid distanc r also contain metric dimens md refin r md distanc two vertic consid infinit real distanc exce r note ic ic ld ld md infti md say md local articl prove comput lower bound sever problem includ distanc identifi set provid gener reduct planar hit set meta problem mainli focu two famili problem meta problem first one call bipartit gift local contain r ic r ld r md posit integ r second one call layer contain ld md r md posit integ r layer problem np hard even bipartit apex graph bipartit gift local problem np hard even bipartit planar graph assum eth problem cannot solv sqrt n restrict bipartit planar apex graph respect cannot solv n bipartit graph even restrict bipartit graph admit parameter algorithm k n except w w k solut size relev identifi set particular metric dimens cannot solv n eth answer question hartung less
90,1810.03867,"
        The performance of autonomous systems heavily relies on their ability to generate a robust representation of the environment. Deep neural networks have greatly improved vision-based perception systems but still fail in challenging situations, e.g. sensor outages or heavy weather. These failures are often introduced by data-inherent perturbations, which significantly reduce the information provided to the perception system. We propose a functionally modularized temporal filter, which stabilizes an abstract feature representation of a single-frame segmentation model using information of previous time steps. Our filter module splits the filter task into multiple less complex and more interpretable subtasks. The basic structure of the filter is inspired by a Bayes estimator consisting of a prediction and an update step. To make the prediction more transparent, we implement it using a geometric projection and estimate its parameters. This additionally enables the decomposition of the filter task into static representation filtering and low-dimensional motion filtering. Our model can cope with missing frames and is trainable in an end-to-end fashion. Using photorealistic, synthetic video data, we show the ability of the proposed architecture to overcome data-inherent perturbations. The experiments especially highlight advantages introduced by an interpretable and explicit filter module.
        △ Less
",perform autonom system heavili reli abil gener robust represent environ deep neural network greatli improv vision base percept system still fail challeng situat e g sensor outag heavi weather failur often introduc data inher perturb significantli reduc inform provid percept system propos function modular tempor filter stabil abstract featur represent singl frame segment model use inform previou time step filter modul split filter task multipl less complex interpret subtask basic structur filter inspir bay estim consist predict updat step make predict transpar implement use geometr project estim paramet addit enabl decomposit filter task static represent filter low dimension motion filter model cope miss frame trainabl end end fashion use photorealist synthet video data show abil propos architectur overcom data inher perturb experi especi highlight advantag introduc interpret explicit filter modul less
91,1810.03857,"
        In recent years, noticeable progress has been made in the development of quantum equipment, reflected through the number of successful demonstrations of Quantum Key Distribution (QKD) technology. Although they showcase the great achievements of QKD, many practical difficulties still need to be resolved. Inspired by the significant similarity between mobile ad-hoc networks and QKD technology, we propose a novel quality of service (QoS) model including new metrics for determining the states of public and quantum channels as well as a comprehensive metric of the QKD link. We also propose a novel routing protocol to achieve high-level scalability and minimize consumption of cryptographic keys. Given the limited mobility of nodes in QKD networks, our routing protocol uses the geographical distance and calculated link states to determine the optimal route. It also benefits from a caching mechanism and detection of returning loops to provide effective forwarding while minimizing key consumption and achieving the desired utilization of network links. Simulation results are presented to demonstrate the validity and accuracy of the proposed solutions.
        △ Less
",recent year notic progress made develop quantum equip reflect number success demonstr quantum key distribut qkd technolog although showcas great achiev qkd mani practic difficulti still need resolv inspir signific similar mobil ad hoc network qkd technolog propos novel qualiti servic qo model includ new metric determin state public quantum channel well comprehens metric qkd link also propos novel rout protocol achiev high level scalabl minim consumpt cryptograph key given limit mobil node qkd network rout protocol use geograph distanc calcul link state determin optim rout also benefit cach mechan detect return loop provid effect forward minim key consumpt achiev desir util network link simul result present demonstr valid accuraci propos solut less
92,1810.03856,"
        While objects from different categories can be reliably decoded from fMRI brain response patterns, it has proved more difficult to distinguish visually similar inputs, such as different instances of the same category. Here, we apply a recently developed deep learning system to the reconstruction of face images from human fMRI patterns. We trained a variational auto-encoder (VAE) neural network using a GAN (Generative Adversarial Network) unsupervised training procedure over a large dataset of celebrity faces. The auto-encoder latent space provides a meaningful, topologically organized 1024-dimensional description of each image. We then presented several thousand face images to human subjects, and learned a simple linear mapping between the multi-voxel fMRI activation patterns and the 1024 latent dimensions. Finally, we applied this mapping to novel test images, turning the obtained fMRI patterns into VAE latent codes, and ultimately the codes into face reconstructions. Qualitative and quantitative evaluation of the reconstructions revealed robust pairwise decoding (>95% correct), and a strong improvement relative to a baseline model (PCA decomposition). Furthermore, this brain decoding model can readily be recycled to probe human face perception along many dimensions of interest; for example, the technique allowed for accurate gender classification, and even to decode which face was imagined, rather than seen by the subject.
        △ Less
",object differ categori reliabl decod fmri brain respons pattern prove difficult distinguish visual similar input differ instanc categori appli recent develop deep learn system reconstruct face imag human fmri pattern train variat auto encod vae neural network use gan gener adversari network unsupervis train procedur larg dataset celebr face auto encod latent space provid meaning topolog organ dimension descript imag present sever thousand face imag human subject learn simpl linear map multi voxel fmri activ pattern latent dimens final appli map novel test imag turn obtain fmri pattern vae latent code ultim code face reconstruct qualit quantit evalu reconstruct reveal robust pairwis decod correct strong improv rel baselin model pca decomposit furthermor brain decod model readili recycl probe human face percept along mani dimens interest exampl techniqu allow accur gender classif even decod face imagin rather seen subject less
93,1810.03851,"
        Visual attention, derived from cognitive neuroscience, facilitates human perception on the most pertinent subset of the sensory data. Recently, significant efforts have been made to exploit attention schemes to advance computer vision systems. For visual tracking, it is often challenging to track target objects undergoing large appearance changes. Attention maps facilitate visual tracking by selectively paying attention to temporal robust features. Existing tracking-by-detection approaches mainly use additional attention modules to generate feature weights as the classifiers are not equipped with such mechanisms. In this paper, we propose a reciprocative learning algorithm to exploit visual attention for training deep classifiers. The proposed algorithm consists of feed-forward and backward operations to generate attention maps, which serve as regularization terms coupled with the original classification loss function for training. The deep classifier learns to attend to the regions of target objects robust to appearance changes. Extensive experiments on large-scale benchmark datasets show that the proposed attentive tracking method performs favorably against the state-of-the-art approaches.
        △ Less
",visual attent deriv cognit neurosci facilit human percept pertin subset sensori data recent signific effort made exploit attent scheme advanc comput vision system visual track often challeng track target object undergo larg appear chang attent map facilit visual track select pay attent tempor robust featur exist track detect approach mainli use addit attent modul gener featur weight classifi equip mechan paper propos reciproc learn algorithm exploit visual attent train deep classifi propos algorithm consist feed forward backward oper gener attent map serv regular term coupl origin classif loss function train deep classifi learn attend region target object robust appear chang extens experi larg scale benchmark dataset show propos attent track method perform favor state art approach less
94,1810.03842,"
        Humans and animals are believed to use a very minimal set of trajectories to perform a wide variety of tasks including walking. Our main objective in this paper is two fold 1) Obtain an effective tool to realize these basic motion patterns for quadrupedal walking, called the kinematic motion primitives (kMPs), via trajectories learned from deep reinforcement learning (D-RL) and 2) Realize a set of behaviors, namely trot, walk, gallop and bound from these kinematic motion primitives in our custom four legged robot, called the 'Stoch'. D-RL is a data driven approach, which has been shown to be very effective for realizing all kinds of robust locomotion behaviors, both in simulation and in experiment. On the other hand, kMPs are known to capture the underlying structure of walking and yield a set of derived behaviors. We first generate walking gaits from D-RL, which uses policy gradient based approaches. We then analyze the resulting walking by using principal component analysis. We observe that the kMPs extracted from PCA followed a similar pattern irrespective of the type of gaits generated. Leveraging on this underlying structure, we then realize walking in Stoch by a straightforward reconstruction of joint trajectories from kMPs. This type of methodology improves the transferability of these gaits to real hardware, lowers the computational overhead on-board, and also avoids multiple training iterations by generating a set of derived behaviors from a single learned gait.
        △ Less
",human anim believ use minim set trajectori perform wide varieti task includ walk main object paper two fold obtain effect tool realiz basic motion pattern quadruped walk call kinemat motion primit kmp via trajectori learn deep reinforc learn rl realiz set behavior name trot walk gallop bound kinemat motion primit custom four leg robot call stoch rl data driven approach shown effect realiz kind robust locomot behavior simul experi hand kmp known captur underli structur walk yield set deriv behavior first gener walk gait rl use polici gradient base approach analyz result walk use princip compon analysi observ kmp extract pca follow similar pattern irrespect type gait gener leverag underli structur realiz walk stoch straightforward reconstruct joint trajectori kmp type methodolog improv transfer gait real hardwar lower comput overhead board also avoid multipl train iter gener set deriv behavior singl learn gait less
95,1810.03824,"
        A large number of services for research data management strive to adhere to the FAIR guiding principles for scientific data management and stewardship. To evaluate these services and to indicate possible improvements, use-case-centric metrics are needed as an addendum to existing metric frameworks. The retrieval of spatially and temporally annotated images can exemplify such a use case. The prototypical implementation indicates that currently no research data repository achieves the full score. Suggestions on how to increase the score include automatic annotation based on the metadata inside the image file and support for content negotiation to retrieve the images. These and other insights can lead to an improvement of data integration workflows, resulting in a better and more FAIR approach to manage research data.
        △ Less
",larg number servic research data manag strive adher fair guid principl scientif data manag stewardship evalu servic indic possibl improv use case centric metric need addendum exist metric framework retriev spatial tempor annot imag exemplifi use case prototyp implement indic current research data repositori achiev full score suggest increas score includ automat annot base metadata insid imag file support content negoti retriev imag insight lead improv data integr workflow result better fair approach manag research data less
96,1810.03822,"
        Based on software-defined principles, we propose a holistic architecture for Cyberphysical Systems (CPS) and Internet of Things (IoT) applications, and highlight the merits pertaining to scalability, flexibility, robustness, interoperability, and cyber security. Our design especially capitalizes on the computational units possessed by smart agents, which may be utilized for decentralized control and in-network data processing. We characterize the data flow, communication flow, and control flow that assimilate a set of components such as sensors, actuators, controllers, and coordinators in a systemic programmable fashion. We specifically aim for distributed and decentralized decision-making by spreading the control over several hierarchical layers. In addition, we propose a middleware layer to encapsulate units and services for time-critical operations in highly dynamic environments. We further enlist a multitude of vulnerabilities to cyberattacks, and integrate software-defined solutions for enabling resilience, detection, and recovery. In this purview, several controllers cooperate to identify and respond to security threats and abnormal situations in a self-adjusting manner. Last, we illustrate numerical simulations in support of the virtues of a software-defined design for CPS and IoT.
        △ Less
",base softwar defin principl propos holist architectur cyberphys system cp internet thing iot applic highlight merit pertain scalabl flexibl robust interoper cyber secur design especi capit comput unit possess smart agent may util decentr control network data process character data flow commun flow control flow assimil set compon sensor actuat control coordin system programm fashion specif aim distribut decentr decis make spread control sever hierarch layer addit propos middlewar layer encapsul unit servic time critic oper highli dynam environ enlist multitud vulner cyberattack integr softwar defin solut enabl resili detect recoveri purview sever control cooper identifi respond secur threat abnorm situat self adjust manner last illustr numer simul support virtu softwar defin design cp iot less
97,1810.03821,"
        Attention mechanisms have been widely used in Visual Question Answering (VQA) solutions due to their capacity to model deep cross-domain interactions. Analyzing attention maps offers us a perspective to find out limitations of current VQA systems and an opportunity to further improve them. In this paper, we select two state-of-the-art VQA approaches with attention mechanisms to study their robustness and disadvantages by visualizing and analyzing their estimated attention maps. We find that both methods are sensitive to features, and simultaneously, they perform badly for counting and multi-object related questions. We believe that the findings and analytical method will help researchers identify crucial challenges on the way to improve their own VQA systems.
        △ Less
",attent mechan wide use visual question answer vqa solut due capac model deep cross domain interact analyz attent map offer us perspect find limit current vqa system opportun improv paper select two state art vqa approach attent mechan studi robust disadvantag visual analyz estim attent map find method sensit featur simultan perform badli count multi object relat question believ find analyt method help research identifi crucial challeng way improv vqa system less
98,1810.03817,"
        Nonlinear kernels can be approximated using finite-dimensional feature maps for efficient risk minimization. Due to the inherent trade-off between the dimension of the (mapped) feature space and the approximation accuracy, the key problem is to identify promising (explicit) features leading to a satisfactory out-of-sample performance. In this work, we tackle this problem by efficiently choosing such features from multiple kernels in a greedy fashion. Our method sequentially selects these explicit features from a set of candidate features using a correlation metric. We establish an out-of-sample error bound capturing the trade-off between the error in terms of explicit features (approximation error) and the error due to spectral properties of the best model in the Hilbert space associated to the combined kernel (spectral error). The result verifies that when the (best) underlying data model is sparse enough, i.e., the spectral error is negligible, one can control the test error with a small number of explicit features, that can scale poly-logarithmically with data. Our empirical results show that given a fixed number of explicit features, the method can achieve a lower test error with a smaller time cost, compared to the state-of-the-art in data-dependent random features.
        △ Less
",nonlinear kernel approxim use finit dimension featur map effici risk minim due inher trade dimens map featur space approxim accuraci key problem identifi promis explicit featur lead satisfactori sampl perform work tackl problem effici choos featur multipl kernel greedi fashion method sequenti select explicit featur set candid featur use correl metric establish sampl error bound captur trade error term explicit featur approxim error error due spectral properti best model hilbert space associ combin kernel spectral error result verifi best underli data model spars enough e spectral error neglig one control test error small number explicit featur scale poli logarithm data empir result show given fix number explicit featur method achiev lower test error smaller time cost compar state art data depend random featur less
99,1810.03813,"
        We consider the problem of maximizing the sum of a monotone submodular function and a linear function subject to a general solvable polytope constraint. Recently, Sviridenko et al. (2017) described an algorithm for this problem whose approximation guarantee is optimal in some intuitive and formal senses. Unfortunately, this algorithm involves a guessing step which makes it less clean and significantly affects its time complexity. In this work we describe a clean alternative algorithm that uses a novel weighting technique in order to avoid the problematic guessing step while keeping the same approximation guarantee as the algorithm of Sviridenko et al.
        △ Less
",consid problem maxim sum monoton submodular function linear function subject gener solvabl polytop constraint recent sviridenko et al describ algorithm problem whose approxim guarante optim intuit formal sens unfortun algorithm involv guess step make less clean significantli affect time complex work describ clean altern algorithm use novel weight techniqu order avoid problemat guess step keep approxim guarante algorithm sviridenko et al less
100,1810.03811,"
        In this work we investigate into energy complexity, a Boolean function measure related to circuit complexity. Given a circuit $\mathcal{C}$ over the standard basis $\{\vee_2,\wedge_2,\neg\}$, the energy complexity of $\mathcal{C}$, denoted by $\mathrm{EC}(\mathcal{C})$, is the maximum number of its activated inner gates over all inputs. The energy complexity of a Boolean function $f$, denoted by $\mathrm{EC}(f)$, is the minimum of $\mathrm{EC}(\mathcal{C})$ over all circuits $\mathcal{C}$ computing $f$. This concept has attracted lots of attention in literature. Recently, Denish, Otiv, and Sarma [COCOON'18] gave $\mathrm{EC}(f)$ an upper bound in terms of the decision tree complexity, $\mathrm{EC}(f)=O(\mathrm{D}(f)^3)$. They also showed that $\mathrm{EC}(f)\leq 3n-1$, where $n$ is the input size. Recall that the minimum size of circuit to compute $f$ could be as large as $2^n/n$. We improve their upper bounds by showing that $\mathrm{EC}(f)\leq\min\{\frac12\mathrm{D}(f)^2+O(\mathrm{D}(f)),n+2\mathrm{D}(f)-2\}$. For the lower bound, Denish, Otiv, and Sarma defined positive sensitivity, a complexity measure denoted by $\mathrm{psens}(f)$, and showed that $\mathrm{EC}(f)\ge\frac{1}{3}\mathrm{psens}(f)$. They asked whether $\mathrm{EC}(f)$ can also be lower bounded by a polynomial of $\mathrm{D}(f)$. In this paper we affirm it by proving $\mathrm{EC}(f)=Ω(\sqrt{\mathrm{D}(f)})$. For non-degenerated functions with input size $n$, we give another lower bound $\mathrm{EC}(f)=Ω(\log{n})$. All these three lower bounds are incomparable to each other. Besides, we also examine the energy complexity of $\mathtt{OR}$ functions and $\mathtt{ADDRESS}$ functions, which implies the tightness of our two lower bounds respectively. In addition, the former one answers another open question asking for a non-trivial lower bounds for the energy complexity of $\mathtt{OR}$ functions.
        △ Less
",work investig energi complex boolean function measur relat circuit complex given circuit mathcal c standard basi vee wedg neg energi complex mathcal c denot mathrm ec mathcal c maximum number activ inner gate input energi complex boolean function f denot mathrm ec f minimum mathrm ec mathcal c circuit mathcal c comput f concept attract lot attent literatur recent denish otiv sarma cocoon gave mathrm ec f upper bound term decis tree complex mathrm ec f mathrm f also show mathrm ec f leq n n input size recal minimum size circuit comput f could larg n n improv upper bound show mathrm ec f leq min frac mathrm f mathrm f n mathrm f lower bound denish otiv sarma defin posit sensit complex measur denot mathrm psen f show mathrm ec f ge frac mathrm psen f ask whether mathrm ec f also lower bound polynomi mathrm f paper affirm prove mathrm ec f sqrt mathrm f non degener function input size n give anoth lower bound mathrm ec f log n three lower bound incompar besid also examin energi complex mathtt function mathtt address function impli tight two lower bound respect addit former one answer anoth open question ask non trivial lower bound energi complex mathtt function less
101,1810.03808,"
        An Implantable Cardioverter Defibrillator (ICD) is a medical device used for the detection of potentially fatal cardiac arrhythmia and their treatment through the delivery of electrical shocks intended to restore normal heart rhythm. An ICD reprogramming attack seeks to alter the device's parameters to induce unnecessary shocks and, even more egregious, prevent required therapy. In this paper, we present a formal approach for the synthesis of ICD reprogramming attacks that are both effective, i.e., lead to fundamental changes in the required therapy, and stealthy, i.e., involve minimal changes to the nominal ICD parameters. We focus on the discrimination algorithm underlying Boston Scientific devices (one of the principal ICD manufacturers) and formulate the synthesis problem as one of multi-objective optimization. Our solution technique is based on an Optimization Modulo Theories encoding of the problem and allows us to derive device parameters that are optimal with respect to the effectiveness-stealthiness tradeoff (i.e., lie along the corresponding Pareto front). To the best of our knowledge, our work is the first to derive systematic ICD reprogramming attacks designed to maximize therapy disruption while minimizing detection. To evaluate our technique, we employ an extensive dataset of synthetic EGMs (cardiac signals), each generated with a prescribed arrhythmia, allowing us to synthesize attacks tailored to the victim's cardiac condition. Our approach readily generalizes to unseen signals, representing the unknown EGM of the victim patient.
        △ Less
",implant cardiovert defibril icd medic devic use detect potenti fatal cardiac arrhythmia treatment deliveri electr shock intend restor normal heart rhythm icd reprogram attack seek alter devic paramet induc unnecessari shock even egregi prevent requir therapi paper present formal approach synthesi icd reprogram attack effect e lead fundament chang requir therapi stealthi e involv minim chang nomin icd paramet focu discrimin algorithm underli boston scientif devic one princip icd manufactur formul synthesi problem one multi object optim solut techniqu base optim modulo theori encod problem allow us deriv devic paramet optim respect effect stealthi tradeoff e lie along correspond pareto front best knowledg work first deriv systemat icd reprogram attack design maxim therapi disrupt minim detect evalu techniqu employ extens dataset synthet egm cardiac signal gener prescrib arrhythmia allow us synthes attack tailor victim cardiac condit approach readili gener unseen signal repres unknown egm victim patient less
102,1810.03806,"
        Many deep learning models are vulnerable to the adversarial attack, i.e., imperceptible but intentionally-designed perturbations to the input can cause incorrect output of the networks. In this paper, using information geometry, we provide a reasonable explanation for the vulnerability of deep learning models. By considering the data space as a non-linear space with the Fisher information metric induced from a neural network, we first propose an adversarial attack algorithm termed one-step spectral attack (OSSA). The method is described by a constrained quadratic form of the Fisher information matrix, where the optimal adversarial perturbation is given by the first eigenvector, and the model vulnerability is reflected by the eigenvalues. The larger an eigenvalue is, the more vulnerable the model is to be attacked by the corresponding eigenvector. Taking advantage of the property, we also propose an adversarial detection method with the eigenvalues serving as characteristics. Both our attack and detection algorithms are numerically optimized to work efficiently on large datasets. Our evaluations show superior performance compared with other methods, implying that the Fisher information is a promising approach to investigate the adversarial attacks and defenses.
        △ Less
",mani deep learn model vulner adversari attack e impercept intent design perturb input caus incorrect output network paper use inform geometri provid reason explan vulner deep learn model consid data space non linear space fisher inform metric induc neural network first propos adversari attack algorithm term one step spectral attack ossa method describ constrain quadrat form fisher inform matrix optim adversari perturb given first eigenvector model vulner reflect eigenvalu larger eigenvalu vulner model attack correspond eigenvector take advantag properti also propos adversari detect method eigenvalu serv characterist attack detect algorithm numer optim work effici larg dataset evalu show superior perform compar method impli fisher inform promis approach investig adversari attack defens less
103,1810.03805,"
        Local explanation frameworks aim to rationalize particular decisions made by a black-box prediction model. Existing techniques are often restricted to a specific type of predictor or based on input saliency, which may be undesirably sensitive to factors unrelated to the model's decision making process. We instead propose sufficient input subsets that identify minimal subsets of features whose observed values alone suffice for the same decision to be reached, even if all other input feature values are missing. General principles that globally govern a model's decision-making can also be revealed by searching for clusters of such input patterns across many data points. Our approach is conceptually straightforward, entirely model-agnostic, simply implemented using instance-wise backward selection, and able to produce more concise rationales than existing techniques. We demonstrate the utility of our interpretation method on various neural network models trained on text, image, and genomic data.
        △ Less
",local explan framework aim ration particular decis made black box predict model exist techniqu often restrict specif type predictor base input salienc may undesir sensit factor unrel model decis make process instead propos suffici input subset identifi minim subset featur whose observ valu alon suffic decis reach even input featur valu miss gener principl global govern model decis make also reveal search cluster input pattern across mani data point approach conceptu straightforward entir model agnost simpli implement use instanc wise backward select abl produc concis rational exist techniqu demonstr util interpret method variou neural network model train text imag genom data less
104,1810.03798,"
        In this paper, we show that feedforward and recurrent neural networks exhibit an outer product derivative structure but that convolutional neural networks do not. This structure makes it possible to use higher-order information without needing approximations or infeasibly large amounts of memory, and it may also provide insights into the geometry of neural network optima. The ability to easily access these derivatives also suggests a new, geometric approach to regularization. We then discuss how this structure could be used to improve training methods, increase network robustness and generalizability, and inform network compression methods.
        △ Less
",paper show feedforward recurr neural network exhibit outer product deriv structur convolut neural network structur make possibl use higher order inform without need approxim infeas larg amount memori may also provid insight geometri neural network optima abil easili access deriv also suggest new geometr approach regular discuss structur could use improv train method increas network robust generaliz inform network compress method less
105,1810.03793,"
        The Iterated Prisoner's Dilemma (IPD) can model transactions among individuals in a population, including human cooperation and trust. Research has focused on equilibrium solutions, mutual cooperation and mutual defection, in IPD. The non-equilibrium strategy profile that one player cooperates and the opponent defects is considered to be unstable and temporal. In this research, we show that non-equilibrium strategies can also be dominant in spatial IPD. An example of such non-equilibrium strategy is a so-called Collective Strategy with Master-Slave Mechanism (CSMSM). A CSMSM identifies the opponent by playing a fixed sequence of moves in the first five rounds of IPD. If the opponent has played the same sequence of moves, it is identified as a kin member. Otherwise it is identified as non-kin. A CSMSM always defects against non-kins. There are two roles in the CSMSM group, master and slave. Every CSMSM acts as a master initially. A master may change to a slave with a predefined probability in each generation. When two CSMSMs meet, they will cooperate with each other if both are masters. In the case that one is a master and another is a slave, the slave will cooperate and the master will defect so that the master's payoff is maximized. Simulation results show that CSMSM outperforms well known strategies like Tit-For-Tat and Always Defect in spatial IPD even if there are only a small ratio of CSMSM in the initial population.
        △ Less
",iter prison dilemma ipd model transact among individu popul includ human cooper trust research focus equilibrium solut mutual cooper mutual defect ipd non equilibrium strategi profil one player cooper oppon defect consid unstabl tempor research show non equilibrium strategi also domin spatial ipd exampl non equilibrium strategi call collect strategi master slave mechan csmsm csmsm identifi oppon play fix sequenc move first five round ipd oppon play sequenc move identifi kin member otherwis identifi non kin csmsm alway defect non kin two role csmsm group master slave everi csmsm act master initi master may chang slave predefin probabl gener two csmsm meet cooper master case one master anoth slave slave cooper master defect master payoff maxim simul result show csmsm outperform well known strategi like tit tat alway defect spatial ipd even small ratio csmsm initi popul less
106,1810.03792,"
        In Maximum $k$-Vertex Cover (Max $k$-VC), the input is an edge-weighted graph $G$ and an integer $k$, and the goal is to find a subset $S$ of $k$ vertices that maximizes the total weight of edges covered by $S$. Here we say that an edge is covered by $S$ iff at least one of its endpoints lies in $S$.
  We present an FPT approximation scheme (FPT-AS) that runs in $(1/ε)^{O(k)} poly(n)$ time for the problem, which improves upon Gupta et al.'s $(k/ε)^{O(k)} poly(n)$-time FPT-AS [SODA'18, FOCS'18]. Our algorithm is simple: just use brute force to find the best $k$-vertex subset among the $O(k/ε)$ vertices with maximum weighted degrees.
  Our algorithm naturally yields an efficient approximate kernelization scheme of $O(k/ε)$ vertices; previously, an $O(k^5/ε^2)$-vertex approximate kernel is only known for the unweighted version of Max $k$-VC [Lokshtanov et al., STOC'17]. Interestingly, this has an application outside of parameterized complexity: using our approximate kernelization as a preprocessing step, we can directly apply Raghavendra and Tan's SDP-based algorithm for 2SAT with cardinality constraint [SODA'12] to give an $0.92$-approximation for Max $k$-VC in polynomial time. This improves upon Feige and Langberg's algorithm [J. Algorithms'01] which yields $(0.75 + δ)$-approximation for some (unspecified) constant $δ> 0$.
  We also consider the minimization version (Min $k$-VC), where the goal is to minimize the total weight of edges covered by $S$. We provide an FPT-AS for Min $k$-VC with similar running time of $(1/ε)^{O(k)} poly(n)$, which again improves on a $(k/ε)^{O(k)} poly(n)$-time FPT-AS of Gupta et al. On the other hand, we show that there is unlikely a polynomial size approximate kernelization for Min $k$-VC for any factor less than two.
        △ Less
",maximum k vertex cover max k vc input edg weight graph g integ k goal find subset k vertic maxim total weight edg cover say edg cover iff least one endpoint lie present fpt approxim scheme fpt run k poli n time problem improv upon gupta et al k k poli n time fpt soda foc algorithm simpl use brute forc find best k vertex subset among k vertic maximum weight degre algorithm natur yield effici approxim kernel scheme k vertic previous k vertex approxim kernel known unweight version max k vc lokshtanov et al stoc interestingli applic outsid parameter complex use approxim kernel preprocess step directli appli raghavendra tan sdp base algorithm sat cardin constraint soda give approxim max k vc polynomi time improv upon feig langberg algorithm j algorithm yield approxim unspecifi constant also consid minim version min k vc goal minim total weight edg cover provid fpt min k vc similar run time k poli n improv k k poli n time fpt gupta et al hand show unlik polynomi size approxim kernel min k vc factor less two less
107,1810.03790,"
        On the off-the-shelf navigational assistance devices, the localization precision is limited to the signal error of global navigation satellite system (GNSS). During travelling outdoors, the inaccurately localization perplexes visually impaired people, especially at key positions, such as gates, bus stations or intersections. The visual localization is a feasible approach to improving the positioning precision of assistive devices. Using multiple image descriptors, the paper proposes a robust and efficient visual localization algorithm, which takes advantage of priori GNSS signals and multi-modal images to achieve the accurate localization of key positions. In the experiments, we implement the approach on the wearable system and test the performance of visual localization under practical scenarios.
        △ Less
",shelf navig assist devic local precis limit signal error global navig satellit system gnss travel outdoor inaccur local perplex visual impair peopl especi key posit gate bu station intersect visual local feasibl approach improv posit precis assist devic use multipl imag descriptor paper propos robust effici visual local algorithm take advantag priori gnss signal multi modal imag achiev accur local key posit experi implement approach wearabl system test perform visual local practic scenario less
108,1810.03783,"
        Unsupervised online video object segmentation (VOS) aims to automatically segment the moving objects over an unconstrained video without the requirements of any prior information about the objects or camera motion. It is therefore a very challenging problem for high-level video analysis. So far, limited number of such methods have been reported in literature and most of them still have distance to a satisfactory performance. Targeting this challenging problem,in this paper, we propose a novel unsupervised online VOS framework by understanding the motion property as the meaning of \emph{moving} in concurrence with \emph{a generic object} for the segmented regions. By incorporating \emph{salient motion detection} and \emph{object proposal}, a pixel-wise fusion strategy is developed to effectively remove detection noises such as background movements and stationary objects. Furthermore, by leveraging the obtained segmentation from immediately preceding frames, a forward propagation algorithm is proposed to deal with the unreliable motion detection and object proposals.
  Experimental results on DAVIS-2016 and SegTrack-v2 benchmark dataset show that the proposed method outperforms the other state-of-the-art unsupervised online segmentation by achieving 5.6\% absolute improvement at least, and additionally even achieves a better performance than the best unsupervised offline method on DAVIS-2016 dataset. Another significant advantage also need to be addressed that in all the experiments, there is only one existing trained model for object proposal (Mask RCNN on COCO dataset) being used without any fine-tuning, which is the demonstration of robustness. The most contribution of this work might sheds light on the potential and to motivate more VOS framework studies based on characteristic motion properties.
        △ Less
",unsupervis onlin video object segment vo aim automat segment move object unconstrain video without requir prior inform object camera motion therefor challeng problem high level video analysi far limit number method report literatur still distanc satisfactori perform target challeng problem paper propos novel unsupervis onlin vo framework understand motion properti mean emph move concurr emph gener object segment region incorpor emph salient motion detect emph object propos pixel wise fusion strategi develop effect remov detect nois background movement stationari object furthermor leverag obtain segment immedi preced frame forward propag algorithm propos deal unreli motion detect object propos experiment result davi segtrack v benchmark dataset show propos method outperform state art unsupervis onlin segment achiev absolut improv least addit even achiev better perform best unsupervis offlin method davi dataset anoth signific advantag also need address experi one exist train model object propos mask rcnn coco dataset use without fine tune demonstr robust contribut work might shed light potenti motiv vo framework studi base characterist motion properti less
109,1810.03779,"
        In many reinforcement learning tasks, the goal is to learn a policy to manipulate an agent, whose design is fixed, to maximize some notion of cumulative reward. The design of the agent's physical structure is rarely optimized for the task at hand. In this work, we explore the possibility of learning a version of the agent's design that is better suited for its task, jointly with the policy. We propose a minor alteration to the OpenAI Gym framework, where we parameterize parts of an environment, and allow an agent to jointly learn to modify these environment parameters along with its policy. We demonstrate that an agent can learn a better structure of its body that is not only better suited for the task, but also facilitates policy learning. Joint learning of policy and structure may even uncover design principles that are useful for assisted-design applications. Videos of results at https://designrl.github.io/
        △ Less
",mani reinforc learn task goal learn polici manipul agent whose design fix maxim notion cumul reward design agent physic structur rare optim task hand work explor possibl learn version agent design better suit task jointli polici propos minor alter openai gym framework parameter part environ allow agent jointli learn modifi environ paramet along polici demonstr agent learn better structur bodi better suit task also facilit polici learn joint learn polici structur may even uncov design principl use assist design applic video result http designrl github io less
110,1810.03774,"
        This paper presents a method which can track and 3D reconstruct the non-rigid surface motion of human performance using a moving RGB-D camera. 3D reconstruction of marker-less human performance is a challenging problem due to the large range of articulated motions and considerable non-rigid deformations. Current approaches use local optimization for tracking. These methods need many iterations to converge and may get stuck in local minima during sudden articulated movements. We propose a puppet model-based tracking approach using skeleton prior, which provides a better initialization for tracking articulated movements. The proposed approach uses an aligned puppet model to estimate correct correspondences for human performance capture. We also contribute a synthetic dataset which provides ground truth locations for frame-by-frame geometry and skeleton joints of human subjects. Experimental results show that our approach is more robust when faced with sudden articulated motions, and provides better 3D reconstruction compared to the existing state-of-the-art approaches.
        △ Less
",paper present method track reconstruct non rigid surfac motion human perform use move rgb camera reconstruct marker less human perform challeng problem due larg rang articul motion consider non rigid deform current approach use local optim track method need mani iter converg may get stuck local minima sudden articul movement propos puppet model base track approach use skeleton prior provid better initi track articul movement propos approach use align puppet model estim correct correspond human perform captur also contribut synthet dataset provid ground truth locat frame frame geometri skeleton joint human subject experiment result show approach robust face sudden articul motion provid better reconstruct compar exist state art approach less
111,1810.03773,"
        Adversarial robustness has become an important research topic given empirical demonstrations on the lack of robustness of deep neural networks. Unfortunately, recent theoretical results suggest that adversarial training induces a strict tradeoff between classification accuracy and adversarial robustness. In this paper, we propose and then study a new regularization for any margin classifier or deep neural network. We motivate this regularization by a novel generalization bound that shows a tradeoff in classifier accuracy between maximizing its margin and average margin. We thus call our approach an average margin (AM) regularization, and it consists of a linear term added to the objective. We theoretically show that for certain distributions AM regularization can both improve classifier accuracy and robustness to adversarial attacks. We conclude by using both synthetic and real data to empirically show that AM regularization can strictly improve both accuracy and robustness for support vector machine's (SVM's) and deep neural networks, relative to unregularized classifiers and adversarially trained classifiers.
        △ Less
",adversari robust becom import research topic given empir demonstr lack robust deep neural network unfortun recent theoret result suggest adversari train induc strict tradeoff classif accuraci adversari robust paper propos studi new regular margin classifi deep neural network motiv regular novel gener bound show tradeoff classifi accuraci maxim margin averag margin thu call approach averag margin regular consist linear term ad object theoret show certain distribut regular improv classifi accuraci robust adversari attack conclud use synthet real data empir show regular strictli improv accuraci robust support vector machin svm deep neural network rel unregular classifi adversari train classifi less
112,1810.03772,"
        We solve the problem of existence of perfect codes in the Doob graph. It is shown that 1-perfect codes in the Doob graph D(m,n) exist if and only if 6m+3n+1 is a power of 2; that is, if the size of a 1-ball divides the number of vertices.
        △ Less
",solv problem exist perfect code doob graph shown perfect code doob graph n exist n power size ball divid number vertic less
113,1810.03767,"
        In this work, we present a new framework for the stylization of text-based binary images. First, our method stylizes the stroke-based geometric shape like text, symbols and icons in the target binary image based on an input style image. Second, the composition of the stylized geometric shape and a background image is explored. To accomplish the task, we propose legibility-preserving structure and texture transfer algorithms, which progressively narrow the visual differences between the binary image and the style image. The stylization is then followed by a context-aware layout design algorithm, where cues for both seamlessness and aesthetics are employed to determine the optimal layout of the shape in the background. Given the layout, the binary image is seamlessly embedded into the background by texture synthesis under a context-aware boundary constraint. According to the contents of binary images, our method can be applied to many fields. We show that the proposed method is capable of addressing the unsupervised text stylization problem and is superior to state-of-the-art style transfer methods in automatic artistic typography creation. Besides, extensive experiments on various tasks, such as visual-textual presentation synthesis, icon/symbol rendering and structure-guided image inpainting, demonstrate the effectiveness of the proposed method.
        △ Less
",work present new framework styliz text base binari imag first method styliz stroke base geometr shape like text symbol icon target binari imag base input style imag second composit styliz geometr shape background imag explor accomplish task propos legibl preserv structur textur transfer algorithm progress narrow visual differ binari imag style imag styliz follow context awar layout design algorithm cue seamless aesthet employ determin optim layout shape background given layout binari imag seamlessli embed background textur synthesi context awar boundari constraint accord content binari imag method appli mani field show propos method capabl address unsupervis text styliz problem superior state art style transfer method automat artist typographi creation besid extens experi variou task visual textual present synthesi icon symbol render structur guid imag inpaint demonstr effect propos method less
114,1810.03764,"
        The Generator of a Generative Adversarial Network (GAN) is trained to transform latent vectors drawn from a prior distribution into realistic looking photos. These latent vectors have been shown to encode information about the content of their corresponding images. Projecting input images onto the latent space of a GAN is non-trivial, but previous work has successfully performed this task for latent spaces with a uniform prior. We extend these techniques to latent spaces with a Gaussian prior, and demonstrate our technique's effectiveness.
        △ Less
",gener gener adversari network gan train transform latent vector drawn prior distribut realist look photo latent vector shown encod inform content correspond imag project input imag onto latent space gan non trivial previou work success perform task latent space uniform prior extend techniqu latent space gaussian prior demonstr techniqu effect less
115,1810.03762,"
        This volume contains the proceedings of the Fifteenth International Workshop on the ACL2 Theorem Prover and Its Applications (ACL2-2018), a two-day workshop held in Austin, Texas, USA, on November 5-6, 2018, immediately after FMCAD'18.  The proceedings of ACL2-2018 include eleven long papers and two extended abstracts.
        △ Less
",volum contain proceed fifteenth intern workshop acl theorem prover applic acl two day workshop held austin texa usa novemb immedi fmcad proceed acl includ eleven long paper two extend abstract less
116,1810.03758,"
        This document summarizes the 4th International Workshop on Recovering 6D Object Pose which was organized in conjunction with ECCV 2018 in Munich. The workshop featured four invited talks, oral and poster presentations of accepted workshop papers, and an introduction of the BOP benchmark for 6D object pose estimation. The workshop was attended by 100+ people working on relevant topics in both academia and industry who shared up-to-date advances and discussed open problems.
        △ Less
",document summar th intern workshop recov object pose organ conjunct eccv munich workshop featur four invit talk oral poster present accept workshop paper introduct bop benchmark object pose estim workshop attend peopl work relev topic academia industri share date advanc discuss open problem less
117,1810.03756,"
        Deep Learning for Computer Vision depends mainly on the source of supervision.Photo-realistic simulators can generate large-scale automatically labeled syntheticdata, but introduce a domain gap negatively impacting performance. We propose anew unsupervised domain adaptation algorithm, called SPIGAN, relying on Sim-ulator Privileged Information (PI) and Generative Adversarial Networks (GAN).We use internal data from the simulator as PI during the training of a target tasknetwork. We experimentally evaluate our approach on semantic segmentation. Wetrain the networks on real-world Cityscapes and Vistas datasets, using only unla-beled real-world images and synthetic labeled data with z-buffer (depth) PI fromthe SYNTHIA dataset. Our method improves over no adaptation and state-of-the-art unsupervised domain adaptation techniques.
        △ Less
",deep learn comput vision depend mainli sourc supervis photo realist simul gener larg scale automat label syntheticdata introduc domain gap neg impact perform propos anew unsupervis domain adapt algorithm call spigan reli sim ulat privileg inform pi gener adversari network gan use intern data simul pi train target tasknetwork experiment evalu approach semant segment wetrain network real world cityscap vista dataset use unla bele real world imag synthet label data z buffer depth pi fromth synthia dataset method improv adapt state art unsupervis domain adapt techniqu less
118,1810.03755,"
        Millimeter-wave communications rely on narrow-beam transmissions to cope with the strong signal attenuation at these frequencies, thus demanding precise alignment between transmitter and receiver. However, the beam-alignment procedure may entail a huge overhead and its performance may be degraded by detection errors. This paper proposes a coded energy-efficient beam-alignment scheme, robust against detection errors. Specifically, the beam-alignment sequence is designed such that the error-free feedback sequences are generated from a codebook with the desired error correction capabilities. Therefore, in the presence of detection errors, the error-free feedback sequences can be recovered with high probability. The assignment of beams to codewords is designed to optimize energy efficiency, and a water-filling solution is proved. The numerical results with analog beams depict up to 4dB and 8dB gains over exhaustive and uncoded beam-alignment schemes, respectively.
        △ Less
",millimet wave commun reli narrow beam transmiss cope strong signal attenu frequenc thu demand precis align transmitt receiv howev beam align procedur may entail huge overhead perform may degrad detect error paper propos code energi effici beam align scheme robust detect error specif beam align sequenc design error free feedback sequenc gener codebook desir error correct capabl therefor presenc detect error error free feedback sequenc recov high probabl assign beam codeword design optim energi effici water fill solut prove numer result analog beam depict db db gain exhaust uncod beam align scheme respect less
119,1810.03749,"
        Sampling efficiency in a highly constrained environment has long been a major challenge for sampling-based planners. In this work, we propose Rapidly-exploring Random disjointed-Trees* (RRdT*), an incremental optimal multi-query planner. RRdT* uses multiple disjointed-trees to exploit local-connectivity of spaces via Markov Chain random sampling, which utilises neighbourhood information derived from previous successful and failed samples. To balance local exploitation, RRdT* actively explore unseen global spaces when local-connectivity exploitation is unsuccessful. The active trade-off between local exploitation and global exploration is formulated as a multi-armed bandit problem. We argue that the active balancing of global exploration and local exploitation is the key to improving sample efficient in sampling-based motion planners. We provide rigorous proofs of completeness and optimal convergence for this novel approach. Furthermore, we demonstrate experimentally the effectiveness of RRdT*'s locally exploring trees in granting improved visibility for planning. Consequently, RRdT* outperforms existing state-of-the-art incremental planners, especially in highly constrained environments.
        △ Less
",sampl effici highli constrain environ long major challeng sampl base planner work propos rapidli explor random disjoint tree rrdt increment optim multi queri planner rrdt use multipl disjoint tree exploit local connect space via markov chain random sampl utilis neighbourhood inform deriv previou success fail sampl balanc local exploit rrdt activ explor unseen global space local connect exploit unsuccess activ trade local exploit global explor formul multi arm bandit problem argu activ balanc global explor local exploit key improv sampl effici sampl base motion planner provid rigor proof complet optim converg novel approach furthermor demonstr experiment effect rrdt local explor tree grant improv visibl plan consequ rrdt outperform exist state art increment planner especi highli constrain environ less
120,1810.03745,"
        We have developed an automatic sleep stage classification algorithm based on deep residual neural networks and raw polysomnogram signals. Briefly, the raw data is passed through 50 convolutional layers before subsequent classification into one of five sleep stages. Three model configurations were trained on 1850 polysomnogram recordings and subsequently tested on 230 independent recordings. Our best performing model yielded an accuracy of 84.1% and a Cohen's kappa of 0.746, improving on previous reported results by other groups also using only raw polysomnogram data. Most errors were made on non-REM stage 1 and 3 decisions, errors likely resulting from the definition of these stages. Further testing on independent cohorts is needed to verify performance for clinical use.
        △ Less
",develop automat sleep stage classif algorithm base deep residu neural network raw polysomnogram signal briefli raw data pass convolut layer subsequ classif one five sleep stage three model configur train polysomnogram record subsequ test independ record best perform model yield accuraci cohen kappa improv previou report result group also use raw polysomnogram data error made non rem stage decis error like result definit stage test independ cohort need verifi perform clinic use less
121,1810.03744,"
        Historically, games of all kinds have often been the subject of study in scientific works of Computer Science, including the field of machine learning. By using machine learning techniques and applying them to a game with defined rules or a structured dataset, it's possible to learn and improve on the already existing techniques and methods to tackle new challenges and solve problems that are out of the ordinary. The already existing work on card games tends to focus on gameplay and card mechanics. This work aims to apply neural networks models, including Convolutional Neural Networks and Recurrent Neural Networks, in order to analyze Magic: the Gathering cards, both in terms of card text and illustrations; the card images and texts are used to train the networks in order to be able to classify them into multiple categories. The ultimate goal was to develop a methodology that could generate card text matching it to an input image, which was attained by relating the prediction values of the images and generated text across the different categories.
        △ Less
",histor game kind often subject studi scientif work comput scienc includ field machin learn use machin learn techniqu appli game defin rule structur dataset possibl learn improv alreadi exist techniqu method tackl new challeng solv problem ordinari alreadi exist work card game tend focu gameplay card mechan work aim appli neural network model includ convolut neural network recurr neural network order analyz magic gather card term card text illustr card imag text use train network order abl classifi multipl categori ultim goal develop methodolog could gener card text match input imag attain relat predict valu imag gener text across differ categori less
122,1810.03742,"
        Sudoku is a widely popular $\mathcal{NP}$-Complete combinatorial puzzle whose prospects for studying human computation have recently received attention, but the algorithmic hardness of Sudoku solving is yet largely unexplored. In this paper, we study the statistical mechanical properties of random Sudoku grids, showing that puzzles of varying sizes attain a hardness peak associated with a critical behavior in the constrainedness of random instances. In doing so, we provide the first description of a Sudoku \emph{freezing} transition, showing that the fraction of backbone variables undergoes a phase transition as the density of pre-filled cells is calibrated. We also uncover a variety of critical phenomena in the applicability of Sudoku elimination strategies, providing explanations as to why puzzles become boring outside the typical range of clue densities adopted by Sudoku publishers. We further show that the constrainedness of Sudoku puzzles can be understood in terms of the informational (Shannon) entropy of their solutions, which only increases up to the critical point where variables become frozen. Our findings shed light on the nature of the $k$-coloring transition when the graph topology is fixed, and are an invitation to the study of phase transition phenomena in problems defined over \emph{alldifferent} constraints. They also suggest advantages to studying the statistical mechanics of popular $\mathcal{NP}$-Hard puzzles, which can both aid the design of hard instances and help understand the difficulty of human problem solving.
        △ Less
",sudoku wide popular mathcal np complet combinatori puzzl whose prospect studi human comput recent receiv attent algorithm hard sudoku solv yet larg unexplor paper studi statist mechan properti random sudoku grid show puzzl vari size attain hard peak associ critic behavior constrained random instanc provid first descript sudoku emph freez transit show fraction backbon variabl undergo phase transit densiti pre fill cell calibr also uncov varieti critic phenomena applic sudoku elimin strategi provid explan puzzl becom bore outsid typic rang clue densiti adopt sudoku publish show constrained sudoku puzzl understood term inform shannon entropi solut increas critic point variabl becom frozen find shed light natur k color transit graph topolog fix invit studi phase transit phenomena problem defin emph alldiffer constraint also suggest advantag studi statist mechan popular mathcal np hard puzzl aid design hard instanc help understand difficulti human problem solv less
123,1810.03739,"
        In recent years, deep neural networks have demonstrated outstanding performance in many machine learning tasks. However, researchers have discovered that these state-of-the-art models are vulnerable to adversarial examples: legitimate examples added by small perturbations which are unnoticeable to human eyes. Adversarial training, which augments the training data with adversarial examples during the training process, is a well known defense to improve the robustness of the model against adversarial attacks. However, this robustness is only effective to the same attack method used for adversarial training. Madry et al.(2017) suggest that effectiveness of iterative multi-step adversarial attacks and particularly that projected gradient descent (PGD) may be considered the universal first order adversary and applying the adversarial training with PGD implies resistance against many other first order attacks. However, the computational cost of the adversarial training with PGD and other multi-step adversarial examples is much higher than that of the adversarial training with other simpler attack techniques. In this paper, we show how strong adversarial examples can be generated only at a cost similar to that of two runs of the fast gradient sign method (FGSM), allowing defense against adversarial attacks with a robustness level comparable to that of the adversarial training with multi-step adversarial examples. We empirically demonstrate the effectiveness of the proposed two-step defense approach against different attack methods and its improvements over existing defense strategies.
        △ Less
",recent year deep neural network demonstr outstand perform mani machin learn task howev research discov state art model vulner adversari exampl legitim exampl ad small perturb unnotic human eye adversari train augment train data adversari exampl train process well known defens improv robust model adversari attack howev robust effect attack method use adversari train madri et al suggest effect iter multi step adversari attack particularli project gradient descent pgd may consid univers first order adversari appli adversari train pgd impli resist mani first order attack howev comput cost adversari train pgd multi step adversari exampl much higher adversari train simpler attack techniqu paper show strong adversari exampl gener cost similar two run fast gradient sign method fgsm allow defens adversari attack robust level compar adversari train multi step adversari exampl empir demonstr effect propos two step defens approach differ attack method improv exist defens strategi less
124,1810.03737,"
        This paper proposes a novel and exact method to reconstruct line-based 3D structure from a single image using Manhattan world assumption. This problem is a distinctly unsolved problem because there can be multiple 3D reconstructions from a single image. Thus, we are often forced to look for priors like Manhattan world assumption and common scene structures. In addition to the standard orthogonality, perspective projection, and parallelism constraints, we investigate a few novel constraints based on the physical realizability of the 3D scene structure. We treat the line segments in the image to be part of a graph similar to straws and connectors game, where the goal is to back-project the line segments in 3D space and while ensuring that some of these 3D line segments connect with each other (i.e., truly intersect in 3D space) to form the 3D structure. We consider three sets of novel constraints while solving the reconstruction: (1) constraints on a series of Manhattan line intersections that form cycles, but are not all physically realizable, (2) constraints on true and false intersections in the case of nearby lines lying on the same Manhattan plane, and (3) constraints from the intersections on boundary and non-boundary line segments. The reconstruction is achieved using mixed integer linear programming (MILP), and we show compelling results on real images. Along with this paper, we will release a challenging Single View Line Reconstruction dataset with ground truth 3D line models for research purposes.
        △ Less
",paper propos novel exact method reconstruct line base structur singl imag use manhattan world assumpt problem distinctli unsolv problem multipl reconstruct singl imag thu often forc look prior like manhattan world assumpt common scene structur addit standard orthogon perspect project parallel constraint investig novel constraint base physic realiz scene structur treat line segment imag part graph similar straw connector game goal back project line segment space ensur line segment connect e truli intersect space form structur consid three set novel constraint solv reconstruct constraint seri manhattan line intersect form cycl physic realiz constraint true fals intersect case nearbi line lie manhattan plane constraint intersect boundari non boundari line segment reconstruct achiev use mix integ linear program milp show compel result real imag along paper releas challeng singl view line reconstruct dataset ground truth line model research purpos less
125,1810.03736,"
        Moral responsibility is a major concern in automated decision-making, with applications ranging from self-driving cars to kidney exchanges. From the viewpoint of automated systems, the urgent questions are: (a) How can models of moral scenarios and blameworthiness be extracted and learnt automatically from data? (b) How can judgements be computed tractably, given the split-second decision points faced by the system? By building on deep tractable probabilistic learning, we propose a learning regime for inducing models of such scenarios automatically from data and reasoning tractably from them. We report on experiments that compare our system with human judgement in three illustrative domains: lung cancer staging, teamwork management, and trolley problems.
        △ Less
",moral respons major concern autom decis make applic rang self drive car kidney exchang viewpoint autom system urgent question model moral scenario blameworthi extract learnt automat data b judgement comput tractabl given split second decis point face system build deep tractabl probabilist learn propos learn regim induc model scenario automat data reason tractabl report experi compar system human judgement three illustr domain lung cancer stage teamwork manag trolley problem less
126,1810.03733,"
        High dimensional data and systems with many degrees of freedom are often characterized by covariance matrices. In this paper, we consider the problem of simultaneously estimating the dimension of the principal (dominant) subspace of these covariance matrices and obtaining an approximation to the subspace. This problem arises in the popular principal component analysis (PCA), and in many applications of machine learning, data analysis, signal and image processing, and others. We first present a novel method for estimating the dimension of the principal subspace. We then show how this method can be coupled with a Krylov subspace method to simultaneously estimate the dimension and obtain an approximation to the subspace. The dimension estimation is achieved at no additional cost. The proposed method operates on a model selection framework, where the novel selection criterion is derived based on random matrix perturbation theory ideas. We present theoretical analyses which (a) show that the proposed method achieves strong consistency (i.e., yields optimal solution as the number of data-points $n\rightarrow \infty$), and (b) analyze conditions for exact dimension estimation in the finite $n$ case. Using recent results, we show that our algorithm also yields near optimal PCA. The proposed method avoids forming the sample covariance matrix (associated with the data) explicitly and computing the complete eigen-decomposition. Therefore, the method is inexpensive, which is particularly advantageous in modern data applications where the covariance matrices can be very large. Numerical experiments illustrate the performance of the proposed method in various applications.
        △ Less
",high dimension data system mani degre freedom often character covari matric paper consid problem simultan estim dimens princip domin subspac covari matric obtain approxim subspac problem aris popular princip compon analysi pca mani applic machin learn data analysi signal imag process other first present novel method estim dimens princip subspac show method coupl krylov subspac method simultan estim dimens obtain approxim subspac dimens estim achiev addit cost propos method oper model select framework novel select criterion deriv base random matrix perturb theori idea present theoret analys show propos method achiev strong consist e yield optim solut number data point n rightarrow infti b analyz condit exact dimens estim finit n case use recent result show algorithm also yield near optim pca propos method avoid form sampl covari matrix associ data explicitli comput complet eigen decomposit therefor method inexpens particularli advantag modern data applic covari matric larg numer experi illustr perform propos method variou applic less
127,1810.03730,"
        In this paper, we develop a non-parametric Bayesian estimation of Hawkes process kernel functions. Our method is based on the cluster representation of Hawkes processes. We sample random branching structures, and thus split the Hawkes process into clusters of Poisson processes, where the intensity function of each of these processes is the nonparametric triggering kernel of the Hawkes process. We derive both a block Gibbs sampler and a maximum a posteriori estimator based on stochastic expectation maximization. On synthetic data, we show our method to be flexible and scalable, and on two largescale Twitter diffusion datasets, we show our method to outperform the parametric Hawkes model. We observe that the learned non-parametric kernel reflects the longevity of different content types.
        △ Less
",paper develop non parametr bayesian estim hawk process kernel function method base cluster represent hawk process sampl random branch structur thu split hawk process cluster poisson process intens function process nonparametr trigger kernel hawk process deriv block gibb sampler maximum posteriori estim base stochast expect maxim synthet data show method flexibl scalabl two largescal twitter diffus dataset show method outperform parametr hawk model observ learn non parametr kernel reflect longev differ content type less
128,1810.03728,"
        Semantic inpainting is the task of inferring missing pixels in an image given surrounding pixels and high level image semantics. Most semantic inpainting algorithms are deterministic: given an image with missing regions, a single inpainted image is generated. However, there are often several plausible inpaintings for a given missing region. In this paper, we propose a method to perform probabilistic semantic inpainting by building a model, based on PixelCNNs, that learns a distribution of images conditioned on a subset of visible pixels. Experiments on the MNIST and CelebA datasets show that our method produces diverse and realistic inpaintings. Further, our model also estimates the likelihood of each sample which we show correlates well with the realism of the generated inpaintings.
        △ Less
",semant inpaint task infer miss pixel imag given surround pixel high level imag semant semant inpaint algorithm determinist given imag miss region singl inpaint imag gener howev often sever plausibl inpaint given miss region paper propos method perform probabilist semant inpaint build model base pixelcnn learn distribut imag condit subset visibl pixel experi mnist celeba dataset show method produc divers realist inpaint model also estim likelihood sampl show correl well realism gener inpaint less
129,1810.03723,"
        The notion of an anonymous shared memory (recently introduced in PODC 2017) considers that processes use different names for the same memory location. Hence, there is permanent disagreement on the location names among processes. In this context, the PODC paper presented -among other results- a symmetric deadlock-free mutual exclusion (mutex) algorithm for two processes and a necessary condition on the size $m$ of the anonymous memory for the existence of a symmetric deadlock-free mutex algorithm in an $n$-process system. This condition states that $m$ must be greater than $1$ and belong to the set $M(n)=\{m:\forall~\ell:1<\ell\leq n:~\gcd(\ell,m)=1\}$ (symmetric means that, while each process has its own identity, process identities can only be compared with equality).
  The present paper answers several open problems related to symmetric deadlock-free mutual exclusion in an $n$-process system ($n\geq 2$) where the processes communicate through $m$ registers. It first presents two algorithms. The first considers that the registers are anonymous read/write atomic registers and works for any $m$ greater than $1$ and belonging to the set $M(n)$. It thus shows that this condition on $m$ is both necessary and sufficient. The second algorithm considers anonymous read/modify/write atomic registers. It assumes that $m\in M(n)$. These algorithms differ in their design principles and their costs (measured as the number of registers which must contain the identity of a process to allow it to enter the critical section). The paper also shows that the condition $m\in M(n)$ is necessary for deadlock-free mutex on top of anonymous read/modify/write atomic registers. It follows that, when $m>1$, $m\in M(n)$ is a tight characterization of the size of the anonymous shared memory needed to solve deadlock-free mutex, be the anonymous registers read/write or read/modify/write.
        △ Less
",notion anonym share memori recent introduc podc consid process use differ name memori locat henc perman disagr locat name among process context podc paper present among result symmetr deadlock free mutual exclus mutex algorithm two process necessari condit size anonym memori exist symmetr deadlock free mutex algorithm n process system condit state must greater belong set n foral ell ell leq n gcd ell symmetr mean process ident process ident compar equal present paper answer sever open problem relat symmetr deadlock free mutual exclus n process system n geq process commun regist first present two algorithm first consid regist anonym read write atom regist work greater belong set n thu show condit necessari suffici second algorithm consid anonym read modifi write atom regist assum n algorithm differ design principl cost measur number regist must contain ident process allow enter critic section paper also show condit n necessari deadlock free mutex top anonym read modifi write atom regist follow n tight character size anonym share memori need solv deadlock free mutex anonym regist read write read modifi write less
130,1810.03717,"
        Simple reference games are of central theoretical and empirical importance in the study of situated language use. Although language provides rich, compositional truth-conditional semantics to facilitate reference, speakers and listeners may sometimes lack the overall lexical and cognitive resources to guarantee successful reference through these means alone. However, language also has rich associational structures that can serve as a further resource for achieving successful reference. Here we investigate this use of associational information in a setting where only associational information is available: a simplified version of the popular game Codenames. Using optimal experiment design techniques, we compare a range of models varying in the type of associative information deployed and in level of pragmatic sophistication against human behavior. In this setting, we find that listeners' behavior reflects direct bigram collocational associations more strongly than word-embedding or semantic knowledge graph-based associations and that there is little evidence for pragmatically sophisticated behavior by either speakers or listeners of the type that might be predicted by recursive-reasoning models such as the Rational Speech Acts theory. These results shed light on the nature of the lexical resources that speakers and listeners can bring to bear in achieving reference through associative meaning alone.
        △ Less
",simpl refer game central theoret empir import studi situat languag use although languag provid rich composit truth condit semant facilit refer speaker listen may sometim lack overal lexic cognit resourc guarante success refer mean alon howev languag also rich associ structur serv resourc achiev success refer investig use associ inform set associ inform avail simplifi version popular game codenam use optim experi design techniqu compar rang model vari type associ inform deploy level pragmat sophist human behavior set find listen behavior reflect direct bigram colloc associ strongli word embed semant knowledg graph base associ littl evid pragmat sophist behavior either speaker listen type might predict recurs reason model ration speech act theori result shed light natur lexic resourc speaker listen bring bear achiev refer associ mean alon less
131,1810.03716,"
        Visual saliency models have enjoyed a big leap in performance in recent years, thanks to advances in deep learning and large scale annotated data. Despite enormous effort and huge breakthroughs, however, models still fall short in reaching human-level accuracy. In this work, I explore the landscape of the field emphasizing on new deep saliency models, benchmarks, and datasets. A large number of image and video saliency models are reviewed and compared over two image benchmarks and two large scale video datasets. Further, I identify factors that contribute to the gap between models and humans and discuss remaining issues that need to be addressed to build the next generation of more powerful saliency models. Some specific questions that are addressed include: in what ways current models fail, how to remedy them, what can be learned from cognitive studies of attention, how explicit saliency judgments relate to fixations, how to conduct fair model comparison, and what are the emerging applications of saliency models.
        △ Less
",visual salienc model enjoy big leap perform recent year thank advanc deep learn larg scale annot data despit enorm effort huge breakthrough howev model still fall short reach human level accuraci work explor landscap field emphas new deep salienc model benchmark dataset larg number imag video salienc model review compar two imag benchmark two larg scale video dataset identifi factor contribut gap model human discuss remain issu need address build next gener power salienc model specif question address includ way current model fail remedi learn cognit studi attent explicit salienc judgment relat fixat conduct fair model comparison emerg applic salienc model less
132,1810.03714,"
        We present a probabilistic modeling framework and adaptive sampling algorithm wherein unsupervised generative models are combined with black box predictive models to tackle the problem of input design. In input design, one is given one or more stochastic ""oracle"" predictive functions, each of which maps from the input design space (e.g. DNA sequences or images) to a distribution over a property of interest (e.g. protein fluorescence or image content). Given such stochastic oracles, the problem is to find an input that is expected to maximize one or more properties, or to achieve a specified value of one or more properties, or any combination thereof. We demonstrate experimentally that our approach substantially outperforms other recently presented methods for tackling a specific version of this problem, namely, maximization when the oracle is assumed to be deterministic and unbiased. We also demonstrate that our method can tackle more general versions of the problem.
        △ Less
",present probabilist model framework adapt sampl algorithm wherein unsupervis gener model combin black box predict model tackl problem input design input design one given one stochast oracl predict function map input design space e g dna sequenc imag distribut properti interest e g protein fluoresc imag content given stochast oracl problem find input expect maxim one properti achiev specifi valu one properti combin thereof demonstr experiment approach substanti outperform recent present method tackl specif version problem name maxim oracl assum determinist unbias also demonstr method tackl gener version problem less
133,1810.03711,"
        This work presents a methodology to design trajectory tracking feedback control laws, which embed non-parametric statistical models, such as Gaussian Processes (GPs). The aim is to minimize unmodeled dynamics such as undesired slippages. The proposed approach has the benefit of avoiding complex terramechanics analysis to directly estimate from data the robot dynamics on a wide class of trajectories. Experiments in both real and simulated environments prove that the proposed methodology is promising.
        △ Less
",work present methodolog design trajectori track feedback control law emb non parametr statist model gaussian process gp aim minim unmodel dynam undesir slippag propos approach benefit avoid complex terramechan analysi directli estim data robot dynam wide class trajectori experi real simul environ prove propos methodolog promis less
134,1810.03707,"
        We introduce a novel learning method for 3D pose estimation from color images. While acquiring annotations for color images is a difficult task, our approach circumvents this problem by learning a mapping from paired color and depth images captured with an RGB-D camera. We jointly learn the pose from synthetic depth images that are easy to generate, and learn to align these synthetic depth images with the real depth images. We show our approach for the task of 3D hand pose estimation and 3D object pose estimation, both from color images only. Our method achieves performances comparable to state-of-the-art methods on popular benchmark datasets, without requiring any annotations for the color images.
        △ Less
",introduc novel learn method pose estim color imag acquir annot color imag difficult task approach circumv problem learn map pair color depth imag captur rgb camera jointli learn pose synthet depth imag easi gener learn align synthet depth imag real depth imag show approach task hand pose estim object pose estim color imag method achiev perform compar state art method popular benchmark dataset without requir annot color imag less
135,1810.03702,"
        In cloud computing management, the dynamic adaptation of computing resource allocations under time-varying workload is an active domain of investigation. Several control strategies were already proposed. Here the model-free control setting and the corresponding ""intelligent"" controllers, which are most successful in many concrete engineering situations, are employed for the ""horizontal elasticity."" When compared to the commercial ""Auto-Scaling"" algorithms, our easily implementable approach, behaves better even with sharp workload fluctuations. This is confirmed by experiments on Amazon Web Services (AWS).
        △ Less
",cloud comput manag dynam adapt comput resourc alloc time vari workload activ domain investig sever control strategi alreadi propos model free control set correspond intellig control success mani concret engin situat employ horizont elast compar commerci auto scale algorithm easili implement approach behav better even sharp workload fluctuat confirm experi amazon web servic aw less
136,1810.03695,"
        We consider the dynamic multichannel access problem, which can be formulated as a partially observable Markov decision process (POMDP). We first propose a model-free actor-critic deep reinforcement learning based framework to explore the sensing policy. To evaluate the performance of the proposed sensing policy and the framework's tolerance against uncertainty, we test the framework in scenarios with different channel switching patterns and consider different switching probabilities. Then, we consider a time-varying environment to identify the adaptive ability of the proposed framework. Additionally, we provide comparisons with the Deep-Q network (DQN) based framework proposed in [1], in terms of both average reward and the time efficiency.
        △ Less
",consid dynam multichannel access problem formul partial observ markov decis process pomdp first propos model free actor critic deep reinforc learn base framework explor sens polici evalu perform propos sens polici framework toler uncertainti test framework scenario differ channel switch pattern consid differ switch probabl consid time vari environ identifi adapt abil propos framework addit provid comparison deep q network dqn base framework propos term averag reward time effici less
137,1810.03679,"
        Advances in renewable energy generation and introduction of the government targets to improve energy efficiency gave rise to a concept of a Zero Energy Building (ZEB). A ZEB is a building whose net energy usage over a year is zero, i.e., its energy use is not larger than its overall renewables generation. A collection of ZEBs forms a Zero Energy Community (ZEC). This paper addresses the problem of energy sharing in such a community. This is different from previously addressed energy sharing between buildings as our focus is on the improvement of community energy status, while traditionally research focused on reducing losses due to transmission and storage, or achieving economic gains. We model this problem in a multi-agent environment and propose a Deep Reinforcement Learning (DRL) based solution. Each building is represented by an intelligent agent that learns over time the appropriate behaviour to share energy. We have evaluated the proposed solution in a multi-agent simulation built using osBrain. Results indicate that with time agents learn to collaborate and learn a policy comparable to the optimal policy, which in turn improves the ZEC's energy status. Buildings with no renewables preferred to request energy from their neighbours rather than from the supply grid.
        △ Less
",advanc renew energi gener introduct govern target improv energi effici gave rise concept zero energi build zeb zeb build whose net energi usag year zero e energi use larger overal renew gener collect zeb form zero energi commun zec paper address problem energi share commun differ previous address energi share build focu improv commun energi statu tradit research focus reduc loss due transmiss storag achiev econom gain model problem multi agent environ propos deep reinforc learn drl base solut build repres intellig agent learn time appropri behaviour share energi evalu propos solut multi agent simul built use osbrain result indic time agent learn collabor learn polici compar optim polici turn improv zec energi statu build renew prefer request energi neighbour rather suppli grid less
138,1810.03670,"
        Software-defined networking (SDN), which has been successfully deployed in the management of complex data centers, has recently been incorporated into a myriad of 5G networks to intelligently manage a wide range of heterogeneous wireless devices, software systems, and wireless access technologies. Thus, the SDN control plane needs to communicate wirelessly with the wireless data plane either directly or indirectly. The uncertainties in the wireless SDN control plane (WCP) make its design challenging. Both WCP schemes (direct WCP, D-WCP, and indirect WCP, I-WCP) have been incorporated into recent 5G networks; however, a discussion of their design principles and their design limitations is missing. This paper introduces an overview of the WCP design (I-WCP and D-WCP) and discusses its intricacies by reviewing its deployment in recent 5G networks. Furthermore, to facilitate synthesizing a robust WCP, this paper proposes a generic WCP framework using deep reinforcement learning (DRL) principles and presents a roadmap for future research.
        △ Less
",softwar defin network sdn success deploy manag complex data center recent incorpor myriad g network intellig manag wide rang heterogen wireless devic softwar system wireless access technolog thu sdn control plane need commun wirelessli wireless data plane either directli indirectli uncertainti wireless sdn control plane wcp make design challeng wcp scheme direct wcp wcp indirect wcp wcp incorpor recent g network howev discuss design principl design limit miss paper introduc overview wcp design wcp wcp discuss intricaci review deploy recent g network furthermor facilit synthes robust wcp paper propos gener wcp framework use deep reinforc learn drl principl present roadmap futur research less
139,1810.03664,"
        Edit distance is a measure of similarity of two strings based on the minimum number of character insertions, deletions, and substitutions required to transform one string into the other. The edit distance can be computed exactly using a dynamic programming algorithm that runs in quadratic time. Andoni, Krauthgamer and Onak (2010) gave a nearly linear time algorithm that approximates edit distance within approximation factor $\text{poly}(\log n)$.
  In this paper, we provide an algorithm with running time $\tilde{O}(n^{2-2/7})$ that approximates the edit distance within a constant factor.
        △ Less
",edit distanc measur similar two string base minimum number charact insert delet substitut requir transform one string edit distanc comput exactli use dynam program algorithm run quadrat time andoni krauthgam onak gave nearli linear time algorithm approxim edit distanc within approxim factor text poli log n paper provid algorithm run time tild n approxim edit distanc within constant factor less
140,1810.03660,"
        Several lexica for sentiment analysis have been developed and made available in the NLP community. While most of these come with word polarity annotations (e.g. positive/negative), attempts at building lexica for finer-grained emotion analysis (e.g. happiness, sadness) have recently attracted significant attention. Such lexica are often exploited as a building block in the process of developing learning models for which emotion recognition is needed, and/or used as baselines to which compare the performance of the models. In this work, we contribute two new resources to the community: a) an extension of an existing and widely used emotion lexicon for English; and b) a novel version of the lexicon targeting Italian. Furthermore, we show how simple techniques can be used, both in supervised and unsupervised experimental settings, to boost performances on datasets and tasks of varying degree of domain-specificity.
        △ Less
",sever lexica sentiment analysi develop made avail nlp commun come word polar annot e g posit neg attempt build lexica finer grain emot analysi e g happi sad recent attract signific attent lexica often exploit build block process develop learn model emot recognit need use baselin compar perform model work contribut two new resourc commun extens exist wide use emot lexicon english b novel version lexicon target italian furthermor show simpl techniqu use supervis unsupervis experiment set boost perform dataset task vari degre domain specif less
141,1810.03654,"
        Learning depth and optical flow via deep neural networks by watching videos has made significant progress recently. In this paper, we jointly solve the two tasks by exploiting the underlying geometric rules within stereo videos. Specifically, given two consecutive stereo image pairs from a video, we first estimate depth, camera ego-motion and optical flow from three neural networks. Then the whole scene is decomposed into moving foreground and static background by compar- ing the estimated optical flow and rigid flow derived from the depth and ego-motion. We propose a novel consistency loss to let the optical flow learn from the more accurate rigid flow in static regions. We also design a rigid alignment module which helps refine ego-motion estimation by using the estimated depth and optical flow. Experiments on the KITTI dataset show that our results significantly outperform other state-of- the-art algorithms. Source codes can be found at https: //github.com/baidu-research/UnDepthflow
        △ Less
",learn depth optic flow via deep neural network watch video made signific progress recent paper jointli solv two task exploit underli geometr rule within stereo video specif given two consecut stereo imag pair video first estim depth camera ego motion optic flow three neural network whole scene decompos move foreground static background compar ing estim optic flow rigid flow deriv depth ego motion propos novel consist loss let optic flow learn accur rigid flow static region also design rigid align modul help refin ego motion estim use estim depth optic flow experi kitti dataset show result significantli outperform state art algorithm sourc code found http github com baidu research undepthflow less
142,1810.03652,"
        Graph clustering is a challenging pattern recognition problem whose goal is to identify vertex partitions with high intra-group connectivity. Because of the rough definition of this problem, there are numerous effective ways to formally determine such partitions. In particular, multi-objective optimization can deal with the trade-offs between different clustering quality measures in order to better assess the partitions. This paper investigates a problem that maximizes the number of intra-cluster edges of a graph and minimizes the expected number of inter-cluster edges in a random graph with the same degree sequence as the original one. The difference between the two investigated objectives is the definition of the well-known measure of graph clustering quality: the modularity. We introduce a spectral decomposition hybridized with an evolutionary heuristic, called MOSpecG, to approach this bi-objective problem and an ensemble strategy to consolidate the solutions found by MOSpecG into a final robust partition. The results of computational experiments with real and artificial LFR networks demonstrated a significant improvement in the results and performance of the introduced method in regard to another bi-objective algorithm found in the literature
        △ Less
",graph cluster challeng pattern recognit problem whose goal identifi vertex partit high intra group connect rough definit problem numer effect way formal determin partit particular multi object optim deal trade off differ cluster qualiti measur order better assess partit paper investig problem maxim number intra cluster edg graph minim expect number inter cluster edg random graph degre sequenc origin one differ two investig object definit well known measur graph cluster qualiti modular introduc spectral decomposit hybrid evolutionari heurist call mospecg approach bi object problem ensembl strategi consolid solut found mospecg final robust partit result comput experi real artifici lfr network demonstr signific improv result perform introduc method regard anoth bi object algorithm found literatur less
143,1810.03649,"
        Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training such as overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings. In this work, we present a novel regularization scheme for VQA that reduces this effect. We introduce a question-only model that takes as input the question encoding from the VQA model and must leverage language biases in order to succeed. We then pose training as an adversarial game between the VQA model and this question-only adversary -- discouraging the VQA model from capturing language biases in its question encoding. Further,we leverage this question-only model to estimate the increase in model confidence after considering the image, which we maximize explicitly to encourage visual grounding. Our approach is a model agnostic training procedure and simple to implement. We show empirically that it can improve performance significantly on a bias-sensitive split of the VQA dataset for multiple base models -- achieving state-of-the-art on this task. Further, on standard VQA tasks, our approach shows significantly less drop in accuracy compared to existing bias-reducing VQA models.
        △ Less
",modern visual question answer vqa model shown reli heavili superfici correl question answer word learn train overwhelmingli report type room kitchen sport play tenni irrespect imag alarmingli shortcom often well reflect evalu strong prior exist test distribut howev vqa system fail ground question imag content would like perform poorli real world set work present novel regular scheme vqa reduc effect introduc question model take input question encod vqa model must leverag languag bias order succeed pose train adversari game vqa model question adversari discourag vqa model captur languag bias question encod leverag question model estim increas model confid consid imag maxim explicitli encourag visual ground approach model agnost train procedur simpl implement show empir improv perform significantli bia sensit split vqa dataset multipl base model achiev state art task standard vqa task approach show significantli less drop accuraci compar exist bia reduc vqa model less
144,1810.03646,"
        We continue to study the construction of cryptographic trilinear maps involving abelian varieties over finite fields. We introduce Weil descent as a tool to strengthen the security of a trilinear map. More specifically, we prepare a trilinear map by starting with an abelian variety of small dimension defined over a finite field $K$ of large extension degree over a finite field $k$. The points and maps and functions involved in the trilinear maps are encoded using Weil descent. However the original abelian variety as well as the descent basis and descent table will be kept secret. We present a concrete construction involving the jacobian varieties of hyperelliptic curves. The idea of using Weil descent to strengthen security raises some interesting computational problems from a cryptanalytic perspective.
        △ Less
",continu studi construct cryptograph trilinear map involv abelian varieti finit field introduc weil descent tool strengthen secur trilinear map specif prepar trilinear map start abelian varieti small dimens defin finit field k larg extens degre finit field k point map function involv trilinear map encod use weil descent howev origin abelian varieti well descent basi descent tabl kept secret present concret construct involv jacobian varieti hyperellipt curv idea use weil descent strengthen secur rais interest comput problem cryptanalyt perspect less
145,1810.03643,"
        In a new type of automated parts-to-picker warehouse system - a Robotic Mobile Fulfillment System (RMFS) - robots are sent to transport pods (movable shelves) to human operators at stations to pick/put items from/to pods. There are many operational decision problems in such a system, and some of them are interdependent and influence each other. In order to analyze the decision problems and the relationships between them, there are two open-source simulation frameworks in the literature, Alphabet Soup and RAWSim-O. However, the steps between simulation and real-world RMFS are not clear in the literature. Therefore, this paper aims to bridge this gap. The simulator is firstly transferred as core software. The core software is connected with an open-source ERP system, called Odoo, while it is also connected with real robots and stations through an XOR-bench. The XOR-bench enables the RMFS to be integrated with several mini-robots and mobile industrial robots in (removed) experiments for the purpose of research and education.
        △ Less
",new type autom part picker warehous system robot mobil fulfil system rmf robot sent transport pod movabl shelv human oper station pick put item pod mani oper decis problem system interdepend influenc order analyz decis problem relationship two open sourc simul framework literatur alphabet soup rawsim howev step simul real world rmf clear literatur therefor paper aim bridg gap simul firstli transfer core softwar core softwar connect open sourc erp system call odoo also connect real robot station xor bench xor bench enabl rmf integr sever mini robot mobil industri robot remov experi purpos research educ less
146,1810.03642,"
        We propose CAML, a meta-learning method for fast adaptation that partitions the model parameters into two parts: context parameters that serve as additional input to the model and are adapted on individual tasks, and shared parameters that are meta-trained and shared across tasks. At test time, the context parameters are updated with one or several gradient steps on a task-specific loss that is backpropagated through the shared part of the network. Compared to approaches that adjust all parameters on a new task (e.g., MAML), our method can be scaled up to larger networks without overfitting on a single task, is easier to implement, and saves memory writes during training and network communication at test time for distributed machine learning systems. We show empirically that this approach outperforms MAML, is less sensitive to the task-specific learning rate, can capture meaningful task embeddings with the context parameters, and outperforms alternative partitionings of the parameter vectors.
        △ Less
",propos caml meta learn method fast adapt partit model paramet two part context paramet serv addit input model adapt individu task share paramet meta train share across task test time context paramet updat one sever gradient step task specif loss backpropag share part network compar approach adjust paramet new task e g maml method scale larger network without overfit singl task easier implement save memori write train network commun test time distribut machin learn system show empir approach outperform maml less sensit task specif learn rate captur meaning task embed context paramet outperform altern partit paramet vector less
147,1810.03611,"
        The power of machine learning systems not only promises great technical progress, but risks societal harm. As a recent example, researchers have shown that popular word embedding algorithms exhibit stereotypical biases, such as gender bias. The widespread use of these algorithms in machine learning systems, from automated translation services to curriculum vitae scanners, can amplify stereotypes in important contexts. Although methods have been developed to measure these biases and alter word embeddings to mitigate their biased representations, there is a lack of understanding in how word embedding bias depends on the training data. In this work, we develop a technique for understanding the origins of bias in word embeddings. Given a word embedding trained on a corpus, our method identifies how perturbing the corpus will affect the bias of the resulting embedding. This can be used to trace the origins of word embedding bias back to the original training documents. Using our method, one can investigate trends in the bias of the underlying corpus and identify subsets of documents whose removal would most reduce bias. We demonstrate our techniques on both a New York Times and Wikipedia corpus and find that our influence function-based approximations are extremely accurate.
        △ Less
",power machin learn system promis great technic progress risk societ harm recent exampl research shown popular word embed algorithm exhibit stereotyp bias gender bia widespread use algorithm machin learn system autom translat servic curriculum vita scanner amplifi stereotyp import context although method develop measur bias alter word embed mitig bias represent lack understand word embed bia depend train data work develop techniqu understand origin bia word embed given word embed train corpu method identifi perturb corpu affect bia result embed use trace origin word embed bia back origin train document use method one investig trend bia underli corpu identifi subset document whose remov would reduc bia demonstr techniqu new york time wikipedia corpu find influenc function base approxim extrem accur less
148,1810.03599,"
        Data-driven character animation based on motion capture can produce highly naturalistic behaviors and, when combined with physics simulation, can provide for natural procedural responses to physical perturbations, environmental changes, and morphological discrepancies. Motion capture remains the most popular source of motion data, but collecting mocap data typically requires heavily instrumented environments and actors. In this paper, we propose a method that enables physically simulated characters to learn skills from videos (SFV). Our approach, based on deep pose estimation and deep reinforcement learning, allows data-driven animation to leverage the abundance of publicly available video clips from the web, such as those from YouTube. This has the potential to enable fast and easy design of character controllers simply by querying for video recordings of the desired behavior. The resulting controllers are robust to perturbations, can be adapted to new settings, can perform basic object interactions, and can be retargeted to new morphologies via reinforcement learning. We further demonstrate that our method can predict potential human motions from still images, by forward simulation of learned controllers initialized from the observed pose. Our framework is able to learn a broad range of dynamic skills, including locomotion, acrobatics, and martial arts.
        △ Less
",data driven charact anim base motion captur produc highli naturalist behavior combin physic simul provid natur procedur respons physic perturb environment chang morpholog discrep motion captur remain popular sourc motion data collect mocap data typic requir heavili instrument environ actor paper propos method enabl physic simul charact learn skill video sfv approach base deep pose estim deep reinforc learn allow data driven anim leverag abund publicli avail video clip web youtub potenti enabl fast easi design charact control simpli queri video record desir behavior result control robust perturb adapt new set perform basic object interact retarget new morpholog via reinforc learn demonstr method predict potenti human motion still imag forward simul learn control initi observ pose framework abl learn broad rang dynam skill includ locomot acrobat martial art less
149,1810.03598,"
        Building on the successes of satisfiability modulo theories (SMT), Bjørner et al. initiated a research programme advocating Horn constraints as a suitable basis for automatic program verification. The notion of first-order constrained Horn clauses has recently been extended to higher-order logic by Cathcart Burn et al. To exploit the remarkable efficiency of SMT solving, a natural approach to solve systems of higher-order Horn constraints is to reduce them to systems of first-order Horn constraints. This paper presents a defunctionalization algorithm to achieve the reduction.
  Given a well-sorted higher-order constrained Horn clause (HoCHC) problem instance, the defunctionalization algorithm constructs a first-order well-sorted constrained Horn clause problem. In addition to well-sortedness of the algorithm's output, we prove that if an input HoCHC is solvable, then the result of its defunctionalization is solvable. The converse also holds, which we prove using a recent result on the continuous semantics of HoCHC. To our knowledge, this defunctionalization algorithm is the first sound and complete reduction from systems of higher-order Horn constraints to systems of first-order Horn constraints.
  We have constructed DefMono, a prototype implementation of the defunctionalization algorithm. It first defunctionalizes an input HoCHC problem and then feeds the result into a backend SMT solver. We have evaluated the performance of DefMono empirically by comparison with two other higher-order verification tools.
        △ Less
",build success satisfi modulo theori smt bj rner et al initi research programm advoc horn constraint suitabl basi automat program verif notion first order constrain horn claus recent extend higher order logic cathcart burn et al exploit remark effici smt solv natur approach solv system higher order horn constraint reduc system first order horn constraint paper present defunction algorithm achiev reduct given well sort higher order constrain horn claus hochc problem instanc defunction algorithm construct first order well sort constrain horn claus problem addit well sorted algorithm output prove input hochc solvabl result defunction solvabl convers also hold prove use recent result continu semant hochc knowledg defunction algorithm first sound complet reduct system higher order horn constraint system first order horn constraint construct defmono prototyp implement defunction algorithm first defunction input hochc problem feed result backend smt solver evalu perform defmono empir comparison two higher order verif tool less
150,1810.03595,"
        For analysing and/or understanding languages having no word boundaries based on morphological analysis such as Japanese, Chinese, and Thai, it is desirable to perform appropriate word segmentation before word embeddings. But it is inherently difficult in these languages. In recent years, various language models based on deep learning have made remarkable progress, and some of these methodologies utilizing character-level features have successfully avoided such a difficult problem. However, when a model is fed character-level features of the above languages, it often causes overfitting due to a large number of character types. In this paper, we propose a CE-CLCNN, character-level convolutional neural networks using a character encoder to tackle these problems. The proposed CE-CLCNN is an end-to-end learning model and has an image-based character encoder, i.e. the CE-CLCNN handles each character in the target document as an image. Through various experiments, we found and confirmed that our CE-CLCNN captured closely embedded features for visually and semantically similar characters and achieves state-of-the-art results on several open document classification tasks. In this paper we report the performance of our CE-CLCNN with the Wikipedia title estimation task and analyse the internal behaviour.
        △ Less
",analys understand languag word boundari base morpholog analysi japanes chines thai desir perform appropri word segment word embed inher difficult languag recent year variou languag model base deep learn made remark progress methodolog util charact level featur success avoid difficult problem howev model fed charact level featur languag often caus overfit due larg number charact type paper propos ce clcnn charact level convolut neural network use charact encod tackl problem propos ce clcnn end end learn model imag base charact encod e ce clcnn handl charact target document imag variou experi found confirm ce clcnn captur close embed featur visual semant similar charact achiev state art result sever open document classif task paper report perform ce clcnn wikipedia titl estim task analys intern behaviour less
151,1810.03594,"
        In online learning, the dynamic regret metric chooses the reference (optimal) solution that may change over time, while the typical (static) regret metric assumes the reference solution to be constant over the whole time horizon. The dynamic regret metric is particularly interesting for applications such as online recommendation (since the customers' preference always evolves over time). While the online gradient method has been shown to be optimal for the static regret metric, the optimal algorithm for the dynamic regret remains unknown. In this paper, we show that proximal online gradient (a general version of online gradient) is optimum to the dynamic regret by showing that the proved lower bound matches the upper bound that slightly improves existing upper bound.
        △ Less
",onlin learn dynam regret metric choos refer optim solut may chang time typic static regret metric assum refer solut constant whole time horizon dynam regret metric particularli interest applic onlin recommend sinc custom prefer alway evolv time onlin gradient method shown optim static regret metric optim algorithm dynam regret remain unknown paper show proxim onlin gradient gener version onlin gradient optimum dynam regret show prove lower bound match upper bound slightli improv exist upper bound less
152,1810.03587,"
        The traditional approach of hand-crafting priors (such as sparsity) for solving inverse problems is slowly being replaced by the use of richer learned priors (such as those modeled by generative adversarial networks, or GANs). In this work, we study the algorithmic aspects of such a learning-based approach from a theoretical perspective. For certain generative network architectures, we establish a simple non-convex algorithmic approach that (a) theoretically enjoys linear convergence guarantees for certain inverse problems, and (b) empirically improves upon conventional techniques such as back-propagation. We also propose an extension of our approach that can handle model mismatch (i.e., situations where the generative network prior is not exactly applicable.) Together, our contributions serve as building blocks towards a more complete algorithmic understanding of generative models in inverse problems.
        △ Less
",tradit approach hand craft prior sparsiti solv invers problem slowli replac use richer learn prior model gener adversari network gan work studi algorithm aspect learn base approach theoret perspect certain gener network architectur establish simpl non convex algorithm approach theoret enjoy linear converg guarante certain invers problem b empir improv upon convent techniqu back propag also propos extens approach handl model mismatch e situat gener network prior exactli applic togeth contribut serv build block toward complet algorithm understand gener model invers problem less
153,1810.03583,"
        Robots require knowledge about objects in order to efficiently perform various household tasks involving objects. The existing knowledge bases for robots acquire symbolic knowledge about objects from manually-coded external common sense knowledge bases such as ConceptNet, Word-Net etc. The problem with such approaches is the discrepancy between human-centric symbolic knowledge and robot-centric object perception due to its limited perception capabilities. Ultimately, significant portion of knowledge in the knowledge base remains ungrounded into robot's perception. To overcome this discrepancy, we propose an approach to enable robots to generate robot-centric symbolic knowledge about objects from their own sensory data, thus, allowing them to assemble their own conceptual understanding of objects. With this goal in mind, the presented paper elaborates on the work-in-progress of the proposed approach followed by the preliminary results.
        △ Less
",robot requir knowledg object order effici perform variou household task involv object exist knowledg base robot acquir symbol knowledg object manual code extern common sens knowledg base conceptnet word net etc problem approach discrep human centric symbol knowledg robot centric object percept due limit percept capabl ultim signific portion knowledg knowledg base remain unground robot percept overcom discrep propos approach enabl robot gener robot centric symbol knowledg object sensori data thu allow assembl conceptu understand object goal mind present paper elabor work progress propos approach follow preliminari result less
154,1810.03581,"
        Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.
        △ Less
",although transform translat model vaswani et al achiev state art perform varieti translat task use document level context deal discours phenomena problemat transform still remain challeng work extend transform model new context encod repres document level context incorpor origin encod decod larg scale document level parallel corpora usual avail introduc two step train method take full advantag abund sentenc level parallel corpora limit document level parallel corpora experi nist chines english dataset iwslt french english dataset show approach improv transform significantli less
155,1810.03579,"
        Changes to network structure can substantially affect when and how widely new ideas, products, and conventions are adopted. In models of biological contagion, interventions that randomly rewire edges (making them ""longer"") accelerate spread. However, there are other models relevant to social contagion, such as those motivated by myopic best-response in games with strategic complements, in which individual's behavior is described by a threshold number of adopting neighbors above which adoption occurs (i.e., complex contagions). Recent work has argued that highly clustered, rather than random, networks facilitate spread of these complex contagions. Here we show that minor modifications of prior analyses, which make them more realistic, reverse this result. The modification is that we allow very rarely below threshold adoption, i.e., very rarely adoption occurs, where there is only one adopting neighbor. To model the trade-off between long and short edges we consider networks that are the union of cycle-power-$k$ graphs and random graphs on $n$ nodes. We study how the time to global spread changes as we replace the cycle edges with (random) long ties. Allowing adoptions below threshold to occur with order $1/\sqrt{n}$ probability is enough to ensure that random rewiring accelerates spread. Simulations illustrate the robustness of these results to other commonly-posited models for noisy best-response behavior. We then examine empirical social networks, where we find that hypothetical interventions that (a) randomly rewire existing edges or (b) add random edges reduce time to spread compared with the original network or addition of ""short"", triad-closing edges, respectively. This substantially revises conclusions about how interventions change the spread of behavior, suggesting that those wanting to increase spread should induce formation of long ties, rather than triad-closing ties.
        △ Less
",chang network structur substanti affect wide new idea product convent adopt model biolog contagion intervent randomli rewir edg make longer acceler spread howev model relev social contagion motiv myopic best respons game strateg complement individu behavior describ threshold number adopt neighbor adopt occur e complex contagion recent work argu highli cluster rather random network facilit spread complex contagion show minor modif prior analys make realist revers result modif allow rare threshold adopt e rare adopt occur one adopt neighbor model trade long short edg consid network union cycl power k graph random graph n node studi time global spread chang replac cycl edg random long tie allow adopt threshold occur order sqrt n probabl enough ensur random rewir acceler spread simul illustr robust result commonli posit model noisi best respons behavior examin empir social network find hypothet intervent randomli rewir exist edg b add random edg reduc time spread compar origin network addit short triad close edg respect substanti revis conclus intervent chang spread behavior suggest want increas spread induc format long tie rather triad close tie less
156,1810.03572,"
        This paper aims to design quadrotor swarm performances, where the swarm acts as an integrated, coordinated unit embodying moving and deforming objects. We divide the task of creating a choreography into three basic steps: designing swarm motion primitives, transitioning between those movements, and synchronizing the motion of the drones. The result is a flexible framework for designing choreographies comprised of a wide variety of motions. The motion primitives can be intuitively designed using few parameters, providing a rich library for choreography design. Moreover, we combine and adapt existing goal assignment and trajectory generation algorithms to maximize the smoothness of the transitions between motion primitives. Finally, we propose a correction algorithm to compensate for motion delays and synchronize the motion of the drones to a desired periodic motion pattern. The proposed methodology was validated experimentally by generating and executing choreographies on a swarm of 25 quadrotors.
        △ Less
",paper aim design quadrotor swarm perform swarm act integr coordin unit embodi move deform object divid task creat choreographi three basic step design swarm motion primit transit movement synchron motion drone result flexibl framework design choreographi compris wide varieti motion motion primit intuit design use paramet provid rich librari choreographi design moreov combin adapt exist goal assign trajectori gener algorithm maxim smooth transit motion primit final propos correct algorithm compens motion delay synchron motion drone desir period motion pattern propos methodolog valid experiment gener execut choreographi swarm quadrotor less
157,1810.03570,"
        Detection of buildings and other objects from aerial images has various applications in urban planning and map making. Automated building detection from aerial imagery is a challenging task, as it is prone to varying lighting conditions, shadows and occlusions. Convolutional Neural Networks (CNNs) are robust against some of these variations, although they fail to distinguish easy and difficult examples. We train a detection algorithm from RGB-D images to obtain a segmented mask by using the CNN architecture DenseNet.First, we improve the performance of the model by applying a statistical re-sampling technique called Bootstrapping and demonstrate that more informative examples are retained. Second, the proposed method outperforms the non-bootstrapped version by utilizing only one-sixth of the original training data and it obtains a precision-recall break-even of 95.10% on our aerial imagery dataset.
        △ Less
",detect build object aerial imag variou applic urban plan map make autom build detect aerial imageri challeng task prone vari light condit shadow occlus convolut neural network cnn robust variat although fail distinguish easi difficult exampl train detect algorithm rgb imag obtain segment mask use cnn architectur densenet first improv perform model appli statist sampl techniqu call bootstrap demonstr inform exampl retain second propos method outperform non bootstrap version util one sixth origin train data obtain precis recal break even aerial imageri dataset less
158,1810.03568,"
        The widespread adoption of continuously connected smartphones and tablets developed the usage of mobile applications, among which many use location to provide geolocated services. These services provide new prospects for users: getting directions to work in the morning, leaving a check-in at a restaurant at noon and checking next day's weather in the evening are possible right from any mobile device embedding a GPS chip. In these location-based applications, the user's location is sent to a server, which uses them to provide contextual and personalised answers. However, nothing prevents the latter from gathering, analysing and possibly sharing the collected information, which opens the door to many privacy threats. Indeed, mobility data can reveal sensitive information about users, among which one's home, work place or even religious and political preferences. For this reason, many privacy-preserving mechanisms have been proposed these last years to enhance location privacy while using geolocated services. This article surveys and organises contributions in this area from classical building blocks to the most recent developments of privacy threats and location privacy-preserving mechanisms. We divide the protection mechanisms between online and offline use cases, and organise them into six categories depending on the nature of their algorithm. Moreover, this article surveys the evaluation metrics used to assess protection mechanisms in terms of privacy, utility and performance. Finally, open challenges and new directions to address the problem of computational location privacy are pointed out and discussed.
        △ Less
",widespread adopt continu connect smartphon tablet develop usag mobil applic among mani use locat provid geoloc servic servic provid new prospect user get direct work morn leav check restaur noon check next day weather even possibl right mobil devic embed gp chip locat base applic user locat sent server use provid contextu personalis answer howev noth prevent latter gather analys possibl share collect inform open door mani privaci threat inde mobil data reveal sensit inform user among one home work place even religi polit prefer reason mani privaci preserv mechan propos last year enhanc locat privaci use geoloc servic articl survey organis contribut area classic build block recent develop privaci threat locat privaci preserv mechan divid protect mechan onlin offlin use case organis six categori depend natur algorithm moreov articl survey evalu metric use assess protect mechan term privaci util perform final open challeng new direct address problem comput locat privaci point discuss less
159,1810.03552,"
        Modern natural language processing and understanding applications have enjoyed a great boost utilizing neural networks models. However, this is not the case for most languages especially low-resource ones with insufficient annotated training data. Cross-lingual transfer learning methods improve the performance on a low-resource target language by leveraging labeled data from other (source) languages, typically with the help of cross-lingual resources such as parallel corpora. In this work, we propose the first zero-resource multilingual transfer learning model that can utilize training data in multiple source languages, while not requiring target language training data nor cross-lingual supervision. Unlike existing methods that only rely on language-invariant features for cross-lingual transfer, our approach utilizes both language-invariant and language-specific features in a coherent way. Our model leverages adversarial networks to learn language-invariant features and mixture-of-experts models to dynamically exploit the relation between the target language and each individual source language. This enables our model to learn effectively what to share between various languages in the multilingual setup. It results in significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale real-world industry dataset.
        △ Less
",modern natur languag process understand applic enjoy great boost util neural network model howev case languag especi low resourc one insuffici annot train data cross lingual transfer learn method improv perform low resourc target languag leverag label data sourc languag typic help cross lingual resourc parallel corpora work propos first zero resourc multilingu transfer learn model util train data multipl sourc languag requir target languag train data cross lingual supervis unlik exist method reli languag invari featur cross lingual transfer approach util languag invari languag specif featur coher way model leverag adversari network learn languag invari featur mixtur expert model dynam exploit relat target languag individu sourc languag enabl model learn effect share variou languag multilingu setup result signific perform gain prior art shown extens set experi multipl text classif sequenc tag task includ larg scale real world industri dataset less
160,1810.03551,"
        We consider the approximate pattern matching problem under edit distance. In this problem we are given a pattern $P$ of length $w$ and a text $T$ of length $n$ over some alphabet $Σ$, and a positive integer $k$. The goal is to find all the positions $j$ in $T$ such that there is a substring of $T$ ending at $j$ which has edit distance at most $k$ from the pattern $P$. Recall, the edit distance between two strings is the minimum number of character insertions, deletions, and substitutions required to transform one string into the other. For a position $t$ in $\{1,...,n\}$, let $k_t$ be the smallest edit distance between $P$ and any substring of $T$ ending at $t$. In this paper we give a constant factor approximation to the sequence $k_1,k_2,...,k_{n}$. We consider both offline and online settings.
  In the offline setting, where both $P$ and $T$ are available, we present an algorithm that for all $t$ in $\{1,...,n\}$, computes the value of $k_t$ approximately within a constant factor. The worst case running time of our algorithm is $O(n w^{3/4})$. As a consequence we break the $O(nw)$-time barrier for this problem.
  In the online setting, we are given $P$ and then $T$ arrives one symbol at a time. We design an algorithm that upon arrival of the $t$-th symbol of $T$ computes $k_t$ approximately within $O(1)$-multiplicative factor and $w^{8/9}$-additive error. Our algorithm takes $O(w^{1-(7/54)})$ amortized time per symbol arrival and takes $O(w^{1-(1/54)})$ additional space apart from storing the pattern $P$.
  Both of our algorithms are randomized and produce correct answer with high probability. To the best of our knowledge this is the first worst-case sub-linear (in the length of the pattern) time and sub-linear succinct space algorithm for online approximate pattern matching problem.
        △ Less
",consid approxim pattern match problem edit distanc problem given pattern p length w text length n alphabet posit integ k goal find posit j substr end j edit distanc k pattern p recal edit distanc two string minimum number charact insert delet substitut requir transform one string posit n let k smallest edit distanc p substr end paper give constant factor approxim sequenc k k k n consid offlin onlin set offlin set p avail present algorithm n comput valu k approxim within constant factor worst case run time algorithm n w consequ break nw time barrier problem onlin set given p arriv one symbol time design algorithm upon arriv th symbol comput k approxim within multipl factor w addit error algorithm take w amort time per symbol arriv take w addit space apart store pattern p algorithm random produc correct answer high probabl best knowledg first worst case sub linear length pattern time sub linear succinct space algorithm onlin approxim pattern match problem less
161,1810.03548,"
        Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.
        △ Less
",meta learn learn learn scienc systemat observ differ machin learn approach perform wide rang learn task learn experi meta data learn new task much faster otherwis possibl dramat speed improv design machin learn pipelin neural architectur also allow us replac hand engin algorithm novel approach learn data driven way chapter provid overview state art fascin continu evolv field less
162,1810.03541,"
        In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score.
        △ Less
",paper propos new rich resourc enhanc amr align produc multipl align new transit system amr pars along oracl parser align tune oracl parser via pick align lead highest score achiev amr graph experiment result show align outperform rule base align previou work achiev higher align f score consist improv two open sourc amr parser base align transit system develop transit base amr parser pars sentenc amr graph directli ensembl parser word po tag input lead smatch f score less
163,1810.03538,"
        Binarized Neural Networks (BNNs) have recently attracted significant interest due to their computational efficiency. Concurrently, it has been shown that neural networks may be overly sensitive to ""attacks"" - tiny adversarial changes in the input - which may be detrimental to their use in safety-critical domains. Designing attack algorithms that effectively fool trained models is a key step towards learning robust neural networks. The discrete, non-differentiable nature of BNNs, which distinguishes them from their full-precision counterparts, poses a challenge to gradient-based attacks. In this work, we study the problem of attacking a BNN through the lens of combinatorial and integer optimization. We propose a Mixed Integer Linear Programming (MILP) formulation of the problem. While exact and flexible, the MILP quickly becomes intractable as the network and perturbation space grow. To address this issue, we propose IProp, a decomposition-based algorithm that solves a sequence of much smaller MILP problems. Experimentally, we evaluate both proposed methods against the standard gradient-based attack (FGSM) on MNIST and Fashion-MNIST, and show that IProp performs favorably compared to FGSM, while scaling beyond the limits of the MILP.
        △ Less
",binar neural network bnn recent attract signific interest due comput effici concurr shown neural network may overli sensit attack tini adversari chang input may detriment use safeti critic domain design attack algorithm effect fool train model key step toward learn robust neural network discret non differenti natur bnn distinguish full precis counterpart pose challeng gradient base attack work studi problem attack bnn len combinatori integ optim propos mix integ linear program milp formul problem exact flexibl milp quickli becom intract network perturb space grow address issu propos iprop decomposit base algorithm solv sequenc much smaller milp problem experiment evalu propos method standard gradient base attack fgsm mnist fashion mnist show iprop perform favor compar fgsm scale beyond limit milp less
164,1810.03530,"
        We present a novel parallelisation scheme that simplifies the adaptation of learning algorithms to growing amounts of data as well as growing needs for accurate and confident predictions in critical applications. In contrast to other parallelisation techniques, it can be applied to a broad class of learning algorithms without further mathematical derivations and without writing dedicated code, while at the same time maintaining theoretical performance guarantees. Moreover, our parallelisation scheme is able to reduce the runtime of many learning algorithms to polylogarithmic time on quasi-polynomially many processing units. This is a significant step towards a general answer to an open question on the efficient parallelisation of machine learning algorithms in the sense of Nick's Class (NC). The cost of this parallelisation is in the form of a larger sample complexity. Our empirical study confirms the potential of our parallelisation scheme with fixed numbers of processors and instances in realistic application scenarios.
        △ Less
",present novel parallelis scheme simplifi adapt learn algorithm grow amount data well grow need accur confid predict critic applic contrast parallelis techniqu appli broad class learn algorithm without mathemat deriv without write dedic code time maintain theoret perform guarante moreov parallelis scheme abl reduc runtim mani learn algorithm polylogarithm time quasi polynomi mani process unit signific step toward gener answer open question effici parallelis machin learn algorithm sens nick class nc cost parallelis form larger sampl complex empir studi confirm potenti parallelis scheme fix number processor instanc realist applic scenario less
165,1810.03527,"
        Many hyperparameter optimization (HyperOpt) methods assume restricted computing resources and mainly focus on enhancing performance. Here we propose a novel cloud-based HyperOpt (CHOPT) framework which can efficiently utilize shared computing resources while supporting various HyperOpt algorithms. We incorporate convenient web-based user interfaces, visualization, and analysis tools, enabling users to easily control optimization procedures and build up valuable insights with an iterative analysis procedure. Furthermore, our framework can be incorporated with any cloud platform, thus complementarily increasing the efficiency of conventional deep learning frameworks. We demonstrate applications of CHOPT with tasks such as image recognition and question-answering, showing that our framework can find hyperparameter configurations competitive with previous work. We also show CHOPT is capable of providing interesting observations through its analysing tools
        △ Less
",mani hyperparamet optim hyperopt method assum restrict comput resourc mainli focu enhanc perform propos novel cloud base hyperopt chopt framework effici util share comput resourc support variou hyperopt algorithm incorpor conveni web base user interfac visual analysi tool enabl user easili control optim procedur build valuabl insight iter analysi procedur furthermor framework incorpor cloud platform thu complementarili increas effici convent deep learn framework demonstr applic chopt task imag recognit question answer show framework find hyperparamet configur competit previou work also show chopt capabl provid interest observ analys tool less
166,1810.03523,"
        This work studies the problem of learning appropriate low dimensional image representations. We propose a generic algorithmic framework, which leverages two classic representation learning paradigms, i.e., sparse representation and the trace quotient criterion. The former is a well-known powerful tool to identify underlying self-explanatory factors of data, while the latter is known for disentangling underlying low dimensional discriminative factors in data. Our developed solutions disentangle sparse representations of images by employing the trace quotient criterion. We construct a unified cost function, coined as the SPARse LOW dimensional representation (SparLow) function, for jointly learning both a sparsifying dictionary and a dimensionality reduction transformation. The SparLow function is widely applicable for developing various algorithms in three classic machine learning scenarios, namely, unsupervised, supervised, and semi-supervised learning. In order to develop efficient joint learning algorithms for maximizing the SparLow function, we deploy a framework of sparse coding with appropriate convex priors to ensure the sparse representations to be locally differentiable. Moreover, we develop an efficient geometric conjugate gradient algorithm to maximize the SparLow function on its underlying Riemannian manifold. Performance of the proposed SparLow algorithmic framework is investigated on several image processing tasks, such as 3D data visualization, face/digit recognition, and object/scene categorization.
        △ Less
",work studi problem learn appropri low dimension imag represent propos gener algorithm framework leverag two classic represent learn paradigm e spars represent trace quotient criterion former well known power tool identifi underli self explanatori factor data latter known disentangl underli low dimension discrimin factor data develop solut disentangl spars represent imag employ trace quotient criterion construct unifi cost function coin spars low dimension represent sparlow function jointli learn sparsifi dictionari dimension reduct transform sparlow function wide applic develop variou algorithm three classic machin learn scenario name unsupervis supervis semi supervis learn order develop effici joint learn algorithm maxim sparlow function deploy framework spars code appropri convex prior ensur spars represent local differenti moreov develop effici geometr conjug gradient algorithm maxim sparlow function underli riemannian manifold perform propos sparlow algorithm framework investig sever imag process task data visual face digit recognit object scene categor less
167,1810.03522,"
        This paper introduces NSGA-Net, an evolutionary approach for neural architecture search (NAS). NSGA-Net is designed with three goals in mind: (1) a NAS procedure for multiple, possibly conflicting, objectives, (2) efficient exploration and exploitation of the space of potential neural network architectures, and (3) output of a diverse set of network architectures spanning a trade-off frontier of the objectives in a single run. NSGA-Net is a population-based search algorithm that explores a space of potential neural network architectures in three steps, namely, a population initialization step that is based on prior-knowledge from hand-crafted architectures, an exploration step comprising crossover and mutation of architectures and finally an exploitation step that applies the entire history of evaluated neural architectures in the form of a Bayesian Network prior. Experimental results suggest that combining the objectives of minimizing both an error metric and computational complexity, as measured by FLOPS, allows NSGA-Net to find competitive neural architectures near the Pareto front of both objectives on two different tasks, object classification and object alignment. NSGA-Net obtains networks that achieve 3.72% (at 4.5 million FLOP) error on CIFAR-10 classification and 8.64% (at 26.6 million FLOP) error on the CMU-Car alignment task. Code available at: https://github.com/ianwhale/nsga-net
        △ Less
",paper introduc nsga net evolutionari approach neural architectur search na nsga net design three goal mind na procedur multipl possibl conflict object effici explor exploit space potenti neural network architectur output divers set network architectur span trade frontier object singl run nsga net popul base search algorithm explor space potenti neural network architectur three step name popul initi step base prior knowledg hand craft architectur explor step compris crossov mutat architectur final exploit step appli entir histori evalu neural architectur form bayesian network prior experiment result suggest combin object minim error metric comput complex measur flop allow nsga net find competit neural architectur near pareto front object two differ task object classif object align nsga net obtain network achiev million flop error cifar classif million flop error cmu car align task code avail http github com ianwhal nsga net less
168,1810.03519,"
        In microblog retrieval, query expansion can be essential to obtain good search results due to the short size of queries and posts. Since information in microblogs is highly dynamic, an up-to-date index coupled with pseudo-relevance feedback (PRF) with an external corpus has a higher chance of retrieving more relevant documents and improving ranking. In this paper, we focus on the research question:how can we reduce the query expansion computational cost while maintaining the same retrieval precision as standard PRF? Therefore, we propose to accelerate the query expansion step of pseudo-relevance feedback. The hypothesis is that using an expansion corpus organized into verticals for expanding the query, will lead to a more efficient query expansion process and improved retrieval effectiveness. Thus, the proposed query expansion method uses a distributed search architecture and resource selection algorithms to provide an efficient query expansion process. Experiments on the TREC Microblog datasets show that the proposed approach can match or outperform standard PRF in MAP and NDCG@30, with a computational cost that is three orders of magnitude lower.
        △ Less
",microblog retriev queri expans essenti obtain good search result due short size queri post sinc inform microblog highli dynam date index coupl pseudo relev feedback prf extern corpu higher chanc retriev relev document improv rank paper focu research question reduc queri expans comput cost maintain retriev precis standard prf therefor propos acceler queri expans step pseudo relev feedback hypothesi use expans corpu organ vertic expand queri lead effici queri expans process improv retriev effect thu propos queri expans method use distribut search architectur resourc select algorithm provid effici queri expans process experi trec microblog dataset show propos approach match outperform standard prf map ndcg comput cost three order magnitud lower less
169,1810.03516,"
        Policy search reinforcement learning allows robots to acquire skills by themselves. However, the learning procedure is inherently unsafe as the robot has no a-priori way to predict the consequences of the exploratory actions it takes. Therefore, exploration can lead to collisions with the potential to harm the robot and/or the environment. In this work we address the safety aspect by constraining the exploration to happen in safe-to-explore state spaces. These are formed by decomposing target skills (e.g., grasping) into higher ranked sub-tasks (e.g., collision avoidance, joint limit avoidance) and lower ranked movement tasks (e.g., reaching). Sub-tasks are defined as concurrent controllers (policies) in different operational spaces together with associated Jacobians representing their joint-space mapping. Safety is ensured by only learning policies corresponding to lower ranked sub-tasks in the redundant null space of higher ranked ones. As a side benefit, learning in sub-manifolds of the state-space also facilitates sample efficiency. Reaching skills performed in simulation and grasping skills performed on a real robot validate the usefulness of the proposed approach.
        △ Less
",polici search reinforc learn allow robot acquir skill howev learn procedur inher unsaf robot priori way predict consequ exploratori action take therefor explor lead collis potenti harm robot environ work address safeti aspect constrain explor happen safe explor state space form decompos target skill e g grasp higher rank sub task e g collis avoid joint limit avoid lower rank movement task e g reach sub task defin concurr control polici differ oper space togeth associ jacobian repres joint space map safeti ensur learn polici correspond lower rank sub task redund null space higher rank one side benefit learn sub manifold state space also facilit sampl effici reach skill perform simul grasp skill perform real robot valid use propos approach less
170,1810.03515,"
        We introduce a logic to express structural properties of automata with string inputs and, possibly, outputs in some monoid. In this logic, the set of predicates talking about the output values is parametric, and we provide sufficient conditions on the predicates under which the model-checking problem is decidable. We then consider three particular automata models (finite automata, transducers and automata weighted by integers -- sum-automata --) and instantiate the generic logic for each of them. We give tight complexity results for the three logics and the model-checking problem, depending on whether the formula is fixed or not. We study the expressiveness of our logics by expressing classical structural patterns characterising for instance finite ambiguity and polynomial ambiguity in the case of finite automata, determinisability and finite-valuedness in the case of transducers and sum-automata. Consequently to our complexity results, we directly obtain that these classical properties can be decided in PTIME.
        △ Less
",introduc logic express structur properti automata string input possibl output monoid logic set predic talk output valu parametr provid suffici condit predic model check problem decid consid three particular automata model finit automata transduc automata weight integ sum automata instanti gener logic give tight complex result three logic model check problem depend whether formula fix studi express logic express classic structur pattern characteris instanc finit ambigu polynomi ambigu case finit automata determinis finit valued case transduc sum automata consequ complex result directli obtain classic properti decid ptime less
171,1810.03513,"
        This paper focuses on showing time-message trade-offs in distributed algorithms for fundamental problems such as leader election, broadcast, spanning tree (ST), minimum spanning tree (MST), minimum cut, and many graph verification problems. We consider the synchronous CONGEST distributed computing model and assume that each node has initial knowledge of itself and the identifiers of its neighbors - the so-called KT1 model - a well-studied model that also naturally arises in many applications. Recently, it has been established that one can obtain (almost) singularly optimal algorithms, i.e., algorithms that have simultaneously optimal time and message complexity (up to polylogarithmic factors), for many fundamental problems in the standard KT0 model (where nodes have only local knowledge of themselves and not their neighbors). The situation is less clear in the KT1 model. In this paper, we present several new distributed algorithms in the KT1 model that trade off between time and message complexity.
  Our distributed algorithms are based on a uniform approach which involves constructing a sparsified spanning subgraph of the original graph - called a danner - that trades off the number of edges with the diameter of the sparsifier. In particular, a key ingredient of our approach is a distributed randomized algorithm that, given a graph G and any delta in [0, 1], with high probability constructs a danner that has diameter Otilde(D + n^(1 - delta)) and Otilde(min{m, n^(1 + delta)}) edges in Otilde(n^(1 - delta)) rounds while using Otilde(min{m, n^(1 + delta)}) messages, where n, m, and D are the number of nodes, edges, and the diameter of G, respectively. Using our danner construction, we present a family of distributed randomized algorithms for various fundamental problems that exhibit a trade-off between message and time complexity and that improve over previous results.
        △ Less
",paper focus show time messag trade off distribut algorithm fundament problem leader elect broadcast span tree st minimum span tree mst minimum cut mani graph verif problem consid synchron congest distribut comput model assum node initi knowledg identifi neighbor call kt model well studi model also natur aris mani applic recent establish one obtain almost singularli optim algorithm e algorithm simultan optim time messag complex polylogarithm factor mani fundament problem standard kt model node local knowledg neighbor situat less clear kt model paper present sever new distribut algorithm kt model trade time messag complex distribut algorithm base uniform approach involv construct sparsifi span subgraph origin graph call danner trade number edg diamet sparsifi particular key ingredi approach distribut random algorithm given graph g delta high probabl construct danner diamet otild n delta otild min n delta edg otild n delta round use otild min n delta messag n number node edg diamet g respect use danner construct present famili distribut random algorithm variou fundament problem exhibit trade messag time complex improv previou result less
172,1810.03510,"
        Covert communication is necessary when revealing the mere existence of a message leaks sensitive information to an attacker. Consider a network link where an authorized transmitter Jack sends packets to an authorized receiver Steve, and the packets visit Alice, Willie, and Bob, respectively, before they reach Steve. Covert transmitter Alice wishes to alter the packet stream in some way to send information to covert receiver Bob without watchful and capable adversary Willie being able to detect the presence of the message. In our previous works, we addressed two techniques for such covert transmission from Alice to Bob: packet insertion and packet timing. In this paper, we consider covert communication via bit insertion in packets with available space (e.g., with size less than the maximum transmission unit). We consider three scenarios: 1) packet sizes are independent and identically distributed (i.i.d.) with a probability mass function (pmf) whose support is a set of one bit spaced values; 2) packet sizes are i.i.d. with a pmf whose support is arbitrary; 3) packet sizes may be dependent. For the first and second assumptions, we show that Alice can covertly insert $\mathcal{O}(\sqrt{n})$ bits of information in a flow of $n$ packets; conversely, if she inserts $ω(\sqrt{n})$ bits of information, Willie can detect her with arbitrarily small error probability. For the third assumption, we prove Alice can covertly insert on average $\mathcal{O}(c(n)/\sqrt{n})$ bits in a sequence of $n$ packets, where $c(n)$ is the average number of conditional pmf of packet sizes given the history, with a support of at least size two.
        △ Less
",covert commun necessari reveal mere exist messag leak sensit inform attack consid network link author transmitt jack send packet author receiv steve packet visit alic willi bob respect reach steve covert transmitt alic wish alter packet stream way send inform covert receiv bob without watch capabl adversari willi abl detect presenc messag previou work address two techniqu covert transmiss alic bob packet insert packet time paper consid covert commun via bit insert packet avail space e g size less maximum transmiss unit consid three scenario packet size independ ident distribut probabl mass function pmf whose support set one bit space valu packet size pmf whose support arbitrari packet size may depend first second assumpt show alic covertli insert mathcal sqrt n bit inform flow n packet convers insert sqrt n bit inform willi detect arbitrarili small error probabl third assumpt prove alic covertli insert averag mathcal c n sqrt n bit sequenc n packet c n averag number condit pmf packet size given histori support least size two less
173,1810.03506,"
        This work introduces an innovative parallel, fully-distributed finite element framework for growing geometries and its application to metal additive manufacturing. It is well-known that virtual part design and qualification in additive manufacturing requires highly-accurate multiscale and multiphysics analyses. Only high performance computing tools are able to handle such complexity in time frames compatible with time-to-market. However, efficiency, without loss of accuracy, has rarely been the focus in the numerical community. Here, in contrast, the framework is designed to adequately exploit the resources of high-end distributed-memory machines. It is grounded on three building blocks: (1) Hierarchical adaptive mesh refinement with octree-based meshes; (2) a parallel strategy to model the geometry growth; (3) the customization of a parallel iterative linear solver, which leverages the so-called balancing domain decomposition by constraints preconditioning approach for fast convergence and high parallel scalability. Computational experiments consider the part-scale thermal analysis of the printing process by powder-bed technologies. After verification against a 3D benchmark, a strong scaling analysis is carried out for a simulation of 48 layers printed in a cuboid. The cuboid is adaptively meshed to model a layer-by-layer metal deposition process and the average global problem size amounts to 10.3 million unknowns. An unprecedented scalability for problems with growing domains is achieved, with the capability of simulating the printing and recoat of a single layer in 8 seconds average on 3,072 processors. Hence, this framework contributes to take on higher complexity and/or accuracy, not only of part-scale simulations of metal or polymer additive manufacturing, but also in welding, sedimentation, atherosclerosis, or any other physical problem with growing-in-time geometries.
        △ Less
",work introduc innov parallel fulli distribut finit element framework grow geometri applic metal addit manufactur well known virtual part design qualif addit manufactur requir highli accur multiscal multiphys analys high perform comput tool abl handl complex time frame compat time market howev effici without loss accuraci rare focu numer commun contrast framework design adequ exploit resourc high end distribut memori machin ground three build block hierarch adapt mesh refin octre base mesh parallel strategi model geometri growth custom parallel iter linear solver leverag call balanc domain decomposit constraint precondit approach fast converg high parallel scalabl comput experi consid part scale thermal analysi print process powder bed technolog verif benchmark strong scale analysi carri simul layer print cuboid cuboid adapt mesh model layer layer metal deposit process averag global problem size amount million unknown unpreced scalabl problem grow domain achiev capabl simul print recoat singl layer second averag processor henc framework contribut take higher complex accuraci part scale simul metal polym addit manufactur also weld sediment atherosclerosi physic problem grow time geometri less
174,1810.03505,"
        In this brief technical report we introduce the CINIC-10 dataset as a plug-in extended alternative for CIFAR-10. It was compiled by combining CIFAR-10 with images selected and downsampled from the ImageNet database. We present the approach to compiling the dataset, illustrate the example images for different classes, give pixel distributions for each part of the repository, and give some standard benchmarks for well known models. Details for download, usage, and compilation can be found in the associated github repository.
        △ Less
",brief technic report introduc cinic dataset plug extend altern cifar compil combin cifar imag select downsampl imagenet databas present approach compil dataset illustr exampl imag differ class give pixel distribut part repositori give standard benchmark well known model detail download usag compil found associ github repositori less
175,1810.03498,"
        In this work, we use percolation theory to study the feasibility of large-scale connectivity of relay-augmented device-to-device (D2D) networks in an urban scenario, featuring a haphazard system of streets and canyon shadowing allowing only for line-of-sight (LOS) communications in a limited finite range. We use a homogeneous Poisson-Voronoi tessellation (PVT) model of streets with homogeneous Poisson users (devices) on its edges and independent Bernoulli relays on the vertices. Using this model, we demonstrated the existence of a minimal threshold for relays below which large-scale connectivity of the network is not possible, regardless of all other network parameters. Through simulations, we estimated this threshold to 71.3%. Moreover, if the mean street length is not larger than some threshold (predicted to 74.3% of the communication range; which might be the case in a typical urban scenario) then any (whatever small) density of users can be compensated by equipping more crossroads with relays. Above this latter threshold, good connectivity requires some minimal density of users, compensated by the relays in a way we make explicit. The existence of the above regimes brings interesting qualitative arguments to the discussion on the possible D2D deployment scenarios.
        △ Less
",work use percol theori studi feasibl larg scale connect relay augment devic devic network urban scenario featur haphazard system street canyon shadow allow line sight lo commun limit finit rang use homogen poisson voronoi tessel pvt model street homogen poisson user devic edg independ bernoulli relay vertic use model demonstr exist minim threshold relay larg scale connect network possibl regardless network paramet simul estim threshold moreov mean street length larger threshold predict commun rang might case typic urban scenario whatev small densiti user compens equip crossroad relay latter threshold good connect requir minim densiti user compens relay way make explicit exist regim bring interest qualit argument discuss possibl deploy scenario less
176,1810.03491,"
        We consider the problem of incremental cycle detection and topological ordering in a directed graph $G = (V, E)$ with $|V| = n$ nodes. In this setting, initially the edge-set $E$ of the graph is empty. Subsequently, at each time-step an edge gets inserted into $G$. After every edge-insertion, we have to report if the current graph contains a cycle, and as long as the graph remains acyclic, we have to maintain a topological ordering of the node-set $V$. Let $m$ be the total number of edges that get inserted into $G$. We present a randomized algorithm for this problem with $\tilde{O}(m^{4/3})$ total expected update time.
        △ Less
",consid problem increment cycl detect topolog order direct graph g v e v n node set initi edg set e graph empti subsequ time step edg get insert g everi edg insert report current graph contain cycl long graph remain acycl maintain topolog order node set v let total number edg get insert g present random algorithm problem tild total expect updat time less
177,1810.03488,"
        We propose a coded distributed computing scheme based on Raptor codes to address the straggler problem. In particular, we consider a scheme where each server computes intermediate values, referred to as droplets, that are either stored locally or sent over the network. Once enough droplets are collected, the computation can be completed. Compared to previous schemes in the literature, our proposed scheme achieves lower computational delay when the decoding time is taken into account.
        △ Less
",propos code distribut comput scheme base raptor code address straggler problem particular consid scheme server comput intermedi valu refer droplet either store local sent network enough droplet collect comput complet compar previou scheme literatur propos scheme achiev lower comput delay decod time taken account less
178,1810.03487,"
        Recent work has introduced attacks that extract the architecture information of deep neural networks (DNN), as this knowledge enhances an adversary's capability to conduct black-box attacks against the model. This paper presents the first in-depth security analysis of DNN fingerprinting attacks that exploit cache side-channels. First, we define the threat model for these attacks: our adversary does not need the ability to query the victim model; instead, she runs a co-located process on the host machine victim's deep learning (DL) system is running and passively monitors the accesses of the target functions in the shared framework. Second, we introduce DeepRecon, an attack that reconstructs the architecture of the victim network by using the internal information extracted via Flush+Reload, a cache side-channel technique. Once the attacker observes function invocations that map directly to architecture attributes of the victim network, the attacker can reconstruct the victim's entire network architecture. In our evaluation, we demonstrate that an attacker can accurately reconstruct two complex networks (VGG19 and ResNet50) having observed only one forward propagation. Based on the extracted architecture attributes, we also demonstrate that an attacker can build a meta-model that accurately fingerprints the architecture and family of the pre-trained model in a transfer learning setting. From this meta-model, we evaluate the importance of the observed attributes in the fingerprinting process. Third, we propose and evaluate new framework-level defense techniques that obfuscate our attacker's observations. Our empirical security analysis represents a step toward understanding the DNNs' vulnerability to cache side-channel attacks.
        △ Less
",recent work introduc attack extract architectur inform deep neural network dnn knowledg enhanc adversari capabl conduct black box attack model paper present first depth secur analysi dnn fingerprint attack exploit cach side channel first defin threat model attack adversari need abil queri victim model instead run co locat process host machin victim deep learn dl system run passiv monitor access target function share framework second introduc deeprecon attack reconstruct architectur victim network use intern inform extract via flush reload cach side channel techniqu attack observ function invoc map directli architectur attribut victim network attack reconstruct victim entir network architectur evalu demonstr attack accur reconstruct two complex network vgg resnet observ one forward propag base extract architectur attribut also demonstr attack build meta model accur fingerprint architectur famili pre train model transfer learn set meta model evalu import observ attribut fingerprint process third propos evalu new framework level defens techniqu obfusc attack observ empir secur analysi repres step toward understand dnn vulner cach side channel attack less
179,1810.03480,"
        Text analytics based on supervised machine learning classifiers has shown great promise in a multitude of domains, but has yet to be applied to Seismology. We test various standard models (Naive Bayes, k-Nearest Neighbors, Support Vector Machines, and Random Forests) on a seismological corpus of 100 articles related to the topic of precursory accelerating seismicity, spanning from 1988 to 2010. This corpus was labelled in Mignan (2011) with the precursor whether explained by critical processes (i.e., cascade triggering) or by other processes (such as signature of main fault loading). We investigate rather the classification process can be automatized to help analyze larger corpora in order to better understand trends in earthquake predictability research. We find that the Naive Bayes model performs best, in agreement with the machine learning literature for the case of small datasets, with cross-validation accuracies of 86% for binary classification. For a refined multiclass classification ('non-critical process' < 'agnostic' < 'critical process assumed' < 'critical process demonstrated'), we obtain up to 78% accuracy. Prediction on a dozen of articles published since 2011 shows however a weak generalization with a F1-score of 60%, only slightly better than a random classifier, which can be explained by a change of authorship and use of different terminologies. Yet, the model shows F1-scores greater than 80% for the two multiclass extremes ('non-critical process' versus 'critical process demonstrated') while it falls to random classifier results (around 25%) for papers labelled 'agnostic' or 'critical process assumed'. Those results are encouraging in view of the small size of the corpus and of the high degree of abstraction of the labelling. Domain knowledge engineering remains essential but can be made transparent by an investigation of Naive Bayes keyword posterior probabilities.
        △ Less
",text analyt base supervis machin learn classifi shown great promis multitud domain yet appli seismolog test variou standard model naiv bay k nearest neighbor support vector machin random forest seismolog corpu articl relat topic precursori acceler seismic span corpu label mignan precursor whether explain critic process e cascad trigger process signatur main fault load investig rather classif process automat help analyz larger corpora order better understand trend earthquak predict research find naiv bay model perform best agreement machin learn literatur case small dataset cross valid accuraci binari classif refin multiclass classif non critic process agnost critic process assum critic process demonstr obtain accuraci predict dozen articl publish sinc show howev weak gener f score slightli better random classifi explain chang authorship use differ terminolog yet model show f score greater two multiclass extrem non critic process versu critic process demonstr fall random classifi result around paper label agnost critic process assum result encourag view small size corpu high degre abstract label domain knowledg engin remain essenti made transpar investig naiv bay keyword posterior probabl less
180,1810.03479,"
        In this paper, we develop a low than character feature embedding called radical embedding, and apply it on LSTM model for sentence segmentation of pre modern Chinese texts. The datasets includes over 150 classical Chinese books from 3 different dynasties and contains different literary styles. LSTM CRF model is a state of art method for the sequence labeling problem. Our new model adds a component of radical embedding, which leads to improved performances. Experimental results based on the aforementioned Chinese books demonstrates a better accuracy than earlier methods on sentence segmentation, especial in Tang Epitaph texts.
        △ Less
",paper develop low charact featur embed call radic embed appli lstm model sentenc segment pre modern chines text dataset includ classic chines book differ dynasti contain differ literari style lstm crf model state art method sequenc label problem new model add compon radic embed lead improv perform experiment result base aforement chines book demonstr better accuraci earlier method sentenc segment especi tang epitaph text less
181,1810.03476,"
        Relaying techniques for millimeter-wave wireless networks represent a powerful solution for improving the transmission performance. In this work, we quantify the benefits in terms of delay and throughput for a random-access multi-user millimeter-wave wireless network, assisted by a full-duplex network cooperative relay. The relay is equipped with a queue for which we analyze the performance characteristics (e.g., arrival rate, service rate, average size, and stability condition). Moreover, we study two possible transmission schemes: fully directional and broadcast. In the former, the source nodes transmit a packet either to the relay or to the destination by using narrow beams, whereas, in the latter, the nodes transmit to both the destination and the relay in the same timeslot by using a wider beam, but with lower beamforming gain. In our analysis, we also take into account the beam alignment phase that occurs every time a transmitter node changes the destination node. We show how the beam alignment duration, as well as position and number of transmitting nodes, significantly affect the network performance. Moreover, we illustrate the optimal transmission scheme (i.e., broadcast or fully directional) for several system parameters and show that a fully directional transmission is not always beneficial, but, in some scenarios, broadcasting and relaying can improve the performance in terms of throughput and delay.
        △ Less
",relay techniqu millimet wave wireless network repres power solut improv transmiss perform work quantifi benefit term delay throughput random access multi user millimet wave wireless network assist full duplex network cooper relay relay equip queue analyz perform characterist e g arriv rate servic rate averag size stabil condit moreov studi two possibl transmiss scheme fulli direct broadcast former sourc node transmit packet either relay destin use narrow beam wherea latter node transmit destin relay timeslot use wider beam lower beamform gain analysi also take account beam align phase occur everi time transmitt node chang destin node show beam align durat well posit number transmit node significantli affect network perform moreov illustr optim transmiss scheme e broadcast fulli direct sever system paramet show fulli direct transmiss alway benefici scenario broadcast relay improv perform term throughput delay less
182,1810.03473,"
        The mobile femtocell is the new paradigm for the femtocellular network deployment. It can enhance the service quality for the users inside the vehicles. The deployment of mobile femtocells generates lot of handover calls. Also, number of group handover scenarios are found in mobile femtocellular network deployment. In this paper, we focus on the resource management for the group handover in mobile femtocellular network deployment. We discuss a number of group handover scenarios. We propose a resource management scheme that contains bandwidth adaptation policy and dynamic bandwidth reservation policy. The simulation results show that the proposed bandwidth management scheme significantly reduces the handover call dropping probability without reducing the bandwidth utilization.
        △ Less
",mobil femtocel new paradigm femtocellular network deploy enhanc servic qualiti user insid vehicl deploy mobil femtocel gener lot handov call also number group handov scenario found mobil femtocellular network deploy paper focu resourc manag group handov mobil femtocellular network deploy discuss number group handov scenario propos resourc manag scheme contain bandwidth adapt polici dynam bandwidth reserv polici simul result show propos bandwidth manag scheme significantli reduc handov call drop probabl without reduc bandwidth util less
183,1810.03470,"
        Multicast/broadcast services (MBS) are able to provide video services for many users simultaneously. Fixed amount of bandwidth allocation for all of the MBS videos is not effective in terms of bandwidth utilization, overall forced call termination probability, and handover call dropping probability. Therefore, variable bandwidth allocation for the MBS videos can efficiently improve the system performance. In this paper, we propose a bandwidth allocation scheme that efficiently allocates bandwidth among the MBS sessions and the non-MBS traffic calls (e.g., voice, unicast, internet, and other background traffic). The proposed scheme reduces the bandwidth allocation for the MBS sessions during the congested traffic condition only to accommodate more number of calls in the system. Our scheme allocates variable amount of bandwidths for the BMS sessions and the non-MBS traffic calls. The performance analyses show that the proposed bandwidth adaptation scheme maximizes the bandwidth utilization and significantly reduces the handover call dropping probability and overall forced call termination probability.
        △ Less
",multicast broadcast servic mb abl provid video servic mani user simultan fix amount bandwidth alloc mb video effect term bandwidth util overal forc call termin probabl handov call drop probabl therefor variabl bandwidth alloc mb video effici improv system perform paper propos bandwidth alloc scheme effici alloc bandwidth among mb session non mb traffic call e g voic unicast internet background traffic propos scheme reduc bandwidth alloc mb session congest traffic condit accommod number call system scheme alloc variabl amount bandwidth bm session non mb traffic call perform analys show propos bandwidth adapt scheme maxim bandwidth util significantli reduc handov call drop probabl overal forc call termin probabl less
184,1810.03469,"
        The femtocell networks that use home base station and existing xDSL or other cable line as backhaul connectivity can fulfill the upcoming demand of high data rate for wireless communication system as well as can extend the coverage area. Hence the modified handover procedure for existing networks is needed to support the macrocell/femtocell integrated network. Some modifications of existing network and protocol architecture for the integration of femtocell networks with the existing UMTS based macrocell networks are essential. These modifications change the signal flow for handover procedures due to different 2-tier cell (macrocell and femtocell) environment. The measurement of signal-to-interference ratio parameter should be considered for handover between macrocell and femtocell. A frequent and unnecessary handover is another problem for hierarchical network environment that must be optimized to improve the performance of macrocell/femtocell integrated network.
  In this paper, firstly we propose the concentrator based and without concentrator based femtocell network architecture. Then we present the signal flow with appropriate parameters for the handover between 3GPP UMTS based macrocell and femtocell networks. A scheme for unnecessary handoff minimization is also presented in this paper. We simulate the proposed handover optimization scheme to validate the performance.
        △ Less
",femtocel network use home base station exist xdsl cabl line backhaul connect fulfil upcom demand high data rate wireless commun system well extend coverag area henc modifi handov procedur exist network need support macrocel femtocel integr network modif exist network protocol architectur integr femtocel network exist umt base macrocel network essenti modif chang signal flow handov procedur due differ tier cell macrocel femtocel environ measur signal interfer ratio paramet consid handov macrocel femtocel frequent unnecessari handov anoth problem hierarch network environ must optim improv perform macrocel femtocel integr network paper firstli propos concentr base without concentr base femtocel network architectur present signal flow appropri paramet handov gpp umt base macrocel femtocel network scheme unnecessari handoff minim also present paper simul propos handov optim scheme valid perform less
185,1810.03468,"
        The multiple choices of access networks offer different opportunities and overcome the limitations of other technologies. Optimal selection of interface is a big challenge for multiple interfaces supported mobile terminals to make a seamless handover and to optimize the power consumption. Seamless handover, resource management, and CAC to support QoS and multiple interface management to reduce power consumption in mobile terminal are the most important issues for the UMTS/WLAN overlaying network. The access of both interfaces simultaneously can reduce the handover latency and data loss in heterogeneous handover. The MN may maintain one interface connection while other interface can be used for handover process. But the access of both interfaces increases the consumption of power in MN. In this paper we present an efficient interface selection scheme including interface selection algorithms, interface selection mechanism and CAC considering battery power consumption for overlaying networks. MN's battery power level and provision of QoS/QoE in the target interface are considered as important parameters for our interface selection algorithm.
        △ Less
",multipl choic access network offer differ opportun overcom limit technolog optim select interfac big challeng multipl interfac support mobil termin make seamless handov optim power consumpt seamless handov resourc manag cac support qo multipl interfac manag reduc power consumpt mobil termin import issu umt wlan overlay network access interfac simultan reduc handov latenc data loss heterogen handov mn may maintain one interfac connect interfac use handov process access interfac increas consumpt power mn paper present effici interfac select scheme includ interfac select algorithm interfac select mechan cac consid batteri power consumpt overlay network mn batteri power level provis qo qoe target interfac consid import paramet interfac select algorithm less
186,1810.03466,"
        This paper proposes a two-stage scoring approach to help lenders decide their fund allocations in the peer-to-peer (P2P) lending market. The existing scoring approaches focus on only either probability of default (PD) prediction, known as credit scoring, or profitability prediction, known as profit scoring, to identify the best loans for investment. Credit scoring fails to deliver the main need of lenders on how much profit they may obtain through their investment. On the other hand, profit scoring can satisfy that need by predicting the investment profitability. However, profit scoring completely ignores the class imbalance problem where most of the past loans are non-default. Consequently, ignorance of the class imbalance problem significantly affects the accuracy of profitability prediction. Our proposed two-stage scoring approach is an integration of credit scoring and profit scoring to address the above challenges. More specifically, stage 1 is designed as credit scoring to identify non-default loans while the imbalanced nature of loan status is considered in PD prediction. The loans identified as non-default are then moved to stage 2 for prediction of profitability, measured by internal rate of return. Wide and deep learning is used to build the predictive models in both stages to achieve both memorization and generalization. Extensive numerical studies are conducted based on real-world data to verify the effectiveness of the proposed approach. The numerical studies indicate our two-stage scoring approach outperforms the existing credit scoring and profit scoring approaches.
        △ Less
",paper propos two stage score approach help lender decid fund alloc peer peer p p lend market exist score approach focu either probabl default pd predict known credit score profit predict known profit score identifi best loan invest credit score fail deliv main need lender much profit may obtain invest hand profit score satisfi need predict invest profit howev profit score complet ignor class imbal problem past loan non default consequ ignor class imbal problem significantli affect accuraci profit predict propos two stage score approach integr credit score profit score address challeng specif stage design credit score identifi non default loan imbalanc natur loan statu consid pd predict loan identifi non default move stage predict profit measur intern rate return wide deep learn use build predict model stage achiev memor gener extens numer studi conduct base real world data verifi effect propos approach numer studi indic two stage score approach outperform exist credit score profit score approach less
187,1810.03464,"
        Advanced Persistent Threat (APT) attacks are sophisticated and stealthy, exploiting multiple software vulnerabilities and plaguing many well-protected businesses with significant financial losses. Due to the complexity introduced by numerous installed software applications and the limited visibility into their behaviors, enterprises are seeking solutions to connect and investigate risky software behaviors across software applications. In this demo, we present AIQL, a tool for investigating complex risky software behaviors via interactive queries. To obtain a global view of software behaviors, AIQL is built upon ubiquitous system monitoring, which records interactions among software applications and system resources. In particular, AIQL provides: (1) domain-specific data model and storage for storing the massive system monitoring data, (2) a domain-specific query language, Attack Investigation Query Language, which integrates critical primitives for risky behavior specification, and (3) an optimized query engine based on the characteristics of the data and the query to efficiently schedule the execution. Demo URL: https://youtu.be/2dDVngg0UN8
        △ Less
",advanc persist threat apt attack sophist stealthi exploit multipl softwar vulner plagu mani well protect busi signific financi loss due complex introduc numer instal softwar applic limit visibl behavior enterpris seek solut connect investig riski softwar behavior across softwar applic demo present aiql tool investig complex riski softwar behavior via interact queri obtain global view softwar behavior aiql built upon ubiquit system monitor record interact among softwar applic system resourc particular aiql provid domain specif data model storag store massiv system monitor data domain specif queri languag attack investig queri languag integr critic primit riski behavior specif optim queri engin base characterist data queri effici schedul execut demo url http youtu ddvngg un less
188,1810.03459,"
        Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multi-lingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.
        △ Less
",sequenc sequenc seq seq approach low resourc asr rel new direct speech research approach benefit perform model train without use lexicon align howev pose new problem requir data compar convent dnn hmm system work attempt use data babel languag build multi lingual seq seq model prior model port toward babel languag use transfer learn approach also explor differ architectur improv prior multilingu seq seq model paper also discuss effect integr recurr neural network languag model rnnlm seq seq model decod experiment result show transfer learn approach multilingu model show substanti gain monolingu model across babel languag incorpor rnnlm also bring signific improv term wer achiev recognit perform compar model train twice train data less
189,1810.03450,"
        We explore active learning (AL) utterance selection for improving the accuracy of new underrepresented domains in a natural language understanding (NLU) system. Moreover, we propose an AL algorithm called Majority-CRF that uses an ensemble of classification and sequence labeling models to guide utterance selection for annotation. Experiments with three domains show that Majority-CRF achieves 6.6%-9% relative error rate reduction compared to random sampling with the same annotation budget, and statistically significant improvements compared to other AL approaches. Additionally, case studies with human-in-the-loop AL on six new domains show 4.6%-9% improvement on an existing NLU system.
        △ Less
",explor activ learn al utter select improv accuraci new underrepres domain natur languag understand nlu system moreov propos al algorithm call major crf use ensembl classif sequenc label model guid utter select annot experi three domain show major crf achiev rel error rate reduct compar random sampl annot budget statist signific improv compar al approach addit case studi human loop al six new domain show improv exist nlu system less
190,1810.03449,"
        The task of event detection involves identifying and categorizing event triggers. Contextual information has been shown effective on the task. However, existing methods which utilize contextual information only process the context once. We argue that the context can be better exploited by processing the context multiple times, allowing the model to perform complex reasoning and to generate better context representation, thus improving the overall performance. Meanwhile, dynamic memory network (DMN) has demonstrated promising capability in capturing contextual information and has been applied successfully to various tasks. In light of the multi-hop mechanism of the DMN to model the context, we propose the trigger detection dynamic memory network (TD-DMN) to tackle the event detection problem. We performed a five-fold cross-validation on the ACE-2005 dataset and experimental results show that the multi-hop mechanism does improve the performance and the proposed model achieves best $F_1$ score compared to the state-of-the-art methods.
        △ Less
",task event detect involv identifi categor event trigger contextu inform shown effect task howev exist method util contextu inform process context argu context better exploit process context multipl time allow model perform complex reason gener better context represent thu improv overal perform meanwhil dynam memori network dmn demonstr promis capabl captur contextu inform appli success variou task light multi hop mechan dmn model context propos trigger detect dynam memori network td dmn tackl event detect problem perform five fold cross valid ace dataset experiment result show multi hop mechan improv perform propos model achiev best f score compar state art method less
191,1810.03445,"
        In this paper, we try to explore the evolution of language through case calculations. First, we chose the novels of eleven British writers from 1400 to 2005 and found the corresponding works; Then, we use the natural language processing tool to construct the corresponding eleven corpora, and calculate the respective word vectors of 100 high-frequency words in eleven corpora; Next, for each corpus, we concatenate the 100 word vectors from beginning to end into one; Finally, we use the similarity comparison and hierarchical clustering method to generate the relationship tree between the combined eleven word vectors. This tree represents the relationship between eleven corpora. We found that in the tree generated by clustering, the distance between the corpus and the year corresponding to the corpus are basically the same. This means that we have discovered a specific language evolution tree. To verify the stability and versatility of this method, we add three other themes: Dickens's eight works, the 19th century poets' works, and art criticism of recent 60 years. For these four themes, we tested different parameters such as the time span of the corpus, the time interval between the corpora, the dimension of the word vector, and the number of high-frequency public words. The results show that this is fairly stable and versatile.
        △ Less
",paper tri explor evolut languag case calcul first chose novel eleven british writer found correspond work use natur languag process tool construct correspond eleven corpora calcul respect word vector high frequenc word eleven corpora next corpu concaten word vector begin end one final use similar comparison hierarch cluster method gener relationship tree combin eleven word vector tree repres relationship eleven corpora found tree gener cluster distanc corpu year correspond corpu basic mean discov specif languag evolut tree verifi stabil versatil method add three theme dicken eight work th centuri poet work art critic recent year four theme test differ paramet time span corpu time interv corpora dimens word vector number high frequenc public word result show fairli stabl versatil less
192,1810.03444,"
        Most state-of-the-art neural machine translation systems, despite being different in architectural skeletons (e.g. recurrence, convolutional), share an indispensable feature: the Attention. However, most existing attention methods are token-based and ignore the importance of phrasal alignments, the key ingredient for the success of phrase-based statistical machine translation. In this paper, we propose novel phrase-based attention methods to model n-grams of tokens as attention entities. We incorporate our phrase-based attentions into the recently proposed Transformer network, and demonstrate that our approach yields improvements of 1.3 BLEU for English-to-German and 0.5 BLEU for German-to-English translation tasks on WMT newstest2014 using WMT'16 training data.
        △ Less
",state art neural machin translat system despit differ architectur skeleton e g recurr convolut share indispens featur attent howev exist attent method token base ignor import phrasal align key ingredi success phrase base statist machin translat paper propos novel phrase base attent method model n gram token attent entiti incorpor phrase base attent recent propos transform network demonstr approach yield improv bleu english german bleu german english translat task wmt newstest use wmt train data less
193,1810.03436,"
        In this paper we evaluate Optical Character Recognition (OCR) of 19th century Fraktur scripts without book-specific training using mixed models, i.e. models trained to recognize a variety of fonts and typesets from previously unseen sources. We describe the training process leading to strong mixed OCR models and compare them to freely available models of the popular open source engines OCRopus and Tesseract as well as the commercial state of the art system ABBYY. For evaluation, we use a varied collection of unseen data from books, journals, and a dictionary from the 19th century. The experiments show that training mixed models with real data is superior to training with synthetic data and that the novel OCR engine Calamari outperforms the other engines considerably, on average reducing ABBYYs character error rate (CER) by over 70%, resulting in an average CER below 1%.
        △ Less
",paper evalu optic charact recognit ocr th centuri fraktur script without book specif train use mix model e model train recogn varieti font typeset previous unseen sourc describ train process lead strong mix ocr model compar freeli avail model popular open sourc engin ocropu tesseract well commerci state art system abbyy evalu use vari collect unseen data book journal dictionari th centuri experi show train mix model real data superior train synthet data novel ocr engin calamari outperform engin consider averag reduc abbyy charact error rate cer result averag cer less
194,1810.03430,"
        The text generated on social media platforms is essentially a mixed lingual text. The mixing of language in any form produces considerable amount of difficulty in language processing systems. Moreover, the advancements in language processing research depends upon the availability of standard corpora. The development of mixed lingual Indian Named Entity Recognition (NER) systems are facing obstacles due to unavailability of the standard evaluation corpora. Such corpora may be of mixed lingual nature in which text is written using multiple languages predominantly using a single script only. The motivation of our work is to emphasize the automatic generation such kind of corpora in order to encourage mixed lingual Indian NER. The paper presents the preparation of a Cross Script Hindi-English Corpora from Wikipedia category pages. The corpora is successfully annotated using standard CoNLL-2003 categories of PER, LOC, ORG, and MISC. Its evaluation is carried out on a variety of machine learning algorithms and favorable results are achieved.
        △ Less
",text gener social media platform essenti mix lingual text mix languag form produc consider amount difficulti languag process system moreov advanc languag process research depend upon avail standard corpora develop mix lingual indian name entiti recognit ner system face obstacl due unavail standard evalu corpora corpora may mix lingual natur text written use multipl languag predominantli use singl script motiv work emphas automat gener kind corpora order encourag mix lingual indian ner paper present prepar cross script hindi english corpora wikipedia categori page corpora success annot use standard conll categori per loc org misc evalu carri varieti machin learn algorithm favor result achiev less
195,1810.03428,"
        Brain Computer Interfaces (BCIs) based on visual evoked potentials (VEP) allow for spelling from a keyboard of flashing characters. Among VEP BCIs, code-modulated visual evoked potentials (c-VEPs) are designed for high-speed communication . In c-VEPs, all characters flash simultaneously. In particular, each character flashes according to a predefined 63-bit binary sequence (m-sequence), circular-shifted by a different time lag. For a given character, the m-sequence evokes a VEP in the electroencephalogram (EEG) of the subject, which can be used as a template. This template is obtained during a calibration phase at the beginning of each session. Then, the system outputs the desired character after a predefined number of repetitions by estimating its time lag with respect to the template. Our work avoids the calibration phase, by extracting from the VEP relative lags between successive characters, and predicting the full word using a dictionary.
        △ Less
",brain comput interfac bci base visual evok potenti vep allow spell keyboard flash charact among vep bci code modul visual evok potenti c vep design high speed commun c vep charact flash simultan particular charact flash accord predefin bit binari sequenc sequenc circular shift differ time lag given charact sequenc evok vep electroencephalogram eeg subject use templat templat obtain calibr phase begin session system output desir charact predefin number repetit estim time lag respect templat work avoid calibr phase extract vep rel lag success charact predict full word use dictionari less
196,1810.03427,"
        A detection system with a single sensor and two detectors is considered, where each of the terminals observes a memoryless source sequence, the sensor sends a message to both detectors and the first detector sends a message to the second detector. Communication of these messages is assumed to be error-free but rate-limited. The joint probability mass function (pmf) of the source sequences observed at the three terminals depends on an $\mathsf{M}$-ary hypothesis $(\mathsf{M} \geq 2)$, and the goal of the communication is that each detector can guess the underlying hypothesis. Detector $k$, $k=1,2$, aims to maximize the error exponent \textit{under hypothesis} $i_k$, $i_k \in \{1,\ldots,\mathsf{M}\}$, while ensuring a small probability of error under all other hypotheses. We study this problem in the case in which the detectors aim to maximize their error exponents under the \textit{same} hypothesis (i.e., $i_1=i_2$) and in the case in which they aim to maximize their error exponents under \textit{distinct} hypotheses (i.e., $i_1 \neq i_2$). For the setting in which $i_1=i_2$, we present an achievable exponents region for the case of positive communication rates, and show that it is optimal for a specific case of testing against independence. We also characterize the optimal exponents region in the case of zero communication rates. For the setting in which $i_1 \neq i_2$, we characterize the optimal exponents region in the case of zero communication rates.
        △ Less
",detect system singl sensor two detector consid termin observ memoryless sourc sequenc sensor send messag detector first detector send messag second detector commun messag assum error free rate limit joint probabl mass function pmf sourc sequenc observ three termin depend mathsf ari hypothesi mathsf geq goal commun detector guess underli hypothesi detector k k aim maxim error expon textit hypothesi k k ldot mathsf ensur small probabl error hypothes studi problem case detector aim maxim error expon textit hypothesi e case aim maxim error expon textit distinct hypothes e neq set present achiev expon region case posit commun rate show optim specif case test independ also character optim expon region case zero commun rate set neq character optim expon region case zero commun rate less
197,1810.03426,"
        In an increasingly connected and networked world, the National Aeronautics and Space Administration (NASA) recognizes the value of the public as a strategic partner in addressing some of our most pressing challenges. The agency is working to more effectively harness the expertise, ingenuity, and creativity of individual members of the public by enabling, accelerating, and scaling the use of open innovation approaches including prizes, challenges, and crowdsourcing. As NASA's use of open innovation tools to solve a variety of types of problems and advance of number of outcomes continues to grow, challenge design is also becoming more sophisticated as our expertise and capacity (personnel, platforms, and partners) grows and develops. NASA has recently pivoted from talking about the benefits of challenge-driven approaches, to the outcomes these types of activities yield. Challenge design should be informed by desired outcomes that align with NASA's mission. This paper provides several case studies of NASA open innovation activities and maps the outcomes of those activities to a successful set of outcomes that challenges can help drive alongside traditional tools such as contracts, grants and partnerships.
        △ Less
",increasingli connect network world nation aeronaut space administr nasa recogn valu public strateg partner address press challeng agenc work effect har expertis ingenu creativ individu member public enabl acceler scale use open innov approach includ prize challeng crowdsourc nasa use open innov tool solv varieti type problem advanc number outcom continu grow challeng design also becom sophist expertis capac personnel platform partner grow develop nasa recent pivot talk benefit challeng driven approach outcom type activ yield challeng design inform desir outcom align nasa mission paper provid sever case studi nasa open innov activ map outcom activ success set outcom challeng help drive alongsid tradit tool contract grant partnership less
198,1810.03423,"
        Probabilistic argumentation is an alternative to causal modeling with Bayesian networks. Probabilistic argumentation structures (PAS) are defined on families of compatible frames (f.c.f). This is a generalization of the usual multivariate models based on families of variables. The crucial relation of conditional independence between frames of a f.c.f is introduced and shown to form a quasi-separoid, a weakening of the well-known structure of a separoid. It is shown that PAS generate probability potentials on the frames of the f.c.f. The operations of aggregating different PAS and of transport of a PAS from one frame to another induce an algebraic structure on the family of potentials on the f.c.f, an algebraic structure which is similar to valuation algebras related to Bayesian networks, but more general. As a consequence the well-known local computation architectures of Bayesian networks for inference apply also for the potentials on f.c.f. Conditioning and conditionals can be defined for potentials and it is shown that these concepts satisfy similar properties as conditional probability distributions. Finally a max/prod algebra between potentials is defined and applied to find most probable configurations for a factorization of potentials.
        △ Less
",probabilist argument altern causal model bayesian network probabilist argument structur pa defin famili compat frame f c f gener usual multivari model base famili variabl crucial relat condit independ frame f c f introduc shown form quasi separoid weaken well known structur separoid shown pa gener probabl potenti frame f c f oper aggreg differ pa transport pa one frame anoth induc algebra structur famili potenti f c f algebra structur similar valuat algebra relat bayesian network gener consequ well known local comput architectur bayesian network infer appli also potenti f c f condit condit defin potenti shown concept satisfi similar properti condit probabl distribut final max prod algebra potenti defin appli find probabl configur factor potenti less
199,1810.03422,"
        This paper presents a generative model for super-resolution in routine clinical magnetic resonance images (MRI), of arbitrary orientation and contrast. The model recasts the recovery of high resolution images as an inverse problem, in which a forward model simulates the slice-select profile of the MR scanner. The paper introduces a prior based on multi-channel total variation for MRI super-resolution. Bias-variance trade-off is handled by estimating hyper-parameters from the low resolution input scans. The model was validated on a large database of brain images. The validation showed that the model can improve brain segmentation, that it can recover anatomical information between images of different MR contrasts, and that it generalises well to the large variability present in MR images of different subjects.
        △ Less
",paper present gener model super resolut routin clinic magnet reson imag mri arbitrari orient contrast model recast recoveri high resolut imag invers problem forward model simul slice select profil mr scanner paper introduc prior base multi channel total variat mri super resolut bia varianc trade handl estim hyper paramet low resolut input scan model valid larg databas brain imag valid show model improv brain segment recov anatom inform imag differ mr contrast generalis well larg variabl present mr imag differ subject less
200,1810.03414,"
        Multiple modalities can provide more valuable information than single one by describing the same contents in various ways. Hence, it is highly expected to learn effective joint representation by fusing the features of different modalities. However, previous methods mainly focus on fusing the shallow features or high-level representations generated by unimodal deep networks, which only capture part of the hierarchical correlations across modalities. In this paper, we propose to densely integrate the representations by greedily stacking multiple shared layers between different modality-specific networks, which is named as Dense Multimodal Fusion (DMF). The joint representations in different shared layers can capture the correlations in different levels, and the connection between shared layers also provides an efficient way to learn the dependence among hierarchical correlations. These two properties jointly contribute to the multiple learning paths in DMF, which results in faster convergence, lower training loss, and better performance. We evaluate our model on three typical multimodal learning tasks, including audiovisual speech recognition, cross-modal retrieval, and multimodal classification. The noticeable performance in the experiments demonstrates that our model can learn more effective joint representation.
        △ Less
",multipl modal provid valuabl inform singl one describ content variou way henc highli expect learn effect joint represent fuse featur differ modal howev previou method mainli focu fuse shallow featur high level represent gener unimod deep network captur part hierarch correl across modal paper propos dens integr represent greedili stack multipl share layer differ modal specif network name dens multimod fusion dmf joint represent differ share layer captur correl differ level connect share layer also provid effici way learn depend among hierarch correl two properti jointli contribut multipl learn path dmf result faster converg lower train loss better perform evalu model three typic multimod learn task includ audiovisu speech recognit cross modal retriev multimod classif notic perform experi demonstr model learn effect joint represent less
201,1810.03410,"
        Object pose estimation is a crucial prerequisite for robots to perform autonomous manipulation in clutter. Real-world bin-picking settings such as warehouses present additional challenges, e.g., new objects are added constantly. Most of the existing object pose estimation methods assume that 3D models of the objects is available beforehand. We present a pipeline that requires minimal human intervention and circumvents the reliance on the availability of 3D models by a fast data acquisition method and a synthetic data generation procedure. This work builds on previous work on semantic segmentation of cluttered bin-picking scenes to isolate individual objects in clutter. An additional network is trained on synthetic scenes to estimate object poses from a cropped object-centered encoding extracted from the segmentation results. The proposed method is evaluated on a synthetic validation dataset and cluttered real-world scenes.
        △ Less
",object pose estim crucial prerequisit robot perform autonom manipul clutter real world bin pick set warehous present addit challeng e g new object ad constantli exist object pose estim method assum model object avail beforehand present pipelin requir minim human intervent circumv relianc avail model fast data acquisit method synthet data gener procedur work build previou work semant segment clutter bin pick scene isol individu object clutter addit network train synthet scene estim object pose crop object center encod extract segment result propos method evalu synthet valid dataset clutter real world scene less
202,1810.03402,"
        The conventional supervised hashing methods based on classification do not entirely meet the requirements of hashing technique, but Linear Discriminant Analysis (LDA) does. In this paper, we propose to perform a revised LDA objective over deep networks to learn efficient hashing codes in a truly end-to-end fashion. However, the complicated eigenvalue decomposition within each mini-batch in every epoch has to be faced with when simply optimizing the deep network w.r.t. the LDA objective. In this work, the revised LDA objective is transformed into a simple least square problem, which naturally overcomes the intractable problems and can be easily solved by the off-the-shelf optimizer. Such deep extension can also overcome the weakness of LDA Hashing in the limited linear projection and feature learning. Amounts of experiments are conducted on three benchmark datasets. The proposed Deep LDA Hashing shows nearly 70 points improvement over the conventional one on the CIFAR-10 dataset. It also beats several state-of-the-art methods on various metrics.
        △ Less
",convent supervis hash method base classif entir meet requir hash techniqu linear discrimin analysi lda paper propos perform revis lda object deep network learn effici hash code truli end end fashion howev complic eigenvalu decomposit within mini batch everi epoch face simpli optim deep network w r lda object work revis lda object transform simpl least squar problem natur overcom intract problem easili solv shelf optim deep extens also overcom weak lda hash limit linear project featur learn amount experi conduct three benchmark dataset propos deep lda hash show nearli point improv convent one cifar dataset also beat sever state art method variou metric less
203,1810.03400,"
        Tasks in outdoor open world environments are now ripe for automation with mobile manipulators. The dynamic, unstructured and unknown environments associated with such tasks -- a prime example would be collecting roadside trash -- makes them particularly challenging. In this paper we present an approach to solving the problem of picking up, transporting, and dropping off novel objects outdoors. Our solution integrates a navigation system, a grasp detection and planning system, and a custom task planner. We perform experiments that demonstrate that the system can be used to transport a wide class of novel objects (trash bags, general garbage, gardening tools and fruits) in unstructured settings outdoors with a relatively high end-to-end success rate of 85%. See it at work at: https://youtu.be/93nWXhaGEWA
        △ Less
",task outdoor open world environ ripe autom mobil manipul dynam unstructur unknown environ associ task prime exampl would collect roadsid trash make particularli challeng paper present approach solv problem pick transport drop novel object outdoor solut integr navig system grasp detect plan system custom task planner perform experi demonstr system use transport wide class novel object trash bag gener garbag garden tool fruit unstructur set outdoor rel high end end success rate see work http youtu nwxhagewa less
204,1810.03395,"
        Nielsen, Plotkin, and Winskel (1981) proved that every 1-safe Petri net $N$ unfolds into an event structure $\mathcal{E}_N$. By a result of Thiagarajan (1996 and 2002), these unfoldings are exactly the trace regular event structures. Thiagarajan (1996 and 2002) conjectured that regular event structures correspond exactly to trace regular event structures. In a recent paper (Chalopin and Chepoi, 2017, 2018), we disproved this conjecture, based on the striking bijection between domains of event structures, median graphs, and CAT(0) cube complexes. On the other hand, in Chalopin and Chepoi (2018) we proved that Thiagarajan's conjecture is true for regular event structures whose domains are principal filters of universal covers of (virtually) finite special cube complexes.
  In the current paper, we prove the converse: to any finite 1-safe Petri net $N$ one can associate a finite special cube complex ${X}_N$ such that the domain of the event structure $\mathcal{E}_N$ (obtained as the unfolding of $N$) is a principal filter of the universal cover $\widetilde{X}_N$ of $X_N$. This establishes a bijection between 1-safe Petri nets and finite special cube complexes and provides a combinatorial characterization of trace regular event structures.
  Using this bijection and techniques from graph theory and geometry (MSO theory of graphs, bounded treewidth, and bounded hyperbolicity) we disprove yet another conjecture by Thiagarajan (from the paper with S. Yang from 2014) that the monadic second order logic of a 1-safe Petri net is decidable if and only if its unfolding is grid-free.
  Our counterexample is the trace regular event structure $\mathcal{\dot E}_Z$ which arises from a virtually special square complex $\dot Z$. The domain of $\mathcal{\dot E}_Z$ is grid-free (because it is hyperbolic), but the MSO theory of the event structure $\mathcal{\dot E}_Z$ is undecidable.
        △ Less
",nielsen plotkin winskel prove everi safe petri net n unfold event structur mathcal e n result thiagarajan unfold exactli trace regular event structur thiagarajan conjectur regular event structur correspond exactli trace regular event structur recent paper chalopin chepoi disprov conjectur base strike biject domain event structur median graph cat cube complex hand chalopin chepoi prove thiagarajan conjectur true regular event structur whose domain princip filter univers cover virtual finit special cube complex current paper prove convers finit safe petri net n one associ finit special cube complex x n domain event structur mathcal e n obtain unfold n princip filter univers cover widetild x n x n establish biject safe petri net finit special cube complex provid combinatori character trace regular event structur use biject techniqu graph theori geometri mso theori graph bound treewidth bound hyperbol disprov yet anoth conjectur thiagarajan paper yang monad second order logic safe petri net decid unfold grid free counterexampl trace regular event structur mathcal dot e z aris virtual special squar complex dot z domain mathcal dot e z grid free hyperbol mso theori event structur mathcal dot e z undecid less
205,1810.03393,"
        This paper focuses on density-based clustering, particularly the Density Peak (DP) algorithm and the one based on density-connectivity DBSCAN; and proposes a new method which takes advantage of the individual strengths of these two methods to yield a density-based hierarchical clustering algorithm. Our investigation begins with formally defining the types of clusters DP and DBSCAN are designed to detect; and then identifies the kinds of distributions that DP and DBSCAN individually fail to detect all clusters in a dataset. These identified weaknesses inspire us to formally define a new kind of clusters and propose a new method called DC-HDP to overcome these weaknesses to identify clusters with arbitrary shapes and varied densities. In addition, the new method produces a richer clustering result in terms of hierarchy or dendrogram for better cluster structures understanding. Our empirical evaluation results show that DC-HDP produces the best clustering results on 14 datasets in comparison with 7 state-of-the-art clustering algorithms.
        △ Less
",paper focus densiti base cluster particularli densiti peak dp algorithm one base densiti connect dbscan propos new method take advantag individu strength two method yield densiti base hierarch cluster algorithm investig begin formal defin type cluster dp dbscan design detect identifi kind distribut dp dbscan individu fail detect cluster dataset identifi weak inspir us formal defin new kind cluster propos new method call dc hdp overcom weak identifi cluster arbitrari shape vari densiti addit new method produc richer cluster result term hierarchi dendrogram better cluster structur understand empir evalu result show dc hdp produc best cluster result dataset comparison state art cluster algorithm less
206,1810.03389,"
        Margin enlargement over training data has been an important strategy since perceptrons in machine learning for the purpose of boosting the robustness of classifiers toward a good generalization ability. Yet Breiman shows a dilemma (Breiman, 1999) that a uniform improvement on margin distribution \emph{does not} necessarily reduces generalization errors. In this paper, we revisit Breiman's dilemma in deep neural networks with recently proposed spectrally normalized margins. A novel perspective is provided to explain Breiman's dilemma based on phase transitions in dynamics of normalized margin distributions, that reflects the trade-off between expressive power of models and complexity of data. When data complexity is comparable to the model expressiveness in the sense that both training and test data share similar phase transitions in normalized margin dynamics, two efficient ways are derived to predict the trend of generalization or test error via classic margin-based generalization bounds with restricted Rademacher complexities. On the other hand, over-expressive models that exhibit uniform improvements on training margins, as a distinct phase transition to test margin dynamics, may lose such a prediction power and fail to prevent the overfitting. Experiments are conducted to show the validity of the proposed method with some basic convolutional networks, AlexNet, VGG-16, and ResNet-18, on several datasets including Cifar10/100 and mini-ImageNet.
        △ Less
",margin enlarg train data import strategi sinc perceptron machin learn purpos boost robust classifi toward good gener abil yet breiman show dilemma breiman uniform improv margin distribut emph necessarili reduc gener error paper revisit breiman dilemma deep neural network recent propos spectral normal margin novel perspect provid explain breiman dilemma base phase transit dynam normal margin distribut reflect trade express power model complex data data complex compar model express sens train test data share similar phase transit normal margin dynam two effici way deriv predict trend gener test error via classic margin base gener bound restrict rademach complex hand express model exhibit uniform improv train margin distinct phase transit test margin dynam may lose predict power fail prevent overfit experi conduct show valid propos method basic convolut network alexnet vgg resnet sever dataset includ cifar mini imagenet less
207,1810.03386,"
        We study the complexity of consistent query answering on databases that may violate primary key constraints. A repair of such a database is any consistent database that can be obtained by deleting a minimal set of tuples. For every Boolean query q, CERTAINTY(q) is the problem that takes a database as input and asks whether q evaluates to true on every repair. In [KW17], the authors show that for every self-join-free Boolean conjunctive query q, the problem CERTAINTY(q) is either in P or coNP-complete, and it is decidable which of the two cases applies. In this paper, we sharpen this result by showing that for every self-join-free Boolean conjunctive query q, the problem CERTAINTY(q) is either expressible in symmetric stratified Datalog or coNP-complete. Since symmetric stratified Datalog is in L, we thus obtain a complexity-theoretic dichotomy between L and coNP-complete. Another new finding of practical importance is that CERTAINTY(q) is on the logspace side of the dichotomy for queries q where all join conditions express foreign-to-primary key matches, which is undoubtedly the most common type of join condition.
        △ Less
",studi complex consist queri answer databas may violat primari key constraint repair databas consist databas obtain delet minim set tupl everi boolean queri q certainti q problem take databas input ask whether q evalu true everi repair kw author show everi self join free boolean conjunct queri q problem certainti q either p conp complet decid two case appli paper sharpen result show everi self join free boolean conjunct queri q problem certainti q either express symmetr stratifi datalog conp complet sinc symmetr stratifi datalog l thu obtain complex theoret dichotomi l conp complet anoth new find practic import certainti q logspac side dichotomi queri q join condit express foreign primari key match undoubtedli common type join condit less
208,1810.03382,"
        Motion analysis is used in computer vision to understand the behaviour of moving objects in sequences of images. Optimising the interpretation of dynamic biological systems requires accurate and precise motion tracking as well as efficient representations of high-dimensional motion trajectories so that these can be used for prediction tasks. Here we use image sequences of the heart, acquired using cardiac magnetic resonance imaging, to create time-resolved three-dimensional segmentations using a fully convolutional network trained on anatomical shape priors. This dense motion model formed the input to a supervised denoising autoencoder (4Dsurvival), which is a hybrid network consisting of an autoencoder that learns a task-specific latent code representation trained on observed outcome data, yielding a latent representation optimised for survival prediction. To handle right-censored survival outcomes, our network used a Cox partial likelihood loss function. In a study of 302 patients the predictive accuracy (quantified by Harrell's C-index) was significantly higher (p < .0001) for our model C=0.73 (95$\%$ CI: 0.68 - 0.78) than the human benchmark of C=0.59 (95$\%$ CI: 0.53 - 0.65). This work demonstrates how a complex computer vision task using high-dimensional medical image data can efficiently predict human survival.
        △ Less
",motion analysi use comput vision understand behaviour move object sequenc imag optimis interpret dynam biolog system requir accur precis motion track well effici represent high dimension motion trajectori use predict task use imag sequenc heart acquir use cardiac magnet reson imag creat time resolv three dimension segment use fulli convolut network train anatom shape prior dens motion model form input supervis denois autoencod dsurviv hybrid network consist autoencod learn task specif latent code represent train observ outcom data yield latent represent optimis surviv predict handl right censor surviv outcom network use cox partial likelihood loss function studi patient predict accuraci quantifi harrel c index significantli higher p model c ci human benchmark c ci work demonstr complex comput vision task use high dimension medic imag data effici predict human surviv less
209,1810.03381,"
        The Eclipse framework is a popular and widely used framework that has been evolving for over a decade. The framework provides both stable interfaces (APIs) and unstable interfaces (non-APIs). Despite being discouraged by Eclipse, client developers often use non-APIs which may cause their systems to fail when ported to new framework releases. To overcome this problem, Eclipse interface producers may promote unstable interfaces to APIs. However, client developers have no assistance to aid them to identify the promoted unstable interfaces in the Eclipse framework. We aim to help API users identify promoted unstable interfaces. We used the clone detection technique to identify promoted unstable interfaces as the framework evolves. Our empirical investigation on 16 Eclipse major releases presents the following observations. First, we have discovered that there exists over 60% non-API methods of the total interfaces in each of the analyzed 16 Eclipse releases. Second, we have discovered that the percentage of promoted non-APIs identified through clone detection ranges from 0.20% to 10.38%.
        △ Less
",eclips framework popular wide use framework evolv decad framework provid stabl interfac api unstabl interfac non api despit discourag eclips client develop often use non api may caus system fail port new framework releas overcom problem eclips interfac produc may promot unstabl interfac api howev client develop assist aid identifi promot unstabl interfac eclips framework aim help api user identifi promot unstabl interfac use clone detect techniqu identifi promot unstabl interfac framework evolv empir investig eclips major releas present follow observ first discov exist non api method total interfac analyz eclips releas second discov percentag promot non api identifi clone detect rang less
210,1810.03377,"
        As Moore's law comes to an end, neuromorphic approaches to computing are on the rise. One of these, passive photonic reservoir computing, is a strong candidate for computing at high bitrates (> 10 Gbps) and with low energy consumption. Currently though, both benefits are limited by the necessity to perform training and readout operations in the electrical domain. Thus, efforts are currently underway in the photonic community to design an integrated optical readout, which allows to perform all operations in the optical domain. In addition to the technological challenge of designing such a readout, new algorithms have to be designed in order to train it. Foremost, suitable algorithms need to be able to deal with the fact that the actual on-chip reservoir states are not directly observable. In this work, we investigate several options for such a training algorithm and propose a solution in which the complex states of the reservoir can be observed by appropriately setting the readout weights, while iterating over a predefined input sequence. We perform numerical simulations in order to compare our method with an ideal baseline requiring full observability as well as with an established black-box optimization approach (CMA-ES).
        △ Less
",moor law come end neuromorph approach comput rise one passiv photon reservoir comput strong candid comput high bitrat gbp low energi consumpt current though benefit limit necess perform train readout oper electr domain thu effort current underway photon commun design integr optic readout allow perform oper optic domain addit technolog challeng design readout new algorithm design order train foremost suitabl algorithm need abl deal fact actual chip reservoir state directli observ work investig sever option train algorithm propos solut complex state reservoir observ appropri set readout weight iter predefin input sequenc perform numer simul order compar method ideal baselin requir full observ well establish black box optim approach cma es less
211,1810.03374,"
        Motivated by the celebrated Beck-Fiala conjecture, we consider the random setting where there are $n$ elements and $m$ sets and each element lies in $t$ randomly chosen sets. In this setting, Ezra and Lovett showed an $O((t \log t)^{1/2})$ discrepancy bound in the regime when $n \leq m$ and an $O(1)$ bound when $n \gg m^t$.
  In this paper, we give a tight $O(\sqrt{t})$ bound for the entire range of $n$ and $m$, under a mild assumption that $t = Ω(\log \log m)^2$. The result is based on two steps. First, applying the partial coloring method to the case when $n = m \log^{O(1)} m$ and using the properties of the random set system we show that the overall discrepancy incurred is at most $O(\sqrt{t})$. Second, we reduce the general case to that of $n \leq m \log^{O(1)}m$ using LP duality and a careful counting argument.
        △ Less
",motiv celebr beck fiala conjectur consid random set n element set element lie randomli chosen set set ezra lovett show log discrep bound regim n leq bound n gg paper give tight sqrt bound entir rang n mild assumpt log log result base two step first appli partial color method case n log use properti random set system show overal discrep incur sqrt second reduc gener case n leq log use lp dualiti care count argument less
212,1810.03373,"
        This paper considers a single cell multi-antenna base station delivering content to multiple cache enabled single-antenna users. Coding strategies are developed that allow for decentralized placement in the wireless setting. Three different cases namely, max-min multicasting, linear combinations in the complex field, and linear combinations in the finite field, are considered and closed-form rate expressions are provided that hold with high probability. For the case of max-min fair multicasting delivery, we propose a new coding scheme that is capable of working with only two-user broadcasts. A cyclic-exchange protocol for efficient content delivery is proposed and shown to perform almost as well as the original multi-user broadcast scheme.
        △ Less
",paper consid singl cell multi antenna base station deliv content multipl cach enabl singl antenna user code strategi develop allow decentr placement wireless set three differ case name max min multicast linear combin complex field linear combin finit field consid close form rate express provid hold high probabl case max min fair multicast deliveri propos new code scheme capabl work two user broadcast cyclic exchang protocol effici content deliveri propos shown perform almost well origin multi user broadcast scheme less
213,1810.03372,"
        We propose a new notion of `non-linearity' of a network layer with respect to an input batch that is based on its proximity to a linear system, which is reflected in the non-negative rank of the activation matrix. We measure this non-linearity by applying non-negative factorization to the activation matrix. Considering batches of similar samples, we find that high non-linearity in deep layers is indicative of memorization. Furthermore, by applying our approach layer-by-layer, we find that the mechanism for memorization consists of distinct phases. We perform experiments on fully-connected and convolutional neural networks trained on several image and audio datasets. Our results demonstrate that as an indicator for memorization, our technique can be used to perform early stopping.
        △ Less
",propos new notion non linear network layer respect input batch base proxim linear system reflect non neg rank activ matrix measur non linear appli non neg factor activ matrix consid batch similar sampl find high non linear deep layer indic memor furthermor appli approach layer layer find mechan memor consist distinct phase perform experi fulli connect convolut neural network train sever imag audio dataset result demonstr indic memor techniqu use perform earli stop less
214,1810.03370,"
        One form of characterizing the expressiveness of a piecewise linear neural network is by the number of linear regions, or pieces, of the function modeled. We have observed substantial progress in this topic through lower and upper bounds on the maximum number of linear regions and a counting procedure. However, these bounds only account for the dimensions of the network and the exact counting may take a prohibitive amount of time, therefore making it infeasible to benchmark the expressiveness of networks. In this work, we approximate the number of linear regions of specific rectifier networks with an algorithm for probabilistic lower bounds of mixed-integer linear sets. In addition, we present a tighter upper bound that leverages network coefficients. We test both on trained networks. The algorithm for probabilistic lower bounds is several orders of magnitude faster than exact counting and the values reach similar orders of magnitude, hence making our approach a viable method to compare the expressiveness of such networks. The refined upper bound is particularly stronger on networks with narrow layers.
        △ Less
",one form character express piecewis linear neural network number linear region piec function model observ substanti progress topic lower upper bound maximum number linear region count procedur howev bound account dimens network exact count may take prohibit amount time therefor make infeas benchmark express network work approxim number linear region specif rectifi network algorithm probabilist lower bound mix integ linear set addit present tighter upper bound leverag network coeffici test train network algorithm probabilist lower bound sever order magnitud faster exact count valu reach similar order magnitud henc make approach viabl method compar express network refin upper bound particularli stronger network narrow layer less
215,1810.03367,"
        Programs for extracting structured information from text, namely information extractors, often operate separately on document segments obtained from a generic splitting operation such as sentences, paragraphs, k-grams, HTTP requests, and so on. An automated detection of this behavior of extractors, which we refer to as split-correctness, would allow text analysis systems to devise query plans with parallel evaluation on segments for accelerating the processing of large documents. Other applications include the incremental evaluation on dynamic content, where re-evaluation of information extractors can be restricted to revised segments, and debugging, where developers of information extractors are informed about potential boundary crossing of different semantic components.
  We propose a new formal framework for split-correctness within the formalism of document spanners. Our preliminary analysis studies the complexity of split-correctness over regular spanners. We also discuss different variants of split-correctness, for instance, in the presence of black-box extractors with so-called split constraints.
        △ Less
",program extract structur inform text name inform extractor often oper separ document segment obtain gener split oper sentenc paragraph k gram http request autom detect behavior extractor refer split correct would allow text analysi system devis queri plan parallel evalu segment acceler process larg document applic includ increment evalu dynam content evalu inform extractor restrict revis segment debug develop inform extractor inform potenti boundari cross differ semant compon propos new formal framework split correct within formal document spanner preliminari analysi studi complex split correct regular spanner also discuss differ variant split correct instanc presenc black box extractor call split constraint less
216,1810.03361,"
        In this paper, we propose a model predictive control based two-stage energy management system that aims at increasing the renewable infeed in interconnected microgrids (MGs). In particular, the proposed approach ensures that each MG in the network benefits from power exchange. In the first stage, the optimal islanded operational cost of each MG is obtained. In the second stage, the power exchange is determined such that the operational cost of each MG is below the optimal islanded cost from the first stage. In this stage, a distributed augmented Lagrangian method is used to solve the optimisation problem and determine the power flow of the network without requiring a central entity. This algorithm has faster convergence and same information exchange at each iteration as the dual decomposition algorithm. The properties of the algorithm are illustrated in a numerical case study.
        △ Less
",paper propos model predict control base two stage energi manag system aim increas renew infe interconnect microgrid mg particular propos approach ensur mg network benefit power exchang first stage optim island oper cost mg obtain second stage power exchang determin oper cost mg optim island cost first stage stage distribut augment lagrangian method use solv optimis problem determin power flow network without requir central entiti algorithm faster converg inform exchang iter dual decomposit algorithm properti algorithm illustr numer case studi less
217,1810.03360,"
        Periocular refers to the facial region in the vicinity of the eye, including eyelids, lashes and eyebrows. While face and irises have been extensively studied, the periocular region has emerged as a promising trait for unconstrained biometrics, following demands for increased robustness of face or iris systems. With a surprisingly high discrimination ability, this region can be easily obtained with existing setups for face and iris, and the requirement of user cooperation can be relaxed, thus facilitating the interaction with biometric systems. It is also available over a wide range of distances even when the iris texture cannot be reliably obtained (low resolution) or under partial face occlusion (close distances). Here, we review the state of the art in periocular biometrics research. A number of aspects are described, including: i) existing databases, ii) algorithms for periocular detection and/or segmentation, iii) features employed for recognition, iv) identification of the most discriminative regions of the periocular area, v) comparison with iris and face modalities, vi) soft-biometrics (gender/ethnicity classification), and vii) impact of gender transformation and plastic surgery on the recognition accuracy. This work is expected to provide an insight of the most relevant issues in periocular biometrics, giving a comprehensive coverage of the existing literature and current state of the art.
        △ Less
",periocular refer facial region vicin eye includ eyelid lash eyebrow face iris extens studi periocular region emerg promis trait unconstrain biometr follow demand increas robust face iri system surprisingli high discrimin abil region easili obtain exist setup face iri requir user cooper relax thu facilit interact biometr system also avail wide rang distanc even iri textur cannot reliabl obtain low resolut partial face occlus close distanc review state art periocular biometr research number aspect describ includ exist databas ii algorithm periocular detect segment iii featur employ recognit iv identif discrimin region periocular area v comparison iri face modal vi soft biometr gender ethnic classif vii impact gender transform plastic surgeri recognit accuraci work expect provid insight relev issu periocular biometr give comprehens coverag exist literatur current state art less
218,1810.03357,"
        Distributed ledger technology has gained wide popularity and adoption since the emergence of bitcoin in 2008 which is based on proof of work (PoW). It is a distributed, transparent and immutable database of records of all the transactions or events that have been shared and executed among the participants. All the transactions are verified and maintained by multiple nodes across a network without a central authority through a distributed cryptographic mechanism, a consensus protocol. It forms the core of this technology that not only validates the information appended to the ledger but also ensures the order in which it is appended across all the nodes. It is the foundation of its security, accountability and trust. While many researchers are working on improving the current protocol to be quantum resistant, fault-tolerant, and energy-efficient. Others are focused on developing different variants of the protocol, best suited for specific use cases. In this paper, we shall review different consensus protocols of distributed ledger technologies and their implementations. We shall also review their properties, concept and similar-work followed by a brief analysis.
        △ Less
",distribut ledger technolog gain wide popular adopt sinc emerg bitcoin base proof work pow distribut transpar immut databas record transact event share execut among particip transact verifi maintain multipl node across network without central author distribut cryptograph mechan consensu protocol form core technolog valid inform append ledger also ensur order append across node foundat secur account trust mani research work improv current protocol quantum resist fault toler energi effici other focus develop differ variant protocol best suit specif use case paper shall review differ consensu protocol distribut ledger technolog implement shall also review properti concept similar work follow brief analysi less
219,1810.03355,"
        Current networks more and more rely on virtualized middleboxes to flexibly provide security, protocol optimization, and policy compliance functionalities. As such, delivering these services requires that the traffic be steered through the desired sequence of virtual appliances. Current solutions introduce a new logically centralized entity, often called orchestrator, needing to build its own holistic view of the whole network so to decide where to direct the traffic. We advocate that such a centralized orchestration is not necessary and that, on the contrary, the same objectives can be achieved by augmenting the network layer routing so to include the notion of service and its chaining. In this paper, we support our claim by designing such a system. We also present an implementation and an early evaluation, showing that we can easily steer traffic through available resources. This approach also offers promising features such as incremental deployability, multi-domain service chaining, failure resiliency, and easy maintenance.
        △ Less
",current network reli virtual middlebox flexibl provid secur protocol optim polici complianc function deliv servic requir traffic steer desir sequenc virtual applianc current solut introduc new logic central entiti often call orchestr need build holist view whole network decid direct traffic advoc central orchestr necessari contrari object achiev augment network layer rout includ notion servic chain paper support claim design system also present implement earli evalu show easili steer traffic avail resourc approach also offer promis featur increment deploy multi domain servic chain failur resili easi mainten less
220,1810.03352,"
        Spontaneous spoken dialogue is often disfluent, containing pauses, hesitations, self-corrections and false starts. Processing such phenomena is essential in understanding a speaker's intended meaning and controlling the flow of the conversation. Furthermore, this processing needs to be word-by-word incremental to allow further downstream processing to begin as early as possible in order to handle real spontaneous human conversational behaviour.
  In addition, from a developer's point of view, it is highly desirable to be able to develop systems which can be trained from `clean' examples while also able to generalise to the very diverse disfluent variations on the same data -- thereby enhancing both data-efficiency and robustness. In this paper, we present a multi-task LSTM-based model for incremental detection of disfluency structure, which can be hooked up to any component for incremental interpretation (e.g. an incremental semantic parser), or else simply used to `clean up' the current utterance as it is being produced.
  We train the system on the Switchboard Dialogue Acts (SWDA) corpus and present its accuracy on this dataset. Our model outperforms prior neural network-based incremental approaches by about 10 percentage points on SWDA while employing a simpler architecture. To test the model's generalisation potential, we evaluate the same model on the bAbI+ dataset, without any additional training. bAbI+ is a dataset of synthesised goal-oriented dialogues where we control the distribution of disfluencies and their types. This shows that our approach has good generalisation potential, and sheds more light on which types of disfluency might be amenable to domain-general processing.
        △ Less
",spontan spoken dialogu often disfluent contain paus hesit self correct fals start process phenomena essenti understand speaker intend mean control flow convers furthermor process need word word increment allow downstream process begin earli possibl order handl real spontan human convers behaviour addit develop point view highli desir abl develop system train clean exampl also abl generalis divers disfluent variat data therebi enhanc data effici robust paper present multi task lstm base model increment detect disfluenc structur hook compon increment interpret e g increment semant parser els simpli use clean current utter produc train system switchboard dialogu act swda corpu present accuraci dataset model outperform prior neural network base increment approach percentag point swda employ simpler architectur test model generalis potenti evalu model babi dataset without addit train babi dataset synthesis goal orient dialogu control distribut disfluenc type show approach good generalis potenti shed light type disfluenc might amen domain gener process less
221,1810.03345,"
        A robot making contact with a human or environment, both intended and unintended, is a major design consideration for interactive robots. Substantial experimental work has investigated the role of inertia, relative velocity, and interface stiffness in collision, but no analytical expression for maximum impact force of complex models is established. Here, the Sobolev norm is used to rigorously bound maximum impact force when both the force and its derivative are in $\mathcal{L}_p$, a condition which holds for a pure stiffness coupling damped inertias under impulsive excitation. The Sobolev norm allows unified consideration of contact with humans (free space and clamped) and pure stiffness environments, accommodating complex models without using elastic/inelastic collision assumptions. The Sobolev norm can be found through the $\mathcal{H}_2$ norm of a related system, allowing efficient computation and connection with existing control theory. The Sobolev norm is validated, first experimentally with an admittance-controlled robot, then in simulation with a linear flexible-joint robot. It is then used to investigate the impact of motor dynamics, control, joint and interface compliance on collision, and a trade-off between collision performance and environmental estimation is shown.
        △ Less
",robot make contact human environ intend unintend major design consider interact robot substanti experiment work investig role inertia rel veloc interfac stiff collis analyt express maximum impact forc complex model establish sobolev norm use rigor bound maximum impact forc forc deriv mathcal l p condit hold pure stiff coupl damp inertia impuls excit sobolev norm allow unifi consider contact human free space clamp pure stiff environ accommod complex model without use elast inelast collis assumpt sobolev norm found mathcal h norm relat system allow effici comput connect exist control theori sobolev norm valid first experiment admitt control robot simul linear flexibl joint robot use investig impact motor dynam control joint interfac complianc collis trade collis perform environment estim shown less
222,1810.03340,"
        Sparse regularization is a central technique for both machine learning (to achieve supervised features selection or unsupervised mixture learning) and imaging sciences (to achieve super-resolution). Existing performance guaranties assume a separation of the spikes based on an ad-hoc (usually Euclidean) minimum distance condition, which ignores the geometry of the problem. In this article, we study the BLASSO (i.e. the off-the-grid version of $\ell^1$ LASSO regularization) and show that the Fisher-Rao distance is the natural way to ensure and quantify support recovery, since it preserves the invariance of the problem under reparameterization. We prove that under mild regularity and curvature conditions, stable support identification is achieved even in the presence of randomized sub-sampled observations (which is the case in compressed sensing or learning scenario). On deconvolution problems, which are translation invariant, this generalizes to the multi-dimensional setting existing results of the literature. For more complex translation-varying problems, such as Laplace transform inversion, this gives the first geometry-aware guarantees for sparse recovery.
        △ Less
",spars regular central techniqu machin learn achiev supervis featur select unsupervis mixtur learn imag scienc achiev super resolut exist perform guaranti assum separ spike base ad hoc usual euclidean minimum distanc condit ignor geometri problem articl studi blasso e grid version ell lasso regular show fisher rao distanc natur way ensur quantifi support recoveri sinc preserv invari problem reparameter prove mild regular curvatur condit stabl support identif achiev even presenc random sub sampl observ case compress sens learn scenario deconvolut problem translat invari gener multi dimension set exist result literatur complex translat vari problem laplac transform invers give first geometri awar guarante spars recoveri less
223,1810.03333,"
        Current advances in emerging memory technologies enable novel and unconventional computing architectures for high-performance and low-power electronic systems, capable of carrying out massively parallel operations at the edge. One emerging technology, ReRAM, also known to belong in the family of memristors (memory resistors), is gathering attention due to its attractive features for logic and in-memory computing; benefits which follow from its technological attributes, such as nanoscale dimensions, low power operation and multi-state programming. At the same time, design with CMOS is quickly reaching its physical and functional limitations, and further research towards novel logic families, such as Threshold Logic Gates (TLGs) is scoped. TLGs constitute a logic family known for its high-speed and low power consumption, yet rely on conventional transistor technology. Introducing memristors enables a more affordable reconfiguration capability of TLGs. Through this work, we are introducing a physical implementation of a memristor-based current-mode TLG (MCMTLG) circuit and validate its design and operation through multiple experimental setups. We demonstrate 2-input and 3-input MCMTLG configurations and showcase their reconfiguration capability. This is achieved by varying memristive weights arbitrarily for shaping the classification decision boundary, thus showing promise as an alternative hardware-friendly implementation of Artificial Neural Networks (ANNs). Through the employment of real memristor devices as the equivalent of synaptic weights in TLGs, we are realizing components that can be used towards an in-silico classifier.
        △ Less
",current advanc emerg memori technolog enabl novel unconvent comput architectur high perform low power electron system capabl carri massiv parallel oper edg one emerg technolog reram also known belong famili memristor memori resistor gather attent due attract featur logic memori comput benefit follow technolog attribut nanoscal dimens low power oper multi state program time design cmo quickli reach physic function limit research toward novel logic famili threshold logic gate tlg scope tlg constitut logic famili known high speed low power consumpt yet reli convent transistor technolog introduc memristor enabl afford reconfigur capabl tlg work introduc physic implement memristor base current mode tlg mcmtlg circuit valid design oper multipl experiment setup demonstr input input mcmtlg configur showcas reconfigur capabl achiev vari memrist weight arbitrarili shape classif decis boundari thu show promis altern hardwar friendli implement artifici neural network ann employ real memristor devic equival synapt weight tlg realiz compon use toward silico classifi less
224,1810.03323,"
        Face liveness detection has become a widely used technique with a growing importance in various authentication scenarios to withstand spoofing attacks. Existing methods that perform liveness detection generally focus on designing intelligent classifiers or customized hardware to differentiate between the image or video samples of a real legitimate user and the imitated ones. Although effective, they can be resource-consuming and detection results may be sensitive to environmental changes. In this paper, we take iris movement as a significant liveness sign and propose a simple and efficient liveness detection system named IriTrack. Users are required to move their eyes along with a randomly generated poly-line, and trajectories of irises are then used as evidences for liveness detection. IriTrack allows checking liveness by using data collected during user-device interactions. We implemented a prototype and conducted extensive experiments to evaluate the performance of the proposed system. The results show that IriTrack can fend against spoofing attacks with a moderate and adjustable time overhead.
        △ Less
",face live detect becom wide use techniqu grow import variou authent scenario withstand spoof attack exist method perform live detect gener focu design intellig classifi custom hardwar differenti imag video sampl real legitim user imit one although effect resourc consum detect result may sensit environment chang paper take iri movement signific live sign propos simpl effici live detect system name iritrack user requir move eye along randomli gener poli line trajectori iris use evid live detect iritrack allow check live use data collect user devic interact implement prototyp conduct extens experi evalu perform propos system result show iritrack fend spoof attack moder adjust time overhead less
225,1810.03307,"
        Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN's output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN's architecture provides a strong prior which significantly affects the representations learned at these lower layers. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we expand on findings and address concerns raised in Sundararajan et. al. (2018).
        △ Less
",explain output complic machin learn model like deep neural network dnn central challeng machin learn sever propos local explan method address issu identifi dimens singl input respons dnn output goal work assess sensit local explan dnn paramet valu somewhat surprisingli find dnn randomli initi weight produc explan visual quantit similar produc dnn learn weight conjectur phenomenon occur explan domin lower level featur dnn dnn architectur provid strong prior significantli affect represent learn lower layer note work subsum recent manuscript saniti check salienc map appear nip expand find address concern rais sundararajan et al less
226,1810.03303,"
        Robotic assistants in a home environment are expected to perform various complex tasks for their users. One particularly challenging task is pouring drinks into cups, which for successful completion, requires the detection and tracking of the liquid level during a pour to determine when to stop. In this paper, we present a novel approach to autonomous pouring that tracks the liquid level using an RGB-D camera and adapts the rate of pouring based on the liquid level feedback. We thoroughly evaluate our system on various types of liquids and under different conditions, conducting over 250 pours with a PR2 robot. The results demonstrate that our approach is able to pour liquids to a target height with an accuracy of a few millimeters.
        △ Less
",robot assist home environ expect perform variou complex task user one particularli challeng task pour drink cup success complet requir detect track liquid level pour determin stop paper present novel approach autonom pour track liquid level use rgb camera adapt rate pour base liquid level feedback thoroughli evalu system variou type liquid differ condit conduct pour pr robot result demonstr approach abl pour liquid target height accuraci millimet less
227,1810.03301,"
        This paper shows that authors have no consistent way to characterize the scalability of their solutions, and so consider only a limited number of scaling characteristics. This review aimed at establishing the evidence that the route for designing and evaluating the scalability of dynamic QoS-aware service composition mechanisms has been lacking systematic guidance, and has been informed by a very limited set of criteria. For such, we analyzed 47 papers, from 2004 to 2018.
        △ Less
",paper show author consist way character scalabl solut consid limit number scale characterist review aim establish evid rout design evalu scalabl dynam qo awar servic composit mechan lack systemat guidanc inform limit set criteria analyz paper less
228,1810.03298,"
        We study multi-stream transmission in the $K \times N \times K$ channel with interfering relay nodes, consisting of $K$ multi-antenna source--destination (S--D) pairs and $N$ single-antenna half-duplex relay nodes between the S--D pairs. We propose a new achievable scheme operating with partial effective channel gain, termed multi-stream opportunistic network decoupling (MS-OND), which achieves the optimal degrees of freedom (DoF) under a certain relay scaling law. Our protocol is built upon the conventional OND that leads to virtual full-duplex mode with one data stream transmission per S--D pair, generalizing the idea of OND to multi-stream scenarios by leveraging relay selection and interference management. Specifically, two subsets of relay nodes are opportunistically selected using alternate relaying in terms of producing or receiving the minimum total interference level. For interference management, each source node sends $S \,(1 \le S \le M)$ data streams to selected relay nodes with random beamforming for the first hop, while each destination node receives its desired $S$ streams from the selected relay nodes via opportunistic interference alignment for the second hop, where $M$ is the number of antennas at each source or destination node. Our analytical results are validated by numerical evaluation.
        △ Less
",studi multi stream transmiss k time n time k channel interf relay node consist k multi antenna sourc destin pair n singl antenna half duplex relay node pair propos new achiev scheme oper partial effect channel gain term multi stream opportunist network decoupl ms ond achiev optim degre freedom dof certain relay scale law protocol built upon convent ond lead virtual full duplex mode one data stream transmiss per pair gener idea ond multi stream scenario leverag relay select interfer manag specif two subset relay node opportunist select use altern relay term produc receiv minimum total interfer level interfer manag sourc node send le le data stream select relay node random beamform first hop destin node receiv desir stream select relay node via opportunist interfer align second hop number antenna sourc destin node analyt result valid numer evalu less
229,1810.03292,"
        Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.
        △ Less
",salienc method emerg popular tool highlight featur input deem relev predict learn model sever salienc method propos often guid visual appeal imag data work propos action methodolog evalu kind explan given method cannot provid find relianc sole visual assess mislead extens experi show exist salienc method independ model data gener process consequ method fail propos test inadequ task sensit either data model find outlier data explain relationship input output model learn debug model interpret find analog edg detect imag techniqu requir neither train data model theori case linear model singl layer convolut neural network support experiment find less
230,1810.03286,"
        We describe a novel learning-by-synthesis method for estimating gaze direction of an automated intelligent surveillance system. Recently, progress in learning-by-synthesis has proposed training models on synthetic images, which can effectively reduce the cost of manpower and material resources. However, learning from synthetic images still fails to achieve the desired performance compared to naturalistic images due to the different distribution of synthetic images. In an attempt to address this issue, previous method is to improve the realism of synthetic images by learning a model. However, the disadvantage of the method is that the distortion has not been improved and the authenticity level is unstable. To solve this problem, we put forward a new structure to improve synthetic images, via the reference to the idea of style transformation, through which we can efficiently reduce the distortion of pictures and minimize the need of real data annotation. We estimate that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on various datasets including MPIIGaze dataset.
        △ Less
",describ novel learn synthesi method estim gaze direct autom intellig surveil system recent progress learn synthesi propos train model synthet imag effect reduc cost manpow materi resourc howev learn synthet imag still fail achiev desir perform compar naturalist imag due differ distribut synthet imag attempt address issu previou method improv realism synthet imag learn model howev disadvantag method distort improv authent level unstabl solv problem put forward new structur improv synthet imag via refer idea style transform effici reduc distort pictur minim need real data annot estim enabl gener highli realist imag demonstr qualit user studi quantit evalu gener imag train model gaze estim show signific improv use synthet imag achiev state art result variou dataset includ mpiigaz dataset less
231,1810.03284,"
        With the analysis of noise-induced synchronization of opinion dynamics with bounded confidence (BC), a natural and fundamental question is what opinion structures can be synchronized by noise. In the traditional Hegselmann-Krause (HK) model, each agent examines the opinion values of all the other ones and then choose neighbors to update its own opinion according to the BC scheme. In reality, people are more likely to interchange opinions with only some individuals, resulting in a predetermined local discourse relationship as introduced by the DeGroot model. In this paper, we consider an opinion dynamics that combines the schemes of BC and local discourse topology and investigate its synchronization induced by noise. The new model endows the heterogeneous HK model with a time-varying discourse topology. With the proposed definition of noise-synchronizability, it is shown that the compound noisy model is almost surely noise-synchronizable if and only if the time-varying discourse graph is uniformly jointly connected, taking the noise-induced synchronization of the classical heterogeneous HK model as a special case. As a natural implication, the result for the first time builds the equivalence between the connectivity of discourse graph and the beneficial effect of noise for opinion consensus.
        △ Less
",analysi nois induc synchron opinion dynam bound confid bc natur fundament question opinion structur synchron nois tradit hegselmann kraus hk model agent examin opinion valu one choos neighbor updat opinion accord bc scheme realiti peopl like interchang opinion individu result predetermin local discours relationship introduc degroot model paper consid opinion dynam combin scheme bc local discours topolog investig synchron induc nois new model endow heterogen hk model time vari discours topolog propos definit nois synchroniz shown compound noisi model almost sure nois synchroniz time vari discours graph uniformli jointli connect take nois induc synchron classic heterogen hk model special case natur implic result first time build equival connect discours graph benefici effect nois opinion consensu less
232,1810.03278,"
        Azure (the cloud service provided by Microsoft) is composed of physical computing units which are called nodes. These nodes are controlled by a software component called Fabric Controller (FC), which can consider the nodes to be in one of many different states such as Ready, Unhealthy, Booting, etc. Some of these states correspond to a node being unresponsive to FCs requests. When a node goes unresponsive for more than a set threshold, FC intervenes and reboots the node. We minimized the downtime caused by the intervention threshold when a node switches to the Unhealthy state by fitting various heavy-tail probability distributions. We consider using features of the node to customize the organic recovery model to the individual nodes that go unhealthy. This regression approach allows us to use information about the node like hardware, software versions, historical performance indicators, etc. to inform the organic recovery model and hence the optimal threshold. In another direction, we consider generalizing this to an arbitrary number of thresholds within the node state machine (or Markov chain). When the states become intertwined in ways that different thresholds start affecting each other, we can't simply optimize each of them in isolation. For best results, we must consider this as an optimization problem in many variables (the number of thresholds). We no longer have a nice closed form solution for this more complex problem like we did with one threshold, but we can still use numerical techniques (gradient descent) to solve it.
        △ Less
",azur cloud servic provid microsoft compos physic comput unit call node node control softwar compon call fabric control fc consid node one mani differ state readi unhealthi boot etc state correspond node unrespons fc request node goe unrespons set threshold fc interven reboot node minim downtim caus intervent threshold node switch unhealthi state fit variou heavi tail probabl distribut consid use featur node custom organ recoveri model individu node go unhealthi regress approach allow us use inform node like hardwar softwar version histor perform indic etc inform organ recoveri model henc optim threshold anoth direct consid gener arbitrari number threshold within node state machin markov chain state becom intertwin way differ threshold start affect simpli optim isol best result must consid optim problem mani variabl number threshold longer nice close form solut complex problem like one threshold still use numer techniqu gradient descent solv less
233,1810.03274,"
        With the development of dialog techniques, conversational search has attracted more and more attention as it enables users to interact with the search engine in a natural and efficient manner. However, comparing with the natural language understanding in traditional task-oriented dialog which focuses on slot filling and tracking, the query understanding in E-commerce conversational search is quite different and more challenging due to more diverse user expressions and complex intentions. In this work, we define the real-world problem of query tracking in E-commerce conversational search, in which the goal is to update the internal query after each round of interaction. We also propose a self attention based neural network to handle the task in a machine comprehension perspective. Further more we build a novel E-commerce query tracking dataset from an operational E-commerce Search Engine, and experimental results on this dataset suggest that our proposed model outperforms several baseline methods by a substantial gain for Exact Match accuracy and F1 score, showing the potential of machine comprehension like model for this task.
        △ Less
",develop dialog techniqu convers search attract attent enabl user interact search engin natur effici manner howev compar natur languag understand tradit task orient dialog focus slot fill track queri understand e commerc convers search quit differ challeng due divers user express complex intent work defin real world problem queri track e commerc convers search goal updat intern queri round interact also propos self attent base neural network handl task machin comprehens perspect build novel e commerc queri track dataset oper e commerc search engin experiment result dataset suggest propos model outperform sever baselin method substanti gain exact match accuraci f score show potenti machin comprehens like model task less
234,1810.03272,"
        We consider an important task of effective and efficient semantic image segmentation. In particular, we adapt a powerful semantic segmentation architecture, called RefineNet, into the more compact one, suitable even for tasks requiring real-time performance on high-resolution inputs. To this end, we identify computationally expensive blocks in the original setup, and propose two modifications aimed to decrease the number of parameters and floating point operations. By doing that, we achieve more than twofold model reduction, while keeping the performance levels almost intact. Our fastest model undergoes a significant speed-up boost from 20 FPS to 55 FPS on a generic GPU card on 512x512 inputs with solid 81.1% mean iou performance on the test set of PASCAL VOC, while our slowest model with 32 FPS (from original 17 FPS) shows 82.7% mean iou on the same dataset. Alternatively, we showcase that our approach is easily mixable with light-weight classification networks: we attain 79.2% mean iou on PASCAL VOC using a model that contains only 3.3M parameters and performs only 9.3B floating point operations.
        △ Less
",consid import task effect effici semant imag segment particular adapt power semant segment architectur call refinenet compact one suitabl even task requir real time perform high resolut input end identifi comput expens block origin setup propos two modif aim decreas number paramet float point oper achiev twofold model reduct keep perform level almost intact fastest model undergo signific speed boost fp fp gener gpu card x input solid mean iou perform test set pascal voc slowest model fp origin fp show mean iou dataset altern showcas approach easili mixabl light weight classif network attain mean iou pascal voc use model contain paramet perform b float point oper less
235,1810.03270,"
        As experts continue to debate the optimal surgery practice for coronary disease - percutaneous coronary intervention (PCI) or coronary aortic bypass graft (CABG) - computational tools may provide a quantitative assessment of each option. Computational fluid dynamics (CFD) has been used to assess the interplay between hemodynamics and stent struts; it is of particular interest in Bioresorbable Vascular Stents (BVS), since their thicker struts may result in impacted flow patterns and possible pathological consequences. Many proofs of concept are presented in the literature; however, a practical method for extracting patient-specific stented coronary artery geometries from images over a large number of patients remains an open problem.
  This work provides a possible pipeline for the reconstruction of the BVS. Using Optical Coherence Tomographies (OCT) and Invasive Coronary Angiographies (ICA), we can reconstruct the 3D geometry of deployed BVS in vivo. We illustrate the stent reconstruction process: (i) automatic strut detection, (ii) identification of stent components, (iii) 3D registration of stent curvature, and (iv) final stent volume reconstruction. The methodology is designed for use on clinical OCT images, as opposed to approaches that relied on a small number of virtually deployed stents.
  The proposed reconstruction process is validated with a virtual phantom stent, providing quantitative assessment of the methodology, and with selected clinical cases, confirming feasibility. Using multimodality image analysis, we obtain reliable reconstructions within a reasonable timeframe. This work is the first step toward a fully automated reconstruction and simulation procedure aiming at an extensive quantitative analysis of the impact of BVS struts on hemodynamics via CFD in clinical trials, going beyond the proof-of-concept stage.
        △ Less
",expert continu debat optim surgeri practic coronari diseas percutan coronari intervent pci coronari aortic bypass graft cabg comput tool may provid quantit assess option comput fluid dynam cfd use assess interplay hemodynam stent strut particular interest bioresorb vascular stent bv sinc thicker strut may result impact flow pattern possibl patholog consequ mani proof concept present literatur howev practic method extract patient specif stent coronari arteri geometri imag larg number patient remain open problem work provid possibl pipelin reconstruct bv use optic coher tomographi oct invas coronari angiographi ica reconstruct geometri deploy bv vivo illustr stent reconstruct process automat strut detect ii identif stent compon iii registr stent curvatur iv final stent volum reconstruct methodolog design use clinic oct imag oppos approach reli small number virtual deploy stent propos reconstruct process valid virtual phantom stent provid quantit assess methodolog select clinic case confirm feasibl use multimod imag analysi obtain reliabl reconstruct within reason timefram work first step toward fulli autom reconstruct simul procedur aim extens quantit analysi impact bv strut hemodynam via cfd clinic trial go beyond proof concept stage less
236,1810.03264,"
        Many distributed machine learning (ML) systems adopt the non-synchronous execution in order to alleviate the network communication bottleneck, resulting in stale parameters that do not reflect the latest updates. Despite much development in large-scale ML, the effects of staleness on learning are inconclusive as it is challenging to directly monitor or control staleness in complex distributed environments. In this work, we study the convergence behaviors of a wide array of ML models and algorithms under delayed updates. Our extensive experiments reveal the rich diversity of the effects of staleness on the convergence of ML algorithms and offer insights into seemingly contradictory reports in the literature. The empirical findings also inspire a new convergence analysis of stochastic gradient descent in non-convex optimization under staleness, matching the best-known convergence rate of O(1/\sqrt{T}).
        △ Less
",mani distribut machin learn ml system adopt non synchron execut order allevi network commun bottleneck result stale paramet reflect latest updat despit much develop larg scale ml effect stale learn inconclus challeng directli monitor control stale complex distribut environ work studi converg behavior wide array ml model algorithm delay updat extens experi reveal rich divers effect stale converg ml algorithm offer insight seemingli contradictori report literatur empir find also inspir new converg analysi stochast gradient descent non convex optim stale match best known converg rate sqrt less
237,1810.03259,"
        We present and investigate a novel and timely application domain for deep reinforcement learning (RL): Internet congestion control. Congestion control is the core networking task of modulating traffic sources' data-transmission rates so as to efficiently and fairly allocate network resources. Congestion control is fundamental to computer networking research and practice, and has recently been the subject of extensive attention in light of the advent of challenging Internet applications such as live video, augmented and virtual reality, Internet-of-Things, and more.
  We build on the recently introduced Performance-oriented Congestion Control (PCC) framework to formulate congestion control protocol design as an RL task. Our RL framework opens up opportunities for network practitioners, and even application developers, to train congestion control models that fit their local performance objectives based on small, bootstrapped models, or complex, custom models, as their resources and requirements merit. We present and discuss the challenges that must be overcome so as to realize our long-term vision for Internet congestion control.
        △ Less
",present investig novel time applic domain deep reinforc learn rl internet congest control congest control core network task modul traffic sourc data transmiss rate effici fairli alloc network resourc congest control fundament comput network research practic recent subject extens attent light advent challeng internet applic live video augment virtual realiti internet thing build recent introduc perform orient congest control pcc framework formul congest control protocol design rl task rl framework open opportun network practition even applic develop train congest control model fit local perform object base small bootstrap model complex custom model resourc requir merit present discuss challeng must overcom realiz long term vision internet congest control less
238,1810.03254,"
        Single image super resolution is of great importance as a low-level computer vision task. Recent approaches with deep convolutional neural networks have achieved im-pressive performance. However, existing architectures have limitations due to the less sophisticated structure along with less strong representational power. In this work, to significantly enhance the feature representation, we proposed Triple Attention mixed link Network (TAN) which consists of 1) three different aspects (i.e., kernel, spatial and channel) of attention mechanisms and 2) fu-sion of both powerful residual and dense connections (i.e., mixed link). Specifically, the network with multi kernel learns multi hierarchical representations under different receptive fields. The output features are recalibrated by the effective kernel and channel attentions and feed into next layer partly residual and partly dense, which filters the information and enable the network to learn more powerful representations. The features finally pass through the spatial attention in the reconstruction network which generates a fusion of local and global information, let the network restore more details and improves the quality of reconstructed images. Thanks to the diverse feature recalibrations and the advanced information flow topology, our proposed model is strong enough to per-form against the state-of-the-art methods on the bench-mark evaluations.
        △ Less
",singl imag super resolut great import low level comput vision task recent approach deep convolut neural network achiev im pressiv perform howev exist architectur limit due less sophist structur along less strong represent power work significantli enhanc featur represent propos tripl attent mix link network tan consist three differ aspect e kernel spatial channel attent mechan fu sion power residu dens connect e mix link specif network multi kernel learn multi hierarch represent differ recept field output featur recalibr effect kernel channel attent feed next layer partli residu partli dens filter inform enabl network learn power represent featur final pass spatial attent reconstruct network gener fusion local global inform let network restor detail improv qualiti reconstruct imag thank divers featur recalibr advanc inform flow topolog propos model strong enough per form state art method bench mark evalu less
239,1810.03249,"
        Fully homomorphic encryption has allowed devices to outsource computation to third parties while preserving the secrecy of the data being computed on. Many images contain sensitive information and are commonly sent to cloud services to encode images for different devices. We implement image processing homomorphically that ensures secrecy of the image while also providing reasonable overhead. We first present some previous related work, as well as the fully homomorphic encryption scheme we use. Then, we introduce our schemes for JPEG encoding and decoding, as well as schemes for bilinear and bicubic image resizing, as well as some data and analysis of our homomorphic schemes. Finally, we outline several issues with the homomorphic evaluation of proprietary algorithms, and how a third party can gain information on the algorithm through noise.
        △ Less
",fulli homomorph encrypt allow devic outsourc comput third parti preserv secreci data comput mani imag contain sensit inform commonli sent cloud servic encod imag differ devic implement imag process homomorph ensur secreci imag also provid reason overhead first present previou relat work well fulli homomorph encrypt scheme use introduc scheme jpeg encod decod well scheme bilinear bicub imag resiz well data analysi homomorph scheme final outlin sever issu homomorph evalu proprietari algorithm third parti gain inform algorithm nois less
240,1810.03243,"
        Over the years many ellipse detection algorithms spring up and are studied broadly, while the critical issue of detecting ellipses accurately and efficiently in real-world images remains a challenge. In this paper, an accurate and efficient ellipse detector by arc-support line segments is proposed. The arc-support line segment simplifies the complicated expression of curves in an image while retains the general properties including convexity and polarity, which grounds the successful detection of ellipses. The arc-support groups are formed by iteratively and robustly linking the arc-support line segments that latently belong to a common ellipse at point statistics level. Afterward, two complementary approaches, namely, selecting the group with higher saliency to fit an ellipse, and searching all the valid paired arc-support groups, are utilized to generate the initial ellipse set, both locally and globally. In ellipse fitting step, a superposition principle for the fast ellipse fitting is developed to accelerate the process. Then, the ellipse candidates can be formulated by the hierarchical clustering of 5D parameter space of initial ellipse set. Finally, the salient ellipse candidates are selected as detections subject to the stringent and effective verification. Extensive experiments on three public datasets are implemented and our method achieves the best F-measure scores compared to the state-of-the-art methods.
        △ Less
",year mani ellips detect algorithm spring studi broadli critic issu detect ellips accur effici real world imag remain challeng paper accur effici ellips detector arc support line segment propos arc support line segment simplifi complic express curv imag retain gener properti includ convex polar ground success detect ellips arc support group form iter robustli link arc support line segment latent belong common ellips point statist level afterward two complementari approach name select group higher salienc fit ellips search valid pair arc support group util gener initi ellips set local global ellips fit step superposit principl fast ellips fit develop acceler process ellips candid formul hierarch cluster paramet space initi ellips set final salient ellips candid select detect subject stringent effect verif extens experi three public dataset implement method achiev best f measur score compar state art method less
241,1810.03241,"
        Convolutional Neural Networks (CNNs) are a class of artificial neural networks whose computational blocks use convolution, together with other linear and non-linear operations, to perform classification or regression. This paper explores the spectral response of CNNs and its potential use in diagnosing problems with their training. We measure the gain of CNNs trained for image classification on ImageNet and observe that the best models are also the most sensitive to perturbations of their input. Further, we perform experiments on MNIST and CIFAR-10 to find that the gain rises as the network learns and then saturates as the network converges. Moreover, we find that strong gain fluctuations can point to overfitting and learning problems caused by a poor choice of learning rate. We argue that the gain of CNNs can act as a diagnostic tool and potential replacement for the validation loss when hold-out validation data are not available.
        △ Less
",convolut neural network cnn class artifici neural network whose comput block use convolut togeth linear non linear oper perform classif regress paper explor spectral respons cnn potenti use diagnos problem train measur gain cnn train imag classif imagenet observ best model also sensit perturb input perform experi mnist cifar find gain rise network learn satur network converg moreov find strong gain fluctuat point overfit learn problem caus poor choic learn rate argu gain cnn act diagnost tool potenti replac valid loss hold valid data avail less
242,1810.03238,"
        We present an architectural design and a reference implementation for horizontal scaling of virtual network function chains. Our solution does not require any changes to network functions and is able to handle stateful network functions for which states may depend on both directions of the traffic. We use connection-aware traffic load balancers based on hashing function to maintain mappings between connections and the dynamically changing network function chains. Our references implementation uses OpenFlow switches to route traffic to the assigned network function instances according to the load balancer decisions. We conducted extensive simulations to test the feasibility of the architecture and evaluate the performance of our implementation.
        △ Less
",present architectur design refer implement horizont scale virtual network function chain solut requir chang network function abl handl state network function state may depend direct traffic use connect awar traffic load balanc base hash function maintain map connect dynam chang network function chain refer implement use openflow switch rout traffic assign network function instanc accord load balanc decis conduct extens simul test feasibl architectur evalu perform implement less
243,1810.03237,"
        Much like humans, robots should have the ability to leverage knowledge from previously learned tasks in order to learn new tasks quickly in new and unfamiliar environments. Despite this, most robot learning approaches have focused on learning a single task, from scratch, with a limited notion of generalisation, and no way of leveraging the knowledge to learn other tasks more efficiently. One possible solution is meta-learning, but many of the related approaches are limited in their ability to scale to a large number of tasks and to learn further tasks without forgetting previously learned ones. With this in mind, we introduce Task-Embedded Control Networks, which employ ideas from metric learning in order to create a task embedding that can be used by a robot to learn new tasks from one or more demonstrations. In the area of visually-guided manipulation, we present simulation results in which we surpass the performance of a state-of-the-art method when using only visual information from each demonstration. Additionally, we demonstrate that our approach can also be used in conjunction with domain randomisation to train our few-shot learning ability in simulation and then deploy in the real world without any additional training. Once deployed, the robot can learn new tasks from a single real-world demonstration.
        △ Less
",much like human robot abil leverag knowledg previous learn task order learn new task quickli new unfamiliar environ despit robot learn approach focus learn singl task scratch limit notion generalis way leverag knowledg learn task effici one possibl solut meta learn mani relat approach limit abil scale larg number task learn task without forget previous learn one mind introduc task embed control network employ idea metric learn order creat task embed use robot learn new task one demonstr area visual guid manipul present simul result surpass perform state art method use visual inform demonstr addit demonstr approach also use conjunct domain randomis train shot learn abil simul deploy real world without addit train deploy robot learn new task singl real world demonstr less
244,1810.03235,"
        Entity-Relationship (E-R) Search is a complex case of Entity Search where the goal is to search for multiple unknown entities and relationships connecting them. We assume that a E-R query can be decomposed as a sequence of sub-queries each containing keywords related to a specific entity or relationship. We adopt a probabilistic formulation of the E-R search problem. When creating specific representations for entities (e.g. context terms) and for pairs of entities (i.e. relationships) it is possible to create a graph of probabilistic dependencies between sub-queries and entity plus relationship representations. To the best of our knowledge this represents the first probabilistic model of E-R search. We propose and develop a novel supervised Early Fusion-based model for E-R search, the Entity-Relationship Dependence Model (ERDM). It uses Markov Random Field to model term dependencies of E-R sub-queries and entity/relationship documents. We performed experiments with more than 800M entities and relationships extractions from ClueWeb-09-B with FACC1 entity linking. We obtained promising results using 3 different query collections comprising 469 E-R queries, with results showing that it is possible to perform E-R search without using fix and pre-defined entity and relationship types, enabling a wide range of queries to be addressed.
        △ Less
",entiti relationship e r search complex case entiti search goal search multipl unknown entiti relationship connect assum e r queri decompos sequenc sub queri contain keyword relat specif entiti relationship adopt probabilist formul e r search problem creat specif represent entiti e g context term pair entiti e relationship possibl creat graph probabilist depend sub queri entiti plu relationship represent best knowledg repres first probabilist model e r search propos develop novel supervis earli fusion base model e r search entiti relationship depend model erdm use markov random field model term depend e r sub queri entiti relationship document perform experi entiti relationship extract clueweb b facc entiti link obtain promis result use differ queri collect compris e r queri result show possibl perform e r search without use fix pre defin entiti relationship type enabl wide rang queri address less
245,1810.03234,"
        Convolutional neural networks (CNN's) are powerful and widely used tools. However, their interpretability is far from ideal. In this paper we use topological data analysis to investigate what various CNN's learn. We show that the weights of convolutional layers at depths from 1 through 13 learn simple global structures. We also demonstrate the change of the simple structures over the course of training. In particular, we define and analyze the spaces of spatial filters of convolutional layers and show the recurrence, among all networks, depths, and during training, of a simple circle consisting of rotating edges, as well as a less recurring unanticipated complex circle that combines lines, edges, and non-linear patterns. We train over a thousand CNN's on MNIST and CIFAR-10, as well as use VGG-networks pretrained on ImageNet.
        △ Less
",convolut neural network cnn power wide use tool howev interpret far ideal paper use topolog data analysi investig variou cnn learn show weight convolut layer depth learn simpl global structur also demonstr chang simpl structur cours train particular defin analyz space spatial filter convolut layer show recurr among network depth train simpl circl consist rotat edg well less recur unanticip complex circl combin line edg non linear pattern train thousand cnn mnist cifar well use vgg network pretrain imagenet less
246,1810.03226,"
        We present a model for capturing musical features and creating novel sequences of music, called the Convolutional Variational Recurrent Neural Network. To generate sequential data, the model uses an encoder-decoder architecture with latent probabilistic connections to capture the hidden structure of music. Using the sequence-to-sequence model, our generative model can exploit samples from a prior distribution and generate a longer sequence of music. We compare the performance of our proposed model with other types of Neural Networks using the criteria of Information Rate that is implemented by Variable Markov Oracle, a method that allows statistical characterization of musical information dynamics and detection of motifs in a song. Our results suggest that the proposed model has a better statistical resemblance to the musical structure of the training data, which improves the creation of new sequences of music in the style of the originals.
        △ Less
",present model captur music featur creat novel sequenc music call convolut variat recurr neural network gener sequenti data model use encod decod architectur latent probabilist connect captur hidden structur music use sequenc sequenc model gener model exploit sampl prior distribut gener longer sequenc music compar perform propos model type neural network use criteria inform rate implement variabl markov oracl method allow statist character music inform dynam detect motif song result suggest propos model better statist resembl music structur train data improv creation new sequenc music style origin less
247,1810.03224,"
        We introduce a new approach to spectral sparsification that approximates the quadratic form of the pseudoinverse of a graph Laplacian restricted to a subspace. We show that sparsifiers with a near-linear number of edges in the dimension of the subspace exist. Our setting generalizes that of Schur complement sparsifiers. Our approach produces sparsifiers by sampling a uniformly random spanning tree of the input graph and using that tree to guide an edge elimination procedure that contracts, deletes, and reweights edges. In the context of Schur complement sparsifiers, our approach has two benefits over prior work. First, it produces a sparsifier in almost-linear time with no runtime dependence on the desired error. We directly exploit this to compute approximate effective resistances for a small set of vertex pairs in faster time than prior work (Durfee-Kyng-Peebles-Rao-Sachdeva '17). Secondly, it yields sparsifiers that are reweighted minors of the input graph. As a result, we give a near-optimal answer to a variant of the Steiner point removal problem.
  A key ingredient of our algorithm is a subroutine of independent interest: a near-linear time algorithm that, given a chosen set of vertices, builds a data structure from which we can query a multiplicative approximation to the decrease in the effective resistance between two vertices after identifying all vertices in the chosen set to a single vertex with inverse polynomial additional additive error in near-constant time.
        △ Less
",introduc new approach spectral sparsif approxim quadrat form pseudoinvers graph laplacian restrict subspac show sparsifi near linear number edg dimens subspac exist set gener schur complement sparsifi approach produc sparsifi sampl uniformli random span tree input graph use tree guid edg elimin procedur contract delet reweight edg context schur complement sparsifi approach two benefit prior work first produc sparsifi almost linear time runtim depend desir error directli exploit comput approxim effect resist small set vertex pair faster time prior work durfe kyng peebl rao sachdeva secondli yield sparsifi reweight minor input graph result give near optim answer variant steiner point remov problem key ingredi algorithm subroutin independ interest near linear time algorithm given chosen set vertic build data structur queri multipl approxim decreas effect resist two vertic identifi vertic chosen set singl vertex invers polynomi addit addit error near constant time less
248,1810.03218,"
        Deep Learning has received significant attention due to its impressive performance in many state-of-the-art learning tasks. Unfortunately, while very powerful, Deep Learning is not well understood theoretically and in particular only recently results for the complexity of training deep neural networks have been obtained. In this work we show that large classes of deep neural networks with various architectures (e.g., DNNs, CNNs, Binary Neural Networks, and ResNets), activation functions (e.g., ReLUs and leaky ReLUs), and loss functions (e.g., Hinge loss, Euclidean loss, etc) can be trained to near optimality with desired target accuracy using linear programming in time that is exponential in the size of the architecture and polynomial in the size of the data set; this is the best one can hope for due to the NP-Hardness of the problem and in line with previous work. In particular, we obtain polynomial time algorithms for training for a given fixed network architecture. Our work applies more broadly to empirical risk minimization problems which allows us to generalize various previous results and obtain new complexity results for previously unstudied architectures in the proper learning setting.
        △ Less
",deep learn receiv signific attent due impress perform mani state art learn task unfortun power deep learn well understood theoret particular recent result complex train deep neural network obtain work show larg class deep neural network variou architectur e g dnn cnn binari neural network resnet activ function e g relu leaki relu loss function e g hing loss euclidean loss etc train near optim desir target accuraci use linear program time exponenti size architectur polynomi size data set best one hope due np hard problem line previou work particular obtain polynomi time algorithm train given fix network architectur work appli broadli empir risk minim problem allow us gener variou previou result obtain new complex result previous unstudi architectur proper learn set less
249,1810.03213,"
        This project performed image completion on CIFAR-10, a dataset of 60,000 32x32 RGB images, using three different neural network architectures: fully convolutional networks, convolutional networks with fully connected layers, and encoder-decoder convolutional networks. The highest performing model was a deep fully convolutional network, which was able to achieve a mean squared error of .015 when comparing the original image pixel values with the predicted pixel values. As well, this network was able to output in-painted images which appeared real to the human eye.
        △ Less
",project perform imag complet cifar dataset x rgb imag use three differ neural network architectur fulli convolut network convolut network fulli connect layer encod decod convolut network highest perform model deep fulli convolut network abl achiev mean squar error compar origin imag pixel valu predict pixel valu well network abl output paint imag appear real human eye less
250,1810.03201,"
        A large body of applications that involve monitoring, decision making, and forecasting require timely status updates for their efficient operation. Age of Information (AoI) is a newly proposed metric that effectively captures this requirement. Recent research on the subject has derived AoI optimal policies for the generation of status updates and AoI optimal packet queueing disciplines. Unlike previous research we focus on low-end devices that typically support monitoring applications in the context of the Internet of Things. We acknowledge that these devices host a diverse set of applications some of which are AoI sensitive while others are not. Furthermore, due to their limited computational resources they typically utilize a simple First-In First-Out (FIFO) queueing discipline. We consider the problem of optimally controlling the status update generation process for a system with a source-destination pair that communicates via a wireless link, whereby the source node is comprised of a FIFO queue and two applications, one that is AoI sensitive and one that is not. We formulate this problem as a dynamic programming problem and utilize the framework of Markov Decision Processes to derive optimal policies for the generation of status update packets. Due to the lack of comparable methods in the literature, we compare the derived optimal policies against baseline policies, such as the zero-wait policy, and investigate the performance of all policies for a variety of network configurations. Results indicate that existing status update policies fail to capture the trade-off between frequent generation of status updates and queueing delay and thus perform poorly.
        △ Less
",larg bodi applic involv monitor decis make forecast requir time statu updat effici oper age inform aoi newli propos metric effect captur requir recent research subject deriv aoi optim polici gener statu updat aoi optim packet queue disciplin unlik previou research focu low end devic typic support monitor applic context internet thing acknowledg devic host divers set applic aoi sensit other furthermor due limit comput resourc typic util simpl first first fifo queue disciplin consid problem optim control statu updat gener process system sourc destin pair commun via wireless link wherebi sourc node compris fifo queue two applic one aoi sensit one formul problem dynam program problem util framework markov decis process deriv optim polici gener statu updat packet due lack compar method literatur compar deriv optim polici baselin polici zero wait polici investig perform polici varieti network configur result indic exist statu updat polici fail captur trade frequent gener statu updat queue delay thu perform poorli less
251,1810.03200,"
        This paper presents a novel tightly-coupled keyframe based Simultaneous Localization and Mapping (SLAM) system with loop-closing and relocalization capabilities targeted to the underwater domain. The state-of-the-art visual-inertial state estimation package OKVIS has been significantly augmented to accommodate acoustic data from sonar and depth measurements from pressure sensor, along with visual and inertial data in a non-linear optimization-based framework. The main contributions of this paper are: a robust initialization method to refine scale using depth measurements and a real-time loop-closing and relocalization method. An additional contribution is the tightly-coupled optimization formulation using acoustic, visual, inertial, and depth data. Experimental results on datasets collected with a custom-made underwater sensor suite and an autonomous underwater vehicle from challenging underwater environments with poor visibility demonstrate the performance of our approach.
        △ Less
",paper present novel tightli coupl keyfram base simultan local map slam system loop close reloc capabl target underwat domain state art visual inerti state estim packag okvi significantli augment accommod acoust data sonar depth measur pressur sensor along visual inerti data non linear optim base framework main contribut paper robust initi method refin scale use depth measur real time loop close reloc method addit contribut tightli coupl optim formul use acoust visual inerti depth data experiment result dataset collect custom made underwat sensor suit autonom underwat vehicl challeng underwat environ poor visibl demonstr perform approach less
252,1810.03199,"
        A central question in neuroscience is how to develop realistic models that predict output firing behavior based on provided external stimulus. Given a set of external inputs and a set of output spike trains, the objective is to discover a network structure which can accomplish the transformation as accurately as possible. Due to the difficulty of this problem in its most general form, approximations have been made in previous work. Past approximations have sacrificed network size, recurrence, allowed spiked count, or have imposed layered network structure. Here we present a learning rule without these sacrifices, which produces a weight matrix of a leaky integrate-and-fire (LIF) network to match the output activity of both deterministic LIF networks as well as probabilistic integrate-and-fire (PIF) networks. Inspired by synaptic scaling, our pre-synaptic pool modification (PSPM) algorithm outputs deterministic, fully recurrent spiking neural networks that can provide a novel generative model for given spike trains. Similarity in output spike trains is evaluated with a variety of metrics including a van-Rossum like measure and a numerical comparison of inter-spike interval distributions. Application of our algorithm to randomly generated networks improves similarity to the reference spike trains on both of these stated measures. In addition, we generated LIF networks that operate near criticality when trained on critical PIF outputs. Our results establish that learning rules based on synaptic homeostasis can be used to represent input-output relationships in fully recurrent spiking neural networks.
        △ Less
",central question neurosci develop realist model predict output fire behavior base provid extern stimulu given set extern input set output spike train object discov network structur accomplish transform accur possibl due difficulti problem gener form approxim made previou work past approxim sacrif network size recurr allow spike count impos layer network structur present learn rule without sacrific produc weight matrix leaki integr fire lif network match output activ determinist lif network well probabilist integr fire pif network inspir synapt scale pre synapt pool modif pspm algorithm output determinist fulli recurr spike neural network provid novel gener model given spike train similar output spike train evalu varieti metric includ van rossum like measur numer comparison inter spike interv distribut applic algorithm randomli gener network improv similar refer spike train state measur addit gener lif network oper near critic train critic pif output result establish learn rule base synapt homeostasi use repres input output relationship fulli recurr spike neural network less
253,1810.03198,"
        In statistical modelling the biggest threat is concept drift which makes the model gradually showing deteriorating performance over time. There are state of the art methodologies to detect the impact of concept drift, however general strategy considered to overcome the issue in performance is to rebuild or re-calibrate the model periodically as the variable patterns for the model changes significantly due to market change or consumer behavior change etc. Quantitative research is the most widely spread application of data science in Marketing or financial domain where applicability of state of the art reinforcement learning for auto-learning is less explored paradigm. Reinforcement learning is heavily dependent on having a simulated environment which is majorly available for gaming or online systems, to learn from the live feedback. However, there are some research happened on the area of online advertisement, pricing etc where due to the nature of the online learning environment scope of reinforcement learning is explored. Our proposed solution is a reinforcement learning based, true self-learning algorithm which can adapt to the data change or concept drift and auto learn and self-calibrate for the new patterns of the data solving the problem of concept drift.
  Keywords - Reinforcement learning, Genetic Algorithm, Q-learning, Classification modelling, CMA-ES, NES, Multi objective optimization, Concept drift, Population stability index, Incremental learning, F1-measure, Predictive Modelling, Self-learning, MCTS, AlphaGo, AlphaZero
        △ Less
",statist model biggest threat concept drift make model gradual show deterior perform time state art methodolog detect impact concept drift howev gener strategi consid overcom issu perform rebuild calibr model period variabl pattern model chang significantli due market chang consum behavior chang etc quantit research wide spread applic data scienc market financi domain applic state art reinforc learn auto learn less explor paradigm reinforc learn heavili depend simul environ majorli avail game onlin system learn live feedback howev research happen area onlin advertis price etc due natur onlin learn environ scope reinforc learn explor propos solut reinforc learn base true self learn algorithm adapt data chang concept drift auto learn self calibr new pattern data solv problem concept drift keyword reinforc learn genet algorithm q learn classif model cma es ne multi object optim concept drift popul stabil index increment learn f measur predict model self learn mct alphago alphazero less
254,1810.03197,"
        Alternating direction method of multiplier (ADMM) is a powerful method to solve decentralized convex optimization problems. In distributed settings, each node performs computation with its local data and the local results are exchanged among neighboring nodes in an iterative fashion. During this iterative process the leakage of data privacy arises and can accumulate significantly over many iterations, making it difficult to balance the privacy-utility tradeoff. In this study we propose Recycled ADMM (R-ADMM), where a linear approximation is applied to every even iteration, its solution directly calculated using only results from the previous, odd iteration. It turns out that under such a scheme, half of the updates incur no privacy loss and require much less computation compared to the conventional ADMM. We obtain a sufficient condition for the convergence of R-ADMM and provide the privacy analysis based on objective perturbation.
        △ Less
",altern direct method multipli admm power method solv decentr convex optim problem distribut set node perform comput local data local result exchang among neighbor node iter fashion iter process leakag data privaci aris accumul significantli mani iter make difficult balanc privaci util tradeoff studi propos recycl admm r admm linear approxim appli everi even iter solut directli calcul use result previou odd iter turn scheme half updat incur privaci loss requir much less comput compar convent admm obtain suffici condit converg r admm provid privaci analysi base object perturb less
255,1810.03196,"
        In this paper, we introduce the Hummingbird: the first completely open-source tail-sitter micro aerial vehicle (MAV) platform. The vehicle has a highly versatile, dual-rotor design and is engineered to be low-cost and easily extensible/modifiable. Our open-source release includes all of the design documents, software resources, and simulation tools needed to build and fly a high-performance tail-sitter for research and educational purposes. To the best of our knowledge, this is the first time that a comprehensive set of (free and open) resources for tail-sitters has been made available to the aerial robotics community.
  The Hummingbird has been developed for precision flight with a high degree of control authority. Our design methodology included extensive testing and characterization of the aerodynamic properties of the vehicle. The platform incorporates many off-the-shelf components and 3D-printed parts, in order to keep the cost down. Nonetheless, the paper includes results from flight trials which demonstrate that the vehicle is capable of very stable hovering and accurate trajectory tracking.
  Our hope is that the open-source Hummingbird reference design will be useful to both researchers and educators. In particular, the details in this paper and the available open-source materials should enable learners to gain an understanding of aerodynamics, flight control, state estimation, software design, and simulation, while experimenting with a unique aerial robot.
        △ Less
",paper introduc hummingbird first complet open sourc tail sitter micro aerial vehicl mav platform vehicl highli versatil dual rotor design engin low cost easili extens modifi open sourc releas includ design document softwar resourc simul tool need build fli high perform tail sitter research educ purpos best knowledg first time comprehens set free open resourc tail sitter made avail aerial robot commun hummingbird develop precis flight high degre control author design methodolog includ extens test character aerodynam properti vehicl platform incorpor mani shelf compon print part order keep cost nonetheless paper includ result flight trial demonstr vehicl capabl stabl hover accur trajectori track hope open sourc hummingbird refer design use research educ particular detail paper avail open sourc materi enabl learner gain understand aerodynam flight control state estim softwar design simul experi uniqu aerial robot less
256,1810.03184,"
        Transliteration converts words in a source language (e.g., English) into words in a target language (e.g., Vietnamese). This conversion considers the phonological structure of the target language, as the transliterated output needs to be pronounceable in the target language. For example, a word in Vietnamese that begins with a consonant cluster is phonologically invalid and thus would be an incorrect output of a transliteration system. Most statistical transliteration approaches, albeit being widely adopted, do not explicitly model the target language's phonology, which often results in invalid outputs. The problem is compounded by the limited linguistic resources available when converting foreign words to transliterated words in the target language. In this work, we present a phonology-augmented statistical framework suitable for transliteration, especially when only limited linguistic resources are available. We propose the concept of pseudo-syllables as structures representing how segments of a foreign word are organized according to the syllables of the target language's phonology. We performed transliteration experiments on Vietnamese and Cantonese. We show that the proposed framework outperforms the statistical baseline by up to 44.68% relative, when there are limited training examples (587 entries).
        △ Less
",transliter convert word sourc languag e g english word target languag e g vietnames convers consid phonolog structur target languag transliter output need pronounc target languag exampl word vietnames begin conson cluster phonolog invalid thu would incorrect output transliter system statist transliter approach albeit wide adopt explicitli model target languag phonolog often result invalid output problem compound limit linguist resourc avail convert foreign word transliter word target languag work present phonolog augment statist framework suitabl transliter especi limit linguist resourc avail propos concept pseudo syllabl structur repres segment foreign word organ accord syllabl target languag phonolog perform transliter experi vietnames cantones show propos framework outperform statist baselin rel limit train exampl entri less
257,1810.03173,"
        Infrared small target detection in an infrared search and track (IRST) system is a challenging task. This situation becomes more complicated when high gray-intensity structural backgrounds appear in the field of view (FoV) of the infrared seeker. While the majority of the infrared small target detection algorithms neglect directional information, in this paper, a directional approach is presented to suppress structural backgrounds and develop more effective detection algorithm. To this end, a similar concept to the average absolute gray difference (AAGD) is utilized to construct a directional small target detection algorithm called absolute directional mean difference (ADMD). Also, an efficient implementation procedure is presented for the proposed algorithm. The proposed algorithm effectively enhances the target area and eliminates background clutter. Simulation results on real infrared images prove the significant effectiveness of the proposed algorithm.
        △ Less
",infrar small target detect infrar search track irst system challeng task situat becom complic high gray intens structur background appear field view fov infrar seeker major infrar small target detect algorithm neglect direct inform paper direct approach present suppress structur background develop effect detect algorithm end similar concept averag absolut gray differ aagd util construct direct small target detect algorithm call absolut direct mean differ admd also effici implement procedur present propos algorithm propos algorithm effect enhanc target area elimin background clutter simul result real infrar imag prove signific effect propos algorithm less
258,1810.03167,"
        Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.
        △ Less
",previou tradit approach unsupervis chines word segment cw roughli classifi discrimin gener model former use care design good measur candid segment latter focus find optim segment highest gener probabl howev exist trivial way extend discrimin model neural version use neural languag model gener one non trivial paper propos segment languag model slm cw approach explicitli focus segment natur chines well preserv sever properti languag model slm context encod encod previou context segment decod gener segment increment far know first propos neural model unsupervis cw achiev competit perform state art statist model four differ dataset sighan bakeoff less
259,1810.03163,"
        Accurately and efficiently crowdsourcing complex, open-ended tasks can be difficult, as crowd participants tend to favor short, repetitive ""microtasks"". We study the crowdsourcing of large networks where the crowd provides the network topology via microtasks. Crowds can explore many types of social and information networks, but we focus on the network of causal attributions, an important network that signifies cause-and-effect relationships. We conduct experiments on Amazon Mechanical Turk (AMT) testing how workers propose and validate individual causal relationships and introduce a method for independent crowd workers to explore large networks. The core of the method, Iterative Pathway Refinement, is a theoretically-principled mechanism for efficient exploration via microtasks. We evaluate the method using synthetic networks and apply it on AMT to extract a large-scale causal attribution network, then investigate the structure of this network as well as the activity patterns and efficiency of the workers who constructed this network. Worker interactions reveal important characteristics of causal perception and the network data they generate can improve our understanding of causality and causal inference.
        △ Less
",accur effici crowdsourc complex open end task difficult crowd particip tend favor short repetit microtask studi crowdsourc larg network crowd provid network topolog via microtask crowd explor mani type social inform network focu network causal attribut import network signifi caus effect relationship conduct experi amazon mechan turk amt test worker propos valid individu causal relationship introduc method independ crowd worker explor larg network core method iter pathway refin theoret principl mechan effici explor via microtask evalu method use synthet network appli amt extract larg scale causal attribut network investig structur network well activ pattern effici worker construct network worker interact reveal import characterist causal percept network data gener improv understand causal causal infer less
260,1810.03162,"
        In the conventional cloud service model, computing resources are allocated for tenants on a pay-per-use basis. However, the performance of applications that communicate inside this network is unpredictable because network resources are not guaranteed. To mitigate this issue, the virtual cluster (VC) model has been developed in which network and compute units are guaranteed. Thereon, many algorithms have been developed that are based on novel extensions of the VC model in order to solve the online virtual cluster embedding problem (VCE) with additional parameters. In the online VCE, the resource footprint is greedily minimized per request which is connected with maximizing the profit for the provider per request. However, this does not imply that a global maximization of the profit over the whole sequence of requests is guaranteed. In fact, these algorithms do not even provide a worst case guarantee on a fraction of the maximum achievable profit of a certain sequence of requests. Thus, these online algorithms do not provide a competitive ratio on the profit.
  In this thesis, two competitive online VCE algorithms and two heuristic algorithms are presented. The competitive online VCE algorithms have different competitive ratios on the objective function and the capacity constraints whereas the heuristic algorithms do not violate the capacity constraints. The worst case competitive ratios are analyzed. After that, the evaluation shows the advantages and disadvantages of these algorithms in several scenarios with different request patterns and profit metrics on the fat-tree and MDCube datacenter topologies. The results show that for different scenarios, different algorithms have the best performance with respect to certain metrics.
        △ Less
",convent cloud servic model comput resourc alloc tenant pay per use basi howev perform applic commun insid network unpredict network resourc guarante mitig issu virtual cluster vc model develop network comput unit guarante thereon mani algorithm develop base novel extens vc model order solv onlin virtual cluster embed problem vce addit paramet onlin vce resourc footprint greedili minim per request connect maxim profit provid per request howev impli global maxim profit whole sequenc request guarante fact algorithm even provid worst case guarante fraction maximum achiev profit certain sequenc request thu onlin algorithm provid competit ratio profit thesi two competit onlin vce algorithm two heurist algorithm present competit onlin vce algorithm differ competit ratio object function capac constraint wherea heurist algorithm violat capac constraint worst case competit ratio analyz evalu show advantag disadvantag algorithm sever scenario differ request pattern profit metric fat tree mdcube datacent topolog result show differ scenario differ algorithm best perform respect certain metric less
261,1810.03159,"
        Consider the following problem: A multi-antenna base station (BS) sends multiple symbol streams to multiple single-antenna users via precoding. However, unlike conventional multiuser precoding, the transmitted signals are subjected to binary, unit-modulus, or even discrete unit-modulus constraints. Such constraints arise in the one-bit and constant-envelope (CE) massive MIMO scenarios, wherein high-resolution digital-to-analog converters (DACs) are replaced by one-bit DACs and phase shifters, respectively, for cutting down hardware cost and energy consumption. Multiuser precoding under one-bit and CE restrictions poses significant design difficulty. In this paper we establish a framework for designing multiuser precoding under the one-bit, continuous CE and discrete CE scenarios---all within one theme. We first formulate a precoding design that focuses on minimizations of symbol-error probabilities (SEPs), assuming quadrature amplitude modulation (QAM) constellations. We then devise an algorithm for our SEP-based design. The algorithm combines i) a novel penalty method for handling binary, unit-modulus and discrete unit-modulus constraints; and ii) a first-order non-convex optimization recipe custom-built for the design. Specifically, the latter is an inexact majorization-minimization method via accelerated projected gradient, which, as shown by simulations, runs very fast and can handle a large number of decision variables. Simulation results indicate that the proposed design offers significantly better bit-error rate performance than the existing designs.
        △ Less
",consid follow problem multi antenna base station bs send multipl symbol stream multipl singl antenna user via precod howev unlik convent multius precod transmit signal subject binari unit modulu even discret unit modulu constraint constraint aris one bit constant envelop ce massiv mimo scenario wherein high resolut digit analog convert dac replac one bit dac phase shifter respect cut hardwar cost energi consumpt multius precod one bit ce restrict pose signific design difficulti paper establish framework design multius precod one bit continu ce discret ce scenario within one theme first formul precod design focus minim symbol error probabl sep assum quadratur amplitud modul qam constel devis algorithm sep base design algorithm combin novel penalti method handl binari unit modulu discret unit modulu constraint ii first order non convex optim recip custom built design specif latter inexact major minim method via acceler project gradient shown simul run fast handl larg number decis variabl simul result indic propos design offer significantli better bit error rate perform exist design less
262,1810.03155,"
        Deep convolutional neural networks (DCNN) have recently shown promising results in low-level computer vision problems such as optical flow and disparity estimation, but still, have much room to further improve their performance. In this paper, we propose a novel sub-pixel convolution-based encoder-decoder network for optical flow and disparity estimations, which can extend FlowNetS and DispNet by replacing the deconvolution layers with sup-pixel convolution blocks. By using sub-pixel refinement and estimation on the decoder stages instead of deconvolution, we can significantly improve the estimation accuracy for optical flow and disparity, even with reduced numbers of parameters. We show a supervised end-to-end training of our proposed networks for optical flow and disparity estimations, and an unsupervised end-to-end training for monocular depth and pose estimations. In order to verify the effectiveness of our proposed networks, we perform intensive experiments for (i) optical flow and disparity estimations, and (ii) monocular depth and pose estimations. Throughout the extensive experiments, our proposed networks outperform the baselines such as FlowNetS and DispNet in terms of estimation accuracy and training times.
        △ Less
",deep convolut neural network dcnn recent shown promis result low level comput vision problem optic flow dispar estim still much room improv perform paper propos novel sub pixel convolut base encod decod network optic flow dispar estim extend flownet dispnet replac deconvolut layer sup pixel convolut block use sub pixel refin estim decod stage instead deconvolut significantli improv estim accuraci optic flow dispar even reduc number paramet show supervis end end train propos network optic flow dispar estim unsupervis end end train monocular depth pose estim order verifi effect propos network perform intens experi optic flow dispar estim ii monocular depth pose estim throughout extens experi propos network outperform baselin flownet dispnet term estim accuraci train time less
263,1810.03151,"
        Minesweeper as a puzzle video game and is proved that it is an NPC problem. We use CSP, Logic Inference and Sampling to make a minesweeper solver and we limit us each select in 5 seconds.
        △ Less
",minesweep puzzl video game prove npc problem use csp logic infer sampl make minesweep solver limit us select second less
264,1810.03148,"
        In an attempt to improve overall translation quality, there has been an increasing focus on integrating more linguistic elements into Machine Translation (MT). While significant progress has been achieved, especially recently with neural models, automatically evaluating the output of such systems is still an open problem. Current practice in MT evaluation relies on a single reference translation, even though there are many ways of translating a particular text, and it tends to disregard higher level information such as discourse. We propose a novel approach that assesses the translated output based on the source text rather than the reference translation, and measures the extent to which the semantics of the discourse elements (discourse relations, in particular) in the source text are preserved in the MT output. The challenge is to detect the discourse relations in the source text and determine whether these relations are correctly transferred crosslingually to the target language -- without a reference translation. This methodology could be used independently for discourse-level evaluation, or as a component in other metrics, at a time where substantial amounts of MT are online and would benefit from evaluation where the source text serves as a benchmark.
        △ Less
",attempt improv overal translat qualiti increas focu integr linguist element machin translat mt signific progress achiev especi recent neural model automat evalu output system still open problem current practic mt evalu reli singl refer translat even though mani way translat particular text tend disregard higher level inform discours propos novel approach assess translat output base sourc text rather refer translat measur extent semant discours element discours relat particular sourc text preserv mt output challeng detect discours relat sourc text determin whether relat correctli transfer crosslingu target languag without refer translat methodolog could use independ discours level evalu compon metric time substanti amount mt onlin would benefit evalu sourc text serv benchmark less
265,1810.03145,"
        Classifying human cognitive states from behavioral and physiological signals is a challenging problem with important applications in robotics. The problem is challenging due to the data variability among individual users, and sensor artefacts. In this work, we propose an end-to-end framework for real-time cognitive workload classification with mixture Hyper Long Short Term Memory Networks, a novel variant of HyperNetworks. Evaluating the proposed approach on an eye-gaze pattern dataset collected from simulated driving scenarios of different cognitive demands, we show that the proposed framework outperforms previous baseline methods and achieves 83.9\% precision and 87.8\% recall during test. We also demonstrate the merit of our proposed architecture by showing improved performance over other LSTM-based methods.
        △ Less
",classifi human cognit state behavior physiolog signal challeng problem import applic robot problem challeng due data variabl among individu user sensor artefact work propos end end framework real time cognit workload classif mixtur hyper long short term memori network novel variant hypernetwork evalu propos approach eye gaze pattern dataset collect simul drive scenario differ cognit demand show propos framework outperform previou baselin method achiev precis recal test also demonstr merit propos architectur show improv perform lstm base method less
266,1810.03143,"
        Coronary artery centerline extraction in cardiac CT angiography (CCTA) images is a prerequisite for evaluation of stenoses and atherosclerotic plaque. We propose an algorithm that extracts coronary artery centerlines in CCTA using a convolutional neural network (CNN).
  A 3D dilated CNN is trained to predict the most likely direction and radius of an artery at any given point in a CCTA image based on a local image patch. Starting from a single seed point placed manually or automatically anywhere in a coronary artery, a tracker follows the vessel centerline in two directions using the predictions of the CNN. Tracking is terminated when no direction can be identified with high certainty.
  The CNN was trained using 32 manually annotated centerlines in a training set consisting of 8 CCTA images provided in the MICCAI 2008 Coronary Artery Tracking Challenge (CAT08). Evaluation using 24 test images of the CAT08 challenge showed that extracted centerlines had an average overlap of 93.7% with 96 manually annotated reference centerlines. Extracted centerline points were highly accurate, with an average distance of 0.21 mm to reference centerline points. In a second test set consisting of 50 CCTA scans, 5,448 markers in the coronary arteries were used as seed points to extract single centerlines. This showed strong correspondence between extracted centerlines and manually placed markers. In a third test set containing 36 CCTA scans, fully automatic seeding and centerline extraction led to extraction of on average 92% of clinically relevant coronary artery segments.
  The proposed method is able to accurately and efficiently determine the direction and radius of coronary arteries. The method can be trained with limited training data, and once trained allows fast automatic or interactive extraction of coronary artery trees from CCTA images.
        △ Less
",coronari arteri centerlin extract cardiac ct angiographi ccta imag prerequisit evalu stenos atherosclerot plaqu propos algorithm extract coronari arteri centerlin ccta use convolut neural network cnn dilat cnn train predict like direct radiu arteri given point ccta imag base local imag patch start singl seed point place manual automat anywher coronari arteri tracker follow vessel centerlin two direct use predict cnn track termin direct identifi high certainti cnn train use manual annot centerlin train set consist ccta imag provid miccai coronari arteri track challeng cat evalu use test imag cat challeng show extract centerlin averag overlap manual annot refer centerlin extract centerlin point highli accur averag distanc mm refer centerlin point second test set consist ccta scan marker coronari arteri use seed point extract singl centerlin show strong correspond extract centerlin manual place marker third test set contain ccta scan fulli automat seed centerlin extract led extract averag clinic relev coronari arteri segment propos method abl accur effici determin direct radiu coronari arteri method train limit train data train allow fast automat interact extract coronari arteri tree ccta imag less
267,1810.03124,"
        (Mini-batch) Stochastic Gradient Descent is a popular optimization method which has been applied to many machine learning applications. But a rather high variance introduced by the stochastic gradient in each step may slow down the convergence. In this paper, we propose the antithetic sampling strategy to reduce the variance by taking advantage of the internal structure in dataset. Under this new strategy, stochastic gradients in a mini-batch are no longer independent but negatively correlated as much as possible, while the mini-batch stochastic gradient is still an unbiased estimator of full gradient. For the binary classification problems, we just need to calculate the antithetic samples in advance, and reuse the result in each iteration, which is practical. Experiments are provided to confirm the effectiveness of the proposed method.
        △ Less
",mini batch stochast gradient descent popular optim method appli mani machin learn applic rather high varianc introduc stochast gradient step may slow converg paper propos antithet sampl strategi reduc varianc take advantag intern structur dataset new strategi stochast gradient mini batch longer independ neg correl much possibl mini batch stochast gradient still unbias estim full gradient binari classif problem need calcul antithet sampl advanc reus result iter practic experi provid confirm effect propos method less
268,1810.03120,"
        Two anonymous mobile agents navigate synchronously in an anonymous graph and have to meet at a node, using a deterministic algorithm. This is a symmetry breaking task called rendezvous, equivalent to the fundamental task of leader election between the agents. When is this feasible in a completely anonymous environment? It is known that agents can always meet if their initial positions are nonsymmetric, and that if they are symmetric and agents start simultaneously then rendezvous is impossible. What happens for symmetric initial positions with non-simultaneous start? Can symmetry between the agents be broken by the delay between their starting times?
  In order to answer these questions, we consider {\em space-time initial configurations} (abbreviated by STIC). A STIC is formalized as $[(u,v),δ]$, where $u$ and $v$ are initial nodes of the agents in some graph and $δ$ is a non-negative integer that represents the difference between their starting times. A STIC is {\em feasible} if there exists a deterministic algorithm, even dedicated to this particular STIC, which accomplishes rendezvous for it. Our main result is a characterization of all feasible STICs and the design of a universal deterministic algorithm that accomplishes rendezvous for all of them without {\em any } a priori knowledge of the agents. Thus, as far as feasibility is concerned, we completely solve the problem of symmetry breaking between two anonymous agents in anonymous graphs. Moreover, we show that such a universal algorithm cannot work for all feasible STICs in time polynomial in the initial distance between the agents.
        △ Less
",two anonym mobil agent navig synchron anonym graph meet node use determinist algorithm symmetri break task call rendezv equival fundament task leader elect agent feasibl complet anonym environ known agent alway meet initi posit nonsymmetr symmetr agent start simultan rendezv imposs happen symmetr initi posit non simultan start symmetri agent broken delay start time order answer question consid em space time initi configur abbrevi stic stic formal u v u v initi node agent graph non neg integ repres differ start time stic em feasibl exist determinist algorithm even dedic particular stic accomplish rendezv main result character feasibl stic design univers determinist algorithm accomplish rendezv without em priori knowledg agent thu far feasibl concern complet solv problem symmetri break two anonym agent anonym graph moreov show univers algorithm cannot work feasibl stic time polynomi initi distanc agent less
269,1810.03116,"
        Existing works have explored the anchor deployment for autonomous underwater vehicles (AUVs) localization under the assumption that the sound propagates straightly underwater at a constant speed. Considering that the underwater acoustic waves propagate along bent curves at varying speeds in practice, it becomes much more challenging to determine a proper anchor deployment configuration. In this paper, taking the practical variability of underwater sound speed into account, we investigate the anchor-AUV geometry problem in a 3-D time-of-flight (ToF) based underwater scenario from the perspective of localization accuracy. To address this problem, we first rigorously derive the Jacobian matrix of measurement errors to quantify the Cramer-Rao lower bound (CRLB) with a widely-adopted isogradient sound speed profile (SSP). We then formulate an optimization problem that minimizes the trace of the CRLB subject to the angle and range constraints to figure out the anchor-AUV geometry, which is multivariate and nonlinear and thus generally hard to handle. For mathematical tractability, by adopting tools from the estimation theory, we interestingly find that this problem can be equivalently transformed into a more explicit univariate optimization problem. By this, we obtain an easy-to-implement anchor-AUV geometry that yields satisfactory localization performance, referred to as the uniform sea-surface circumference (USC) deployment. Extensive simulation results validate our theoretical analysis and show that our proposed USC scheme outperforms both the cube and the random deployment schemes in terms of localization accuracy under the same parameter settings.
        △ Less
",exist work explor anchor deploy autonom underwat vehicl auv local assumpt sound propag straightli underwat constant speed consid underwat acoust wave propag along bent curv vari speed practic becom much challeng determin proper anchor deploy configur paper take practic variabl underwat sound speed account investig anchor auv geometri problem time flight tof base underwat scenario perspect local accuraci address problem first rigor deriv jacobian matrix measur error quantifi cramer rao lower bound crlb wide adopt isogradi sound speed profil ssp formul optim problem minim trace crlb subject angl rang constraint figur anchor auv geometri multivari nonlinear thu gener hard handl mathemat tractabl adopt tool estim theori interestingli find problem equival transform explicit univari optim problem obtain easi implement anchor auv geometri yield satisfactori local perform refer uniform sea surfac circumfer usc deploy extens simul result valid theoret analysi show propos usc scheme outperform cube random deploy scheme term local accuraci paramet set less
270,1810.03115,"
        This paper presents thirteen datasets for binary, multiclass and multilabel classification based on the European Court of Human Rights judgments since its creation. The interest of such datasets is explained through the prism of the researcher, the data scientist, the citizen and the legal practitioner. Contrarily to many datasets, the creation process, from the collection of raw data to the feature transformation, is provided under the form of a collection of fully automated and open-source scripts. It ensures reproducibility and a high level of confidence in the processed data which is some of the most important issues in data governance nowadays.
        △ Less
",paper present thirteen dataset binari multiclass multilabel classif base european court human right judgment sinc creation interest dataset explain prism research data scientist citizen legal practition contrarili mani dataset creation process collect raw data featur transform provid form collect fulli autom open sourc script ensur reproduc high level confid process data import issu data govern nowaday less
271,1810.03113,"
        In this paper, we analyze the inverse dynamics and control of a bacteria-inspired uniflagellar robot in a fluid medium at low Reynolds number. Inspired by the mechanism behind the locomotion of flagellated bacteria, we consider a robot comprised of a flagellum -- a flexible helical filament -- attached to a spherical head. The flagellum rotates about the head at a controlled angular velocity and generates a propulsive force that moves the robot forward. When the angular velocity exceeds a threshold value, the hydrodynamic force exerted by the fluid can cause the soft flagellum to buckle, characterized by a dramatic change in shape. In this computational study, a fluid-structure interaction model that combines Discrete Elastic Rods (DER) algorithm with Lighthill's Slender Body Theory (LSBT) is employed to simulate the locomotion and deformation of the robot. We demonstrate that the robot can follow a prescribed path in three dimensional space by exploiting buckling of the flagellum. The control scheme involves only a single (binary) scalar input -- the angular velocity of the flagellum. By triggering the buckling instability at the right moment, the robot can follow an arbitrary path in three dimensional space. We also show that the complexity of the dynamics of the helical filament can be captured using a deep neural network, from which we identify the input-output functional relationship between the control inputs and the trajectory of the robot. Furthermore, our study underscores the potential role of buckling in the locomotion of natural bacteria.
        △ Less
",paper analyz invers dynam control bacteria inspir uniflagellar robot fluid medium low reynold number inspir mechan behind locomot flagel bacteria consid robot compris flagellum flexibl helic filament attach spheric head flagellum rotat head control angular veloc gener propuls forc move robot forward angular veloc exce threshold valu hydrodynam forc exert fluid caus soft flagellum buckl character dramat chang shape comput studi fluid structur interact model combin discret elast rod der algorithm lighthil slender bodi theori lsbt employ simul locomot deform robot demonstr robot follow prescrib path three dimension space exploit buckl flagellum control scheme involv singl binari scalar input angular veloc flagellum trigger buckl instabl right moment robot follow arbitrari path three dimension space also show complex dynam helic filament captur use deep neural network identifi input output function relationship control input trajectori robot furthermor studi underscor potenti role buckl locomot natur bacteria less
272,1810.03105,"
        This paper proposes an accelerated proximal stochastic variance reduced gradient (ASVRG) method, in which we design a simple and effective momentum acceleration trick. Unlike most existing accelerated stochastic variance reduction methods such as Katyusha, ASVRG has only one additional variable and one momentum parameter. Thus, ASVRG is much simpler than those methods, and has much lower per-iteration complexity. We prove that ASVRG achieves the best known oracle complexities for both strongly convex and non-strongly convex objectives. In addition, we extend ASVRG to mini-batch and non-smooth settings. We also empirically verify our theoretical results and show that the performance of ASVRG is comparable with, and sometimes even better than that of the state-of-the-art stochastic methods.
        △ Less
",paper propos acceler proxim stochast varianc reduc gradient asvrg method design simpl effect momentum acceler trick unlik exist acceler stochast varianc reduct method katyusha asvrg one addit variabl one momentum paramet thu asvrg much simpler method much lower per iter complex prove asvrg achiev best known oracl complex strongli convex non strongli convex object addit extend asvrg mini batch non smooth set also empir verifi theoret result show perform asvrg compar sometim even better state art stochast method less
273,1810.03102,"
        One of the important factors that make a search engine fast and accurate is a concise and duplicate free index. In order to remove duplicate and near-duplicate documents from the index, a search engine needs a swift and reliable duplicate and near-duplicate text document detection system. Traditional approaches to this problem, such as brute force comparisons or simple hash-based algorithms are not suitable as they are not scalable and are not capable of detecting near-duplicate documents effectively. In this paper, a new signature-based approach to text similarity detection is introduced which is fast, scalable, reliable and needs less storage space. The proposed method is examined on popular text document data-sets such as CiteseerX, Enron, Gold Set of Near-duplicate News Articles and etc. The results are promising and comparable with the best cutting-edge algorithms, considering the accuracy and performance. The proposed method is based on the idea of using reference texts to generate signatures for text documents. The novelty of this paper is the use of genetic algorithms to generate better reference texts.
        △ Less
",one import factor make search engin fast accur concis duplic free index order remov duplic near duplic document index search engin need swift reliabl duplic near duplic text document detect system tradit approach problem brute forc comparison simpl hash base algorithm suitabl scalabl capabl detect near duplic document effect paper new signatur base approach text similar detect introduc fast scalabl reliabl need less storag space propos method examin popular text document data set citeseerx enron gold set near duplic news articl etc result promis compar best cut edg algorithm consid accuraci perform propos method base idea use refer text gener signatur text document novelti paper use genet algorithm gener better refer text less
274,1810.03099,"
        The importance of an efficient and scalable document similarity detection system is undeniable nowadays. Search engines need batch text similarity measures to detect duplicated and near-duplicated web pages in their indexes in order to prevent indexing a web page multiple times. Furthermore, in the scoring phase, search engines need similarity measures to detect duplicated contents on web pages so as to increase the quality of their results. In this paper, a new approach to batch text similarity detection is proposed by combining some ideas from dimensionality reduction techniques and information gain theory. The new approach is focused on search engines need to detect duplicated and near-duplicated web pages. The new approach is evaluated on the NEWS20 dataset and the results show that the new approach is faster than the cosine text similarity algorithm in terms of speed and performance. On top of that, It is faster and more accurate than the other rival method, Simhash similarity algorithm.
        △ Less
",import effici scalabl document similar detect system undeni nowaday search engin need batch text similar measur detect duplic near duplic web page index order prevent index web page multipl time furthermor score phase search engin need similar measur detect duplic content web page increas qualiti result paper new approach batch text similar detect propos combin idea dimension reduct techniqu inform gain theori new approach focus search engin need detect duplic near duplic web page new approach evalu news dataset result show new approach faster cosin text similar algorithm term speed perform top faster accur rival method simhash similar algorithm less
275,1810.03087,"
        In the counting Graph Homomorphism problem (#GraphHom) the question is: Given graphs G,H, find the number of homomorphisms from G to H. This problem is generally #P-complete, moreover, Cygan et al. proved that unless the ETH is false there is no algorithm that solves this problem in time O(|V(H)|^{o(|V(G)|)}. This, however, does not rule out the possibility that faster algorithms exist for restricted problems of this kind. Wahlstrom proved that #GraphHom can be solved in plain exponential time, that is, in time k^{|V(G)|+V(H)|}\poly(|V(H)|,|V(G)|) provided H has clique width k. We generalize this result to a larger class of graphs, and also identify several other graph classes that admit a plain exponential algorithm for #GraphHom.
        △ Less
",count graph homomorph problem graphhom question given graph g h find number homomorph g h problem gener p complet moreov cygan et al prove unless eth fals algorithm solv problem time v h v g howev rule possibl faster algorithm exist restrict problem kind wahlstrom prove graphhom solv plain exponenti time time k v g v h poli v h v g provid h cliqu width k gener result larger class graph also identifi sever graph class admit plain exponenti algorithm graphhom less
276,1810.03084,"
        Due to the increased rate of information in the present era, local identification of similar and related data points by using neighborhood construction algorithms is highly significant for processing information in various sciences. Geometric methods are especially useful for their accuracy in locating highly similar neighborhood points using efficient geometric structures. Geometric methods should be examined for each individual point in neighborhood data set so that similar groups would be formed. Those algorithms are not highly accurate for high dimension of data. Due to the important challenges in data point analysis, we have used geometric method in which the Apollonius circle is used to achieve high local accuracy with high dimension data. In this paper, we propose a neighborhood construction algorithm, namely Neighborhood Construction by Apollonius Region Density (NCARD). In this study, the neighbors of data points are determined using not only the geometric structures, but also the density information. Apollonius circle, one of the state-of-the-art proximity geometry methods, Apollonius circle, is used for this purpose. For efficient clustering, our algorithm works better with high dimension of data than the previous methods; it is also able to identify the local outlier data. We have no prior information about the data in the proposed algorithm. Moreover, after locating similar data points with Apollonius circle, we will extract density and relationship among the points, and a unique and accurate neighborhood is created in this way. The proposed algorithm is more accurate than the state-of-the-art and well-known algorithms up to almost 8-13% in real and artificial data sets.
        △ Less
",due increas rate inform present era local identif similar relat data point use neighborhood construct algorithm highli signific process inform variou scienc geometr method especi use accuraci locat highli similar neighborhood point use effici geometr structur geometr method examin individu point neighborhood data set similar group would form algorithm highli accur high dimens data due import challeng data point analysi use geometr method apolloniu circl use achiev high local accuraci high dimens data paper propos neighborhood construct algorithm name neighborhood construct apolloniu region densiti ncard studi neighbor data point determin use geometr structur also densiti inform apolloniu circl one state art proxim geometri method apolloniu circl use purpos effici cluster algorithm work better high dimens data previou method also abl identifi local outlier data prior inform data propos algorithm moreov locat similar data point apolloniu circl extract densiti relationship among point uniqu accur neighborhood creat way propos algorithm accur state art well known algorithm almost real artifici data set less
277,1810.03083,"
        Finding neighbourhood structures is very useful in extracting valuable relationships among data samples. This paper presents a survey of recent neighbourhood construction algorithms for pattern clustering and classifying data points. Extracting neighbourhoods and connections among the points is extremely useful for clustering and classifying the data. Many applications such as detecting social network communities, bundling related edges, and solving location and routing problems all indicate the usefulness of this problem. Finding data point neighbourhood in data mining and pattern recognition should generally improve knowledge extraction from databases. Several algorithms of data point neighbourhood construction have been proposed to analyse the data in this sense. They will be described and discussed from different aspects in this paper. Finally, the future challenges concerning the title of the present paper will be outlined.
        △ Less
",find neighbourhood structur use extract valuabl relationship among data sampl paper present survey recent neighbourhood construct algorithm pattern cluster classifi data point extract neighbourhood connect among point extrem use cluster classifi data mani applic detect social network commun bundl relat edg solv locat rout problem indic use problem find data point neighbourhood data mine pattern recognit gener improv knowledg extract databas sever algorithm data point neighbourhood construct propos analys data sens describ discuss differ aspect paper final futur challeng concern titl present paper outlin less
278,1810.03078,"
        Graphlets are defined as k-node connected induced subgraph patterns. For an undirected graph, 3-node graphlets include close triangle and open triangle. When k = 4, there are six types of graphlets, e.g., tailed-triangle and clique are two possible 4-node graphlets. The number of each graphlet, called graphlet count, is a signature which characterizes the local network structure of a given graph. Graphlet count plays a prominent role in network analysis of many fields, most notably bioinformatics and social science.
  However, computing exact graphlet count is inherently difficult and computational expensive because the number of graphlets grows exponentially large as the graph size and/or graphlet size k grow. To deal with this difficulty, many sampling methods were proposed to estimate graphlet count with bounded error. Nevertheless, these methods require large number of samples to be statistically reliable, which is still computationally demanding. Moreover, they have to repeat laborious counting procedure even if a new graph is similar or exactly the same as previous studied graphs.
  Intuitively, learning from historic graphs can make estimation more accurate and avoid many repetitive counting to reduce computational cost. Based on this idea, we propose a convolutional neural network (CNN) framework and two preprocessing techniques to estimate graphlet count. Extensive experiments on two types of random graphs and real world biochemistry graphs show that our framework can offer substantial speedup on estimating graphlet count of new graphs with high accuracy.
        △ Less
",graphlet defin k node connect induc subgraph pattern undirect graph node graphlet includ close triangl open triangl k six type graphlet e g tail triangl cliqu two possibl node graphlet number graphlet call graphlet count signatur character local network structur given graph graphlet count play promin role network analysi mani field notabl bioinformat social scienc howev comput exact graphlet count inher difficult comput expens number graphlet grow exponenti larg graph size graphlet size k grow deal difficulti mani sampl method propos estim graphlet count bound error nevertheless method requir larg number sampl statist reliabl still comput demand moreov repeat labori count procedur even new graph similar exactli previou studi graph intuit learn histor graph make estim accur avoid mani repetit count reduc comput cost base idea propos convolut neural network cnn framework two preprocess techniqu estim graphlet count extens experi two type random graph real world biochemistri graph show framework offer substanti speedup estim graphlet count new graph high accuraci less
279,1810.03077,"
        In this paper we address the task of determining the geographical location of an image, a pertinent problem in learning and computer vision. This research was inspired from playing GeoGuessr, a game that tests a humans' ability to localize themselves using just images of their surroundings. In particular, we wish to investigate how geographical, ecological and man-made features generalize for random location prediction. This is framed as a classification problem: given images sampled from the USA, the most-probable state among 50 is predicted. Previous work uses models extensively trained on large, unfiltered online datasets that are primed towards specific locations. To this end, we create (and open-source) the 50States10K dataset - with 0.5 million Google Street View images of the country. A deep neural network based on the ResNet architecture is trained, and four different strategies of incorporating low-level cardinality information are presented. This model achieves an accuracy 20 times better than chance on a test dataset, which rises to 71.87% when taking the best of top-5 guesses. The network also beats human subjects in 4 out of 5 rounds of GeoGuessr.
        △ Less
",paper address task determin geograph locat imag pertin problem learn comput vision research inspir play geoguessr game test human abil local use imag surround particular wish investig geograph ecolog man made featur gener random locat predict frame classif problem given imag sampl usa probabl state among predict previou work use model extens train larg unfilt onlin dataset prime toward specif locat end creat open sourc state k dataset million googl street view imag countri deep neural network base resnet architectur train four differ strategi incorpor low level cardin inform present model achiev accuraci time better chanc test dataset rise take best top guess network also beat human subject round geoguessr less
280,1810.03076,"
        We present a novel application of robust control and online learning for the balancing of a n Degree of Freedom (DoF), Wheeled Inverted Pendulum (WIP) humanoid robot. Our technique condenses the inaccuracies of a mass model into a Center of Mass (CoM) error, balances despite this error, and uses online learning to update the mass model for a better CoM estimate. Using a simulated model of our robot, we meta-learn a set of excitory joint poses that makes our gradient descent algorithm quickly converge to an accurate (CoM) estimate. This simulated pipeline executes in a fully online fashion, using active disturbance rejection to address the mass errors that result from a steadily evolving mass model. Experiments were performed on a 19 DoF WIP, in which we manually acquired the data for the learned set of poses and show that the mass model produced by a gradient descent produces a CoM estimate that improves overall control and efficiency. This work contributes to a greater corpus of whole body control on the Golem Krang humanoid robot.
        △ Less
",present novel applic robust control onlin learn balanc n degre freedom dof wheel invert pendulum wip humanoid robot techniqu condens inaccuraci mass model center mass com error balanc despit error use onlin learn updat mass model better com estim use simul model robot meta learn set excitori joint pose make gradient descent algorithm quickli converg accur com estim simul pipelin execut fulli onlin fashion use activ disturb reject address mass error result steadili evolv mass model experi perform dof wip manual acquir data learn set pose show mass model produc gradient descent produc com estim improv overal control effici work contribut greater corpu whole bodi control golem krang humanoid robot less
281,1810.03075,"
        Automated cell detection and localization from microscopy images are significant tasks in biomedical research and clinical practice. In this paper, we design a new cell detection and localization algorithm that combines deep convolutional neural network (CNN) and compressed sensing (CS) or sparse coding (SC) for end-to-end training. We also derive, for the first time, a backpropagation rule, which is applicable to train any algorithm that implements a sparse code recovery layer. The key observation behind our algorithm is that cell detection task is a point object detection task in computer vision, where the cell centers (i.e., point objects) occupy only a tiny fraction of the total number of pixels in an image. Thus, we can apply compressed sensing (or, equivalently sparse coding) to compactly represent a variable number of cells in a projected space. Then, CNN regresses this compressed vector from the input microscopy image. Thanks to the SC/CS recovery algorithm (L1 optimization) that can recover sparse cell locations from the output of CNN. We train this entire processing pipeline end-to-end and demonstrate that end-to-end training provides accuracy improvements over a training paradigm that treats CNN and CS-recovery layers separately. Our algorithm design also takes into account a form of ensemble average of trained models naturally to further boost accuracy of cell detection. We have validated our algorithm on benchmark datasets and achieved excellent performances.
        △ Less
",autom cell detect local microscopi imag signific task biomed research clinic practic paper design new cell detect local algorithm combin deep convolut neural network cnn compress sens cs spars code sc end end train also deriv first time backpropag rule applic train algorithm implement spars code recoveri layer key observ behind algorithm cell detect task point object detect task comput vision cell center e point object occupi tini fraction total number pixel imag thu appli compress sens equival spars code compactli repres variabl number cell project space cnn regress compress vector input microscopi imag thank sc cs recoveri algorithm l optim recov spars cell locat output cnn train entir process pipelin end end demonstr end end train provid accuraci improv train paradigm treat cnn cs recoveri layer separ algorithm design also take account form ensembl averag train model natur boost accuraci cell detect valid algorithm benchmark dataset achiev excel perform less
282,1810.03074,"
        In this paper, we present a whole-body control framework for Wheeled Inverted Pendulum (WIP) Humanoids. WIP Humanoids are redundant manipulators dynamically balancing themselves on wheels. Characterized by several degrees of freedom, they have the ability to perform several tasks simultaneously, such as balancing, maintaining a body pose, controlling the gaze, lifting a load or maintaining end-effector configuration in operation space. The problem of whole-body control is to enable simultaneous performance of these tasks with optimal participation of all degrees of freedom at specified priorities for each objective. The control also has to obey constraint of angle and torque limits on each joint. The proposed approach is hierarchical with a low level controller for body joints manipulation and a high-level controller that defines center of mass (CoM) targets for the low-level controller to control zero dynamics of the system driving the wheels. The low-level controller plans for shorter horizons while considering more complete dynamics of the system, while the high-level controller plans for longer horizon based on an approximate model of the robot for computational efficiency.
        △ Less
",paper present whole bodi control framework wheel invert pendulum wip humanoid wip humanoid redund manipul dynam balanc wheel character sever degre freedom abil perform sever task simultan balanc maintain bodi pose control gaze lift load maintain end effector configur oper space problem whole bodi control enabl simultan perform task optim particip degre freedom specifi prioriti object control also obey constraint angl torqu limit joint propos approach hierarch low level control bodi joint manipul high level control defin center mass com target low level control control zero dynam system drive wheel low level control plan shorter horizon consid complet dynam system high level control plan longer horizon base approxim model robot comput effici less
283,1810.03071,"
        Search-based motion planning has been used for mobile robots in many applications. However, it has not been fully developed and applied for planning full state trajectories of Micro Aerial Vehicles (MAVs) due to their complicated dynamics and the requirement of real-time computation. In this paper, we explore a search-based motion planning framework that plans dynamically feasible, collision-free, and resolution optimal and complete trajectories. This paper extends the search-based planning approach to address three important scenarios for MAVs navigation: (i) planning safe trajectories in the presence of motion uncertainty; (ii) planning with constraints on field-of-view and (iii) planning in dynamic environments. We show that these problems can be solved effectively and efficiently using the proposed search-based planning with motion primitives.
        △ Less
",search base motion plan use mobil robot mani applic howev fulli develop appli plan full state trajectori micro aerial vehicl mav due complic dynam requir real time comput paper explor search base motion plan framework plan dynam feasibl collis free resolut optim complet trajectori paper extend search base plan approach address three import scenario mav navig plan safe trajectori presenc motion uncertainti ii plan constraint field view iii plan dynam environ show problem solv effect effici use propos search base plan motion primit less
284,1810.03069,"
        Shared edge computing platforms deployed at the radio access network are expected to significantly improve quality of service delivered by Application Service Providers (ASPs) in a flexible and economic way. However, placing edge service in every possible edge site by an ASP is practically infeasible due to the ASP's prohibitive budget requirement. In this paper, we investigate the edge service placement problem of an ASP under a limited budget, where the ASP dynamically rents computing/storage resources in edge sites to host its applications in close proximity to end users. Since the benefit of placing edge service in a specific site is usually unknown to the ASP a priori, optimal placement decisions must be made while learning this benefit. We pose this problem as a novel combinatorial contextual bandit learning problem. It is ""combinatorial"" because only a limited number of edge sites can be rented to provide the edge service given the ASP's budget. It is ""contextual"" because we utilize user context information to enable finer-grained learning and decision making. To solve this problem and optimize the edge computing performance, we propose SEEN, a Spatial-temporal Edge sErvice placemeNt algorithm. Furthermore, SEEN is extended to scenarios with overlapping service coverage by incorporating a disjunctively constrained knapsack problem. In both cases, we prove that our algorithm achieves a sublinear regret bound when it is compared to an oracle algorithm that knows the exact benefit information. Simulations are carried out on a real-world dataset, whose results show that SEEN significantly outperforms benchmark solutions.
        △ Less
",share edg comput platform deploy radio access network expect significantli improv qualiti servic deliv applic servic provid asp flexibl econom way howev place edg servic everi possibl edg site asp practic infeas due asp prohibit budget requir paper investig edg servic placement problem asp limit budget asp dynam rent comput storag resourc edg site host applic close proxim end user sinc benefit place edg servic specif site usual unknown asp priori optim placement decis must made learn benefit pose problem novel combinatori contextu bandit learn problem combinatori limit number edg site rent provid edg servic given asp budget contextu util user context inform enabl finer grain learn decis make solv problem optim edg comput perform propos seen spatial tempor edg servic placement algorithm furthermor seen extend scenario overlap servic coverag incorpor disjunct constrain knapsack problem case prove algorithm achiev sublinear regret bound compar oracl algorithm know exact benefit inform simul carri real world dataset whose result show seen significantli outperform benchmark solut less
285,1810.03068,"
        One of the most notable contributions of deep learning is the application of convolutional neural networks (ConvNets) to structured signal classification, and in particular image classification. Beyond their impressive performances in supervised learning, the structure of such networks inspired the development of deep filter banks referred to as scattering transforms. These transforms apply a cascade of wavelet transforms and complex modulus operators to extract features that are invariant to group operations and stable to deformations. Furthermore, ConvNets inspired recent advances in geometric deep learning, which aim to generalize these networks to graph data by applying notions from graph signal processing to learn deep graph filter cascades. We further advance these lines of research by proposing a geometric scattering transform using graph wavelets defined in terms of random walks on the graph. We demonstrate the utility of features extracted with this designed deep filter bank in graph classification, and show its competitive performance relative to other methods, including graph kernel methods and geometric deep learning ones, on both social and biochemistry data.
        △ Less
",one notabl contribut deep learn applic convolut neural network convnet structur signal classif particular imag classif beyond impress perform supervis learn structur network inspir develop deep filter bank refer scatter transform transform appli cascad wavelet transform complex modulu oper extract featur invari group oper stabl deform furthermor convnet inspir recent advanc geometr deep learn aim gener network graph data appli notion graph signal process learn deep graph filter cascad advanc line research propos geometr scatter transform use graph wavelet defin term random walk graph demonstr util featur extract design deep filter bank graph classif show competit perform rel method includ graph kernel method geometr deep learn one social biochemistri data less
286,1810.03067,"
        In this paper, we introduce the first geolocation inference approach for reddit, a social media platform where user pseudonymity has thus far made supervised demographic inference difficult to implement and validate. In particular, we design a text-based heuristic schema to generate ground truth location labels for reddit users in the absence of explicitly geotagged data. After evaluating the accuracy of our labeling procedure, we train and test several geolocation inference models across our reddit data set and three benchmark Twitter geolocation data sets. Ultimately, we show that geolocation models trained and applied on the same domain substantially outperform models attempting to transfer training data across domains, even more so on reddit where platform-specific interest-group metadata can be used to improve inferences.
        △ Less
",paper introduc first geoloc infer approach reddit social media platform user pseudonym thu far made supervis demograph infer difficult implement valid particular design text base heurist schema gener ground truth locat label reddit user absenc explicitli geotag data evalu accuraci label procedur train test sever geoloc infer model across reddit data set three benchmark twitter geoloc data set ultim show geoloc model train appli domain substanti outperform model attempt transfer train data across domain even reddit platform specif interest group metadata use improv infer less
287,1810.03065,"
        We present a novel approach for model-based 6D pose refinement in color data. Building on the established idea of contour-based pose tracking, we teach a deep neural network to predict a translational and rotational update. At the core, we propose a new visual loss that drives the pose update by aligning object contours, thus avoiding the definition of any explicit appearance model. In contrast to previous work our method is correspondence-free, segmentation-free, can handle occlusion and is agnostic to geometrical symmetry as well as visual ambiguities. Additionally, we observe a strong robustness towards rough initialization. The approach can run in real-time and produces pose accuracies that come close to 3D ICP without the need for depth data. Furthermore, our networks are trained from purely synthetic data and will be published together with the refinement code to ensure reproducibility.
        △ Less
",present novel approach model base pose refin color data build establish idea contour base pose track teach deep neural network predict translat rotat updat core propos new visual loss drive pose updat align object contour thu avoid definit explicit appear model contrast previou work method correspond free segment free handl occlus agnost geometr symmetri well visual ambigu addit observ strong robust toward rough initi approach run real time produc pose accuraci come close icp without need depth data furthermor network train pure synthet data publish togeth refin code ensur reproduc less
288,1810.03064,"
        Channel State Information (CSI) of WiFi signals becomes increasingly attractive in human sensing applications due to the pervasiveness of WiFi, robustness to illumination and view points, and little privacy concern comparing to cameras. In majority of existing works, CSI sequences are analyzed by traditional signal processing approaches. These approaches rely on strictly imposed assumption on propagation paths, reflection and attenuation of signal interacting with human bodies and indoor background. This makes existing approaches very difficult to model the delicate body characteristics and activities in the real applications. To address these issues, we build CSI-Net, a unified Deep Neural Network (DNN), that fully utilizes the strength of deep feature representation and the power of existing DNN architectures for CSI-based human sensing problems. Using CSI-Net, we jointly solved two body characterization problems: biometrics estimation (including body fat, muscle, water and bone rates) and human identification. We also demonstrated the application of CSI-Net on two distinctive action recognition tasks: the hand sign recognition (fine-scaled action of the hand) and falling detection (coarse-scaled motion of the body). Besides the technical contribution of CSI-Net, we present major discoveries and insights on how the multi-frequency CSI signals are encoded and processed in DNNs, which, to the best of our knowledge, is the first attempt that bridges the WiFi sensing and deep learning in human sensing problems.
        △ Less
",channel state inform csi wifi signal becom increasingli attract human sens applic due pervas wifi robust illumin view point littl privaci concern compar camera major exist work csi sequenc analyz tradit signal process approach approach reli strictli impos assumpt propag path reflect attenu signal interact human bodi indoor background make exist approach difficult model delic bodi characterist activ real applic address issu build csi net unifi deep neural network dnn fulli util strength deep featur represent power exist dnn architectur csi base human sens problem use csi net jointli solv two bodi character problem biometr estim includ bodi fat muscl water bone rate human identif also demonstr applic csi net two distinct action recognit task hand sign recognit fine scale action hand fall detect coars scale motion bodi besid technic contribut csi net present major discoveri insight multi frequenc csi signal encod process dnn best knowledg first attempt bridg wifi sens deep learn human sens problem less
289,1810.03063,"
        There has been tremendous recent progress on equilibrium-finding algorithms for zero-sum imperfect-information extensive-form games, but there has been a puzzling gap between theory and practice. First-order methods have significantly better theoretical convergence rates than any counterfactual-regret minimization (CFR) variant. Despite this, CFR variants have been favored in practice. Experiments with first-order methods have only been conducted on small- and medium-sized games because those methods are complicated to implement in this setting, and because CFR variants have been enhanced extensively for over a decade they perform well in practice. In this paper we show that a particular first-order method, a state-of-the-art variant of the excessive gap technique---instantiated with the dilated entropy distance function---can efficiently solve large real-world problems competitively with CFR and its variants. We show this on large endgames encountered by the Libratus poker AI, which recently beat top human poker specialist professionals at no-limit Texas hold'em. We show experimental results on our variant of the excessive gap technique as well as a prior version. We introduce a numerically friendly implementation of the smoothed best response computation associated with first-order methods for extensive-form game solving. We present, to our knowledge, the first GPU implementation of a first-order method for extensive-form games. We present comparisons of several excessive gap technique and CFR variants.
        △ Less
",tremend recent progress equilibrium find algorithm zero sum imperfect inform extens form game puzzl gap theori practic first order method significantli better theoret converg rate counterfactu regret minim cfr variant despit cfr variant favor practic experi first order method conduct small medium size game method complic implement set cfr variant enhanc extens decad perform well practic paper show particular first order method state art variant excess gap techniqu instanti dilat entropi distanc function effici solv larg real world problem competit cfr variant show larg endgam encount libratu poker ai recent beat top human poker specialist profession limit texa hold em show experiment result variant excess gap techniqu well prior version introduc numer friendli implement smooth best respons comput associ first order method extens form game solv present knowledg first gpu implement first order method extens form game present comparison sever excess gap techniqu cfr variant less
290,1810.03060,"
        Packet scheduling determines the ordering of packets in a queuing data structure with respect to some ranking function that is mandated by a scheduling policy. It is the core component in many recent innovations to optimize network performance and utilization. Our focus in this paper is on the design and deployment of packet scheduling in software. Software schedulers have several advantages over hardware including shorter development cycle and flexibility in functionality and deployment location. We substantially improve current software packet scheduling performance, while maintaining flexibility, by exploiting underlying features of packet ranking; namely, packet ranks are integers and, at any point in time, fall within a limited range of values. We introduce Eiffel, a novel programmable packet scheduling system. At the core of Eiffel is an integer priority queue based on the Find First Set (FFS) instruction and designed to support a wide range of policies and ranking functions efficiently. As an even more efficient alternative, we also propose a new approximate priority queue that can outperform FFS-based queues for some scenarios. To support flexibility, Eiffel introduces novel programming abstractions to express scheduling policies that cannot be captured by current, state-of-the-art scheduler programming models. We evaluate Eiffel in a variety of settings and in both kernel and userspace deployments. We show that it outperforms state of the art systems by 3-40x in terms of either number of cores utilized for network processing or number of flows given fixed processing capacity.
        △ Less
",packet schedul determin order packet queu data structur respect rank function mandat schedul polici core compon mani recent innov optim network perform util focu paper design deploy packet schedul softwar softwar schedul sever advantag hardwar includ shorter develop cycl flexibl function deploy locat substanti improv current softwar packet schedul perform maintain flexibl exploit underli featur packet rank name packet rank integ point time fall within limit rang valu introduc eiffel novel programm packet schedul system core eiffel integ prioriti queue base find first set ff instruct design support wide rang polici rank function effici even effici altern also propos new approxim prioriti queue outperform ff base queue scenario support flexibl eiffel introduc novel program abstract express schedul polici cannot captur current state art schedul program model evalu eiffel varieti set kernel userspac deploy show outperform state art system x term either number core util network process number flow given fix process capac less
291,1810.03056,"
        The advent of experimental science facilities, instruments and observatories, such as the Large Hadron Collider (LHC), the Laser Interferometer Gravitational Wave Observatory (LIGO), and the upcoming Large Synoptic Survey Telescope (LSST), has brought about challenging, large-scale computational and data processing requirements. Traditionally, the computing infrastructures to support these facility's requirements were organized into separate infrastructure that supported their high-throughput needs and those that supported their high-performance computing needs. We argue that in order to enable and accelerate scientific discovery at the scale and sophistication that is now needed, this separation between High-Performance Computing (HPC) and High-Throughput Computing (HTC) must be bridged and an integrated, unified infrastructure must be provided. In this paper, we discuss several case studies where such infrastructures have been implemented. These case studies span different science domains, software systems, and application requirements as well as levels of sustainable. A further aim of this paper is to provide a basis to determine the common characteristics and requirements of such infrastructures, as well as to begin a discussion of how best to support the computing requirements of existing and future experimental science facilities.
        △ Less
",advent experiment scienc facil instrument observatori larg hadron collid lhc laser interferomet gravit wave observatori ligo upcom larg synopt survey telescop lsst brought challeng larg scale comput data process requir tradit comput infrastructur support facil requir organ separ infrastructur support high throughput need support high perform comput need argu order enabl acceler scientif discoveri scale sophist need separ high perform comput hpc high throughput comput htc must bridg integr unifi infrastructur must provid paper discuss sever case studi infrastructur implement case studi span differ scienc domain softwar system applic requir well level sustain aim paper provid basi determin common characterist requir infrastructur well begin discuss best support comput requir exist futur experiment scienc facil less
292,1810.03052,"
        We propose deep convolutional Gaussian processes, a deep Gaussian process architecture with convolutional structure. The model is a principled Bayesian framework for detecting hierarchical combinations of local features for image classification. We demonstrate greatly improved image classification performance compared to current Gaussian process approaches on the MNIST and CIFAR-10 datasets. In particular, we improve CIFAR-10 accuracy by over 10 percentage points.
        △ Less
",propos deep convolut gaussian process deep gaussian process architectur convolut structur model principl bayesian framework detect hierarch combin local featur imag classif demonstr greatli improv imag classif perform compar current gaussian process approach mnist cifar dataset particular improv cifar accuraci percentag point less
293,1810.03051,"
        We study the related problems of subspace tracking in the presence of missing data (ST-miss) as well as robust subspace tracking with missing data (RST-miss). Here ""robust"" refers to robustness to sparse outliers. In recent work, we have studied the RST problem without missing data. In this work, we show that simple modifications of our solution approach for RST also provably solve ST-miss and RST-miss under weaker and similar assumptions respectively. To our knowledge, our result is the first complete guarantee for both ST-miss and RST-miss. This means we are able to show that, under assumptions on only the algorithm inputs (input data and/or initialization), the output subspace estimates are close to the true data subspaces at all times. Our guarantees hold under mild and easily interpretable assumptions and handle time-varying subspaces (unlike all previous work). We also show that our algorithm and its extensions are fast and have competitive experimental performance when compared with existing methods.
        △ Less
",studi relat problem subspac track presenc miss data st miss well robust subspac track miss data rst miss robust refer robust spars outlier recent work studi rst problem without miss data work show simpl modif solut approach rst also provabl solv st miss rst miss weaker similar assumpt respect knowledg result first complet guarante st miss rst miss mean abl show assumpt algorithm input input data initi output subspac estim close true data subspac time guarante hold mild easili interpret assumpt handl time vari subspac unlik previou work also show algorithm extens fast competit experiment perform compar exist method less
294,1810.03048,"
        We present the first PAC optimal algorithm for Bayes-Adaptive Markov Decision Processes (BAMDPs) in continuous state and action spaces, to the best of our knowledge. The BAMDP framework elegantly addresses model uncertainty by incorporating Bayesian belief updates into long-term expected return. However, computing an exact optimal Bayesian policy is intractable. Our key insight is to compute a near-optimal value function by covering the continuous state-belief-action space with a finite set of representative samples and exploiting the Lipschitz continuity of the value function. We prove the near-optimality of our algorithm and analyze a number of schemes that boost the algorithm's efficiency. Finally, we empirically validate our approach on a number of discrete and continuous BAMDPs and show that the learned policy has consistently competitive performance against baseline approaches.
        △ Less
",present first pac optim algorithm bay adapt markov decis process bamdp continu state action space best knowledg bamdp framework elegantli address model uncertainti incorpor bayesian belief updat long term expect return howev comput exact optim bayesian polici intract key insight comput near optim valu function cover continu state belief action space finit set repres sampl exploit lipschitz continu valu function prove near optim algorithm analyz number scheme boost algorithm effici final empir valid approach number discret continu bamdp show learn polici consist competit perform baselin approach less
295,1810.03046,"
        Meetup.com is a global online platform which facilitates the organisation of meetups in different parts of the world. A meetup group typically focuses on one specific topic of interest, such as sports, music, language, or technology. However, many users of this platform attend multiple meetups. On this basis, we can construct a co-membership network for a given location. This network encodes how pairs of meetups are connected to one another via common members. In this work we demonstrate that, by applying techniques from social network analysis to this type of representation, we can reveal the underlying meetup community structure, which is not immediately apparent from the platform's website. Specifically, we map the landscape of Dublin's meetup communities, to explore the interests and activities of meetup.com users in the city.
        △ Less
",meetup com global onlin platform facilit organis meetup differ part world meetup group typic focus one specif topic interest sport music languag technolog howev mani user platform attend multipl meetup basi construct co membership network given locat network encod pair meetup connect one anoth via common member work demonstr appli techniqu social network analysi type represent reveal underli meetup commun structur immedi appar platform websit specif map landscap dublin meetup commun explor interest activ meetup com user citi less
296,1810.03043,"
        Prediction is an appealing objective for self-supervised learning of behavioral skills, particularly for autonomous robots. However, effectively utilizing predictive models for control, especially with raw image inputs, poses a number of major challenges. How should the predictions be used? What happens when they are inaccurate? In this paper, we tackle these questions by proposing a method for learning robotic skills from raw image observations, using only autonomously collected experience. We show that even an imperfect model can complete complex tasks if it can continuously retry, but this requires the model to not lose track of the objective (e.g., the object of interest). To enable a robot to continuously retry a task, we devise a self-supervised algorithm for learning image registration, which can keep track of objects of interest for the duration of the trial. We demonstrate that this idea can be combined with a video-prediction based controller to enable complex behaviors to be learned from scratch using only raw visual inputs, including grasping, repositioning objects, and non-prehensile manipulation. Our real-world experiments demonstrate that a model trained with 160 robot hours of autonomously collected, unlabeled data is able to successfully perform complex manipulation tasks with a wide range of objects not seen during training.
        △ Less
",predict appeal object self supervis learn behavior skill particularli autonom robot howev effect util predict model control especi raw imag input pose number major challeng predict use happen inaccur paper tackl question propos method learn robot skill raw imag observ use autonom collect experi show even imperfect model complet complex task continu retri requir model lose track object e g object interest enabl robot continu retri task devis self supervis algorithm learn imag registr keep track object interest durat trial demonstr idea combin video predict base control enabl complex behavior learn scratch use raw visual input includ grasp reposit object non prehensil manipul real world experi demonstr model train robot hour autonom collect unlabel data abl success perform complex manipul task wide rang object seen train less
297,1810.03037,"
        Empirical evidence suggests that neural networks with ReLU activations generalize better with over-parameterization. However, there is currently no theoretical analysis that explains this observation. In this work, we study a simplified learning task with over-parameterized convolutional networks that empirically exhibits the same qualitative phenomenon. For this setting, we provide a theoretical analysis of the optimization and generalization performance of gradient descent. Specifically, we prove data-dependent sample complexity bounds which show that over-parameterization improves the generalization performance of gradient descent.
        △ Less
",empir evid suggest neural network relu activ gener better parameter howev current theoret analysi explain observ work studi simplifi learn task parameter convolut network empir exhibit qualit phenomenon set provid theoret analysi optim gener perform gradient descent specif prove data depend sampl complex bound show parameter improv gener perform gradient descent less
298,1810.03035,"
        The performance of Deep-Learning (DL) computing frameworks rely on the performance of data ingestion and checkpointing. In fact, during the training, a considerable high number of relatively small files are first loaded and pre-processed on CPUs and then moved to accelerator for computation. In addition, checkpointing and restart operations are carried out to allow DL computing frameworks to restart quickly from a checkpoint. Because of this, I/O affects the performance of DL applications. In this work, we characterize the I/O performance and scaling of TensorFlow, an open-source programming framework developed by Google and specifically designed for solving DL problems. To measure TensorFlow I/O performance, we first design a micro-benchmark to measure TensorFlow reads, and then use a TensorFlow mini-application based on AlexNet to measure the performance cost of I/O and checkpointing in TensorFlow. To improve the checkpointing performance, we design and implement a burst buffer. We find that increasing the number of threads increases TensorFlow bandwidth by a maximum of 2.3x and 7.8x on our benchmark environments. The use of the tensorFlow prefetcher results in a complete overlap of computation on accelerator and input pipeline on CPU eliminating the effective cost of I/O on the overall performance. The use of a burst buffer to checkpoint to a fast small capacity storage and copy asynchronously the checkpoints to a slower large capacity storage resulted in a performance improvement of 2.6x with respect to checkpointing directly to slower storage on our benchmark environment.
        △ Less
",perform deep learn dl comput framework reli perform data ingest checkpoint fact train consider high number rel small file first load pre process cpu move acceler comput addit checkpoint restart oper carri allow dl comput framework restart quickli checkpoint affect perform dl applic work character perform scale tensorflow open sourc program framework develop googl specif design solv dl problem measur tensorflow perform first design micro benchmark measur tensorflow read use tensorflow mini applic base alexnet measur perform cost checkpoint tensorflow improv checkpoint perform design implement burst buffer find increas number thread increas tensorflow bandwidth maximum x x benchmark environ use tensorflow prefetch result complet overlap comput acceler input pipelin cpu elimin effect cost overal perform use burst buffer checkpoint fast small capac storag copi asynchron checkpoint slower larg capac storag result perform improv x respect checkpoint directli slower storag benchmark environ less
299,1810.03031,"
        Sentiment polarity of tweets, blog posts or product reviews has become highly attractive and is utilized in recommender systems, market predictions, business intelligence and more. Deep learning techniques are becoming top performers on analyzing such texts. There are however several problems that need to be solved for efficient use of deep neural networks on text mining and text polarity analysis. First, deep neural networks need to be fed with data sets that are big in size as well as properly labeled. Second, there are various uncertainties regarding the use of word embedding vectors: should they be generated from the same data set that is used to train the model or it is better to source them from big and popular collections? Third, to simplify model creation it is convenient to have generic neural network architectures that are effective and can adapt to various texts, encapsulating much of design complexity. This thesis addresses the above problems to provide methodological and practical insights for utilizing neural networks on sentiment analysis of texts and achieving state of the art results. Regarding the first problem, the effectiveness of various crowdsourcing alternatives is explored and two medium-sized and emotion-labeled song data sets are created utilizing social tags. To address the second problem, a series of experiments with large text collections of various contents and domains were conducted, trying word embeddings of various parameters. Regarding the third problem, a series of experiments involving convolution and max-pooling neural layers were conducted. Combining convolutions of words, bigrams, and trigrams with regional max-pooling layers in a couple of stacks produced the best results. The derived architecture achieves competitive performance on sentiment polarity analysis of movie, business and product reviews.
        △ Less
",sentiment polar tweet blog post product review becom highli attract util recommend system market predict busi intellig deep learn techniqu becom top perform analyz text howev sever problem need solv effici use deep neural network text mine text polar analysi first deep neural network need fed data set big size well properli label second variou uncertainti regard use word embed vector gener data set use train model better sourc big popular collect third simplifi model creation conveni gener neural network architectur effect adapt variou text encapsul much design complex thesi address problem provid methodolog practic insight util neural network sentiment analysi text achiev state art result regard first problem effect variou crowdsourc altern explor two medium size emot label song data set creat util social tag address second problem seri experi larg text collect variou content domain conduct tri word embed variou paramet regard third problem seri experi involv convolut max pool neural layer conduct combin convolut word bigram trigram region max pool layer coupl stack produc best result deriv architectur achiev competit perform sentiment polar analysi movi busi product review less
300,1810.03024,"
        We introduce algorithms that achieve state-of-the-art \emph{dynamic regret} bounds for non-stationary linear stochastic bandits setting. It captures natural applications such as dynamic pricing and ads allocation in a changing environment. We show how the difficulty posed by the (possibly adversarial) non-stationarity can be overcome by a novel marriage between stochastic and adversarial bandits learning algorithms. Defining $d,B_T,$ and $T$ as the problem dimension, the \emph{variation budget}, and the total time horizon, respectively, our main contributions are the tuned Sliding Window UCB (\texttt{SW-UCB}) algorithm with optimal $\widetilde{O}(d^{2/3}(B_T+1)^{1/3}T^{2/3})$ dynamic regret, and the tuning free bandits-over-bandits (\texttt{BOB}) framework built on top of the \texttt{SW-UCB} algorithm with best $\widetilde{O}(d^{2/3}(B_T+1)^{1/4}T^{3/4})$ dynamic regret.
        △ Less
",introduc algorithm achiev state art emph dynam regret bound non stationari linear stochast bandit set captur natur applic dynam price ad alloc chang environ show difficulti pose possibl adversari non stationar overcom novel marriag stochast adversari bandit learn algorithm defin b problem dimens emph variat budget total time horizon respect main contribut tune slide window ucb texttt sw ucb algorithm optim widetild b dynam regret tune free bandit bandit texttt bob framework built top texttt sw ucb algorithm best widetild b dynam regret less
301,1810.03019,"
        This paper reports on a simple visual technique that boils extracting a subgraph down to two operations---pivots and filters---that is agnostic to both the data abstraction and the size of the graph. The system's design, as well as its qualitative evaluation with users, clarify exactly when and how the user's intent in a series of pivots is ambiguous---and, more usefully, when it is not. Reflections on our results show how, in the event of an ambiguous case, this innately practical operation could be further extended into ""smart pivots"" that anticipate the user's intent beyond the current step. They also reveal ways that a series of graph pivots can expose the semantics of the data from the user's perspective, and how this information could be leveraged to create adaptive data abstractions that do not rely as heavily on a system designer to create a comprehensive abstraction that anticipates all the user's tasks.
        △ Less
",paper report simpl visual techniqu boil extract subgraph two oper pivot filter agnost data abstract size graph system design well qualit evalu user clarifi exactli user intent seri pivot ambigu use reflect result show event ambigu case innat practic oper could extend smart pivot anticip user intent beyond current step also reveal way seri graph pivot expos semant data user perspect inform could leverag creat adapt data abstract reli heavili system design creat comprehens abstract anticip user task less
302,1810.03018,"
        A radar system emits probing signals and records the reflections. Estimating the relative angles, delays, and Doppler shifts from the received signals allows to determine the locations and velocities of objects. However, due to practical constraints, the probing signals have finite bandwidth B, the received signals are observed over a finite time interval of length T only, and a radar typically has only one or a few transmit and receive antennas. These constraints fundamentally limit the resolution up to which objects can be distinguished. Specifically, a radar can not distinguish objects with delay and Doppler shifts much closer than 1/B and 1/T, respectively, and a radar system with N_T transmit and N_R receive antennas cannot distinguish objects with angels closer than 1/(N_T N_R). As a consequence, the delay, Doppler, and angular resolution of standard radars is proportional to 1/B and 1/T, and 1/(N_T N_R). In this chapter, we show that the continuous angle-delay-Doppler triplets and the corresponding attenuation factors can be resolved at much finer resolution, using ideas from compressive sensing. Specifically, provided the angle-delay-Doppler triplets are separated either by factors proportional to 1/(N_T N_R-1) in angle, 1/B in delay, or 1/T in Doppler direction, they can be recovered a significantly smaller scale or higher resolution.
        △ Less
",radar system emit probe signal record reflect estim rel angl delay doppler shift receiv signal allow determin locat veloc object howev due practic constraint probe signal finit bandwidth b receiv signal observ finit time interv length radar typic one transmit receiv antenna constraint fundament limit resolut object distinguish specif radar distinguish object delay doppler shift much closer b respect radar system n transmit n r receiv antenna cannot distinguish object angel closer n n r consequ delay doppler angular resolut standard radar proport b n n r chapter show continu angl delay doppler triplet correspond attenu factor resolv much finer resolut use idea compress sens specif provid angl delay doppler triplet separ either factor proport n n r angl b delay doppler direct recov significantli smaller scale higher resolut less
303,1810.03010,"
        Industry 4.0 envisions a fully automated manufacturing environment, in which computerized manufacturing equipment--Cyber-Physical Systems (CPS)--performs all tasks. These machines are open to a variety of cyber and cyber-physical attacks, including sabotage. In the manufacturing context, sabotage attacks aim to damage equipment or degrade a manufactured part's mechanical properties. In this paper, we focus on the latter, specifically for composite materials. Composite material parts are predominantly used in safety-critical systems, e.g., as load-bearing parts of aircraft. Further, we distinguish between the methods to compromise various manufacturing equipment, and the malicious manipulations that will sabotage a part. As the research literature has numerous examples of the former, in this paper we assume that the equipment is already compromised, our discussion is solely on manipulations.
  We develop a simulation approach to designing sabotage attacks against composite material parts. The attack can be optimized by two criteria, minimizing the ""footprint"" of manipulations. We simulate two optimal attacks against the design of a spar, a load bearing component of an airplane wing. Our simulation identifies the minimal manipulations needed to degrade its strength to three desired levels, as well as the resulting failure characteristics. Last but not least, we outline an approach to identifying sabotaged parts.
        △ Less
",industri envis fulli autom manufactur environ computer manufactur equip cyber physic system cp perform task machin open varieti cyber cyber physic attack includ sabotag manufactur context sabotag attack aim damag equip degrad manufactur part mechan properti paper focu latter specif composit materi composit materi part predominantli use safeti critic system e g load bear part aircraft distinguish method compromis variou manufactur equip malici manipul sabotag part research literatur numer exampl former paper assum equip alreadi compromis discuss sole manipul develop simul approach design sabotag attack composit materi part attack optim two criteria minim footprint manipul simul two optim attack design spar load bear compon airplan wing simul identifi minim manipul need degrad strength three desir level well result failur characterist last least outlin approach identifi sabotag part less
304,1810.03005,"
        Women are severely marginalized in software development, especially in open source. In this article we argue that disadvantage is more due to gendered behavior than to categorical discrimination: women are at a disadvantage because of what they do, rather than because of who they are. Using data on entire careers of users from GitHub.com, we develop a measure to capture the gendered pattern of behavior: We use a random forest prediction of being female (as opposed to being male) by behavioral choices in the level of activity, specialization in programming languages, and choice of partners. We test differences in success and survival along both categorical gender and the gendered pattern of behavior. We find that 84.5% of women's disadvantage (compared to men) in success and 34.8% of their disadvantage in survival are due to the female pattern of their behavior. Men are also disadvantaged along their interquartile range of the female pattern of their behavior, and users who don't reveal their gender suffer an even more drastic disadvantage in survival probability. Moreover, we do not see evidence for any reduction of these inequalities in time. Our findings are robust to noise in gender recognition, and to taking into account particular programming languages, or decision tree classes of gendered behavior. Our results suggest that fighting categorical gender discrimination will have a limited impact on gender inequalities in open source software development, and that gender hiding is not a viable strategy for women.
        △ Less
",women sever margin softwar develop especi open sourc articl argu disadvantag due gender behavior categor discrimin women disadvantag rather use data entir career user github com develop measur captur gender pattern behavior use random forest predict femal oppos male behavior choic level activ special program languag choic partner test differ success surviv along categor gender gender pattern behavior find women disadvantag compar men success disadvantag surviv due femal pattern behavior men also disadvantag along interquartil rang femal pattern behavior user reveal gender suffer even drastic disadvantag surviv probabl moreov see evid reduct inequ time find robust nois gender recognit take account particular program languag decis tree class gender behavior result suggest fight categor gender discrimin limit impact gender inequ open sourc softwar develop gender hide viabl strategi women less
305,1810.03002,"
        We analyse so-called computable laws, i.e., laws that can be enforced by automatic procedures. These laws should be logically perfect and unambiguous, but sometimes they are not. We use a regulation on road transport to illustrate this issue, and show what some fragments of this regulation would look like if rewritten in the image of logic. We further propose desiderata to be fulfilled by computable laws, and provide a critical platform from which to assess existing laws and a guideline for composing future ones.
        △ Less
",analys call comput law e law enforc automat procedur law logic perfect unambigu sometim use regul road transport illustr issu show fragment regul would look like rewritten imag logic propos desiderata fulfil comput law provid critic platform assess exist law guidelin compos futur one less
306,1810.02999,"
        In computational 3D geometric problems involving rotations, it is often that people have to convert back and forth between a rotational matrix and a rotation described by an axis and a corresponding angle. For this purpose, Rodrigues' rotation formula is a very popular expression to use because of its simplicity and efficiency. Nevertheless, while converting a rotation matrix to an axis of rotation and the rotation angle, there exists ambiguity. Further judgement or even manual interference may be necessary in some situations. An extension of the Rodrigues' formula helps to find the sine and cosine values of the rotation angle with respect to a given rotation axis is found and this simple extension may help to accelerate many applications.
        △ Less
",comput geometr problem involv rotat often peopl convert back forth rotat matrix rotat describ axi correspond angl purpos rodrigu rotat formula popular express use simplic effici nevertheless convert rotat matrix axi rotat rotat angl exist ambigu judgement even manual interfer may necessari situat extens rodrigu formula help find sine cosin valu rotat angl respect given rotat axi found simpl extens may help acceler mani applic less
307,1810.02997,"
        The Mohamed Bin Zayed International Robotics Challenge (MBZIRC) 2017 has defined ambitious new benchmarks to advance the state-of-the-art in autonomous operation of ground-based and flying robots. In this article, we describe our winning entry to MBZIRC Challenge 2: the mobile manipulation robot Mario. It is capable of autonomously solving a valve manipulation task using a wrench tool detected, grasped, and finally employed to turn a valve stem. Mario's omnidirectional base allows both fast locomotion and precise close approach to the manipulation panel. We describe an efficient detector for medium-sized objects in 3D laser scans and apply it to detect the manipulation panel. An object detection architecture based on deep neural networks is used to find and select the correct tool from grayscale images. Parametrized motion primitives are adapted online to percepts of the tool and valve stem in order to turn the stem. We report in detail on our winning performance at the challenge and discuss lessons learned.
        △ Less
",moham bin zay intern robot challeng mbzirc defin ambiti new benchmark advanc state art autonom oper ground base fli robot articl describ win entri mbzirc challeng mobil manipul robot mario capabl autonom solv valv manipul task use wrench tool detect grasp final employ turn valv stem mario omnidirect base allow fast locomot precis close approach manipul panel describ effici detector medium size object laser scan appli detect manipul panel object detect architectur base deep neural network use find select correct tool grayscal imag parametr motion primit adapt onlin percept tool valv stem order turn stem report detail win perform challeng discuss lesson learn less
308,1810.02994,"
        As a fundamental and challenging problem in computer vision, hand pose estimation aims to estimate the hand joint locations from depth images. Typically, the problem is modeled as learning a mapping function from images to hand joint coordinates in a data-driven manner. In this paper, we propose Context-Aware Deep Spatio-Temporal Network (CADSTN), a novel method to jointly model the spatio-temporal properties for hand pose estimation. Our proposed network is able to learn the representations of the spatial information and the temporal structure from the image sequences. Moreover, by adopting adaptive fusion method, the model is capable of dynamically weighting different predictions to lay emphasis on sufficient context. Our method is examined on two common benchmarks, the experimental results demonstrate that our proposed approach achieves the best or the second-best performance with state-of-the-art methods and runs in 60fps.
        △ Less
",fundament challeng problem comput vision hand pose estim aim estim hand joint locat depth imag typic problem model learn map function imag hand joint coordin data driven manner paper propos context awar deep spatio tempor network cadstn novel method jointli model spatio tempor properti hand pose estim propos network abl learn represent spatial inform tempor structur imag sequenc moreov adopt adapt fusion method model capabl dynam weight differ predict lay emphasi suffici context method examin two common benchmark experiment result demonstr propos approach achiev best second best perform state art method run fp less
309,1810.02981,"
        Source camera identification is the process of determining which camera or model has been used to capture an image. In the recent years, there has been a rapid growth of research interest in the domain of forensics. In the current work, we describe our Deep Learning approach to the camera detection task of 10 cameras as a part of the Camera Model Identification Challenge hosted by Kaggle.com where our team finished 2nd out of 582 teams with the accuracy on the unseen data of 98%. We used aggressive data augmentations that allowed a model to stay robust against transformations. A number of experiments are carried out on datasets collected by organizers and scraped from the web.
        △ Less
",sourc camera identif process determin camera model use captur imag recent year rapid growth research interest domain forens current work describ deep learn approach camera detect task camera part camera model identif challeng host kaggl com team finish nd team accuraci unseen data use aggress data augment allow model stay robust transform number experi carri dataset collect organ scrape web less
310,1810.02980,"
        Fundamental Big Five personality traits and their facets are known to correlate with a wide range of linguistic features and, accordingly, the recognition of personality traits from text is a well-established NLP task. Obtaining facets information may however require extensive personality inventories and, as a result, existing computational models are usually limited to the recognition of the five main personality categories. Based on these observations, this paper investigates the recognition of a number of personality facets from a Brazilian Facebook corpus obtained (at low cost) from a shorter personality inventory. In doing so, we compare a number of personality facet recognition models, and present preliminary reference results for further studies in the field.
        △ Less
",fundament big five person trait facet known correl wide rang linguist featur accordingli recognit person trait text well establish nlp task obtain facet inform may howev requir extens person inventori result exist comput model usual limit recognit five main person categori base observ paper investig recognit number person facet brazilian facebook corpu obtain low cost shorter person inventori compar number person facet recognit model present preliminari refer result studi field less
311,1810.02978,"
        Content delivery networks (CDN) contribute more than 50% of today's Internet traffic. Meta-CDNs, an evolution of centrally controlled CDNs, promise increased flexibility by multihoming content. So far, efforts to understand the characteristics of Meta-CDNs focus mainly on third-party Meta-CDN services. A common, but unexplored, use case for Meta-CDNs is to use the CDNs mapping infrastructure to form self-operated Meta-CDNs integrating third-party CDNs. These CDNs assist in the build-up phase of a CDN's infrastructure or mitigate capacity shortages by offloading traffic. This paper investigates the Apple CDN as a prominent example of self-operated Meta-CDNs. We describe the involved CDNs, the request-mapping mechanism, and show the cache locations of the Apple CDN using measurements of more than 800 RIPE Atlas probes worldwide. We further measure its load-sharing behavior by observing a major iOS update in Sep. 2017, a significant event potentially reaching up to an estimated 1 billion iOS devices. Furthermore, by analyzing data from a European Eyeball ISP, we quantify third-party traffic offloading effects and find third-party CDNs increase their traffic by 438% while saturating seemingly unrelated links.
        △ Less
",content deliveri network cdn contribut today internet traffic meta cdn evolut central control cdn promis increas flexibl multihom content far effort understand characterist meta cdn focu mainli third parti meta cdn servic common unexplor use case meta cdn use cdn map infrastructur form self oper meta cdn integr third parti cdn cdn assist build phase cdn infrastructur mitig capac shortag offload traffic paper investig appl cdn promin exampl self oper meta cdn describ involv cdn request map mechan show cach locat appl cdn use measur ripe atla probe worldwid measur load share behavior observ major io updat sep signific event potenti reach estim billion io devic furthermor analyz data european eyebal isp quantifi third parti traffic offload effect find third parti cdn increas traffic satur seemingli unrel link less
312,1810.02977,"
        Robotic picking from cluttered bins is a demanding task, for which Amazon Robotics holds challenges. The 2017 Amazon Robotics Challenge (ARC) required stowing items into a storage system, picking specific items, and packing them into boxes. In this paper, we describe the entry of team NimbRo Picking. Our deep object perception pipeline can be quickly and efficiently adapted to new items using a custom turntable capture system and transfer learning. It produces high-quality item segments, on which grasp poses are found. A planning component coordinates manipulation actions between two robot arms, minimizing execution time. The system has been demonstrated successfully at ARC, where our team reached second places in both the picking task and the final stow-and-pick task. We also evaluate individual components.
        △ Less
",robot pick clutter bin demand task amazon robot hold challeng amazon robot challeng arc requir stow item storag system pick specif item pack box paper describ entri team nimbro pick deep object percept pipelin quickli effici adapt new item use custom turntabl captur system transfer learn produc high qualiti item segment grasp pose found plan compon coordin manipul action two robot arm minim execut time system demonstr success arc team reach second place pick task final stow pick task also evalu individu compon less
313,1810.02976,"
        In this paper, we focus on approaches to parallelizing stochastic gradient descent (SGD) wherein data is farmed out to a set of workers, the results of which, after a number of updates, are then combined at a central master node. Although such synchronized SGD approaches parallelize well in idealized computing environments, they often fail to realize their promised computational acceleration in practical settings. One cause is slow workers, termed stragglers, who can cause the fusion step at the master node to stall, which greatly slowing convergence. In many straggler mitigation approaches work completed by these nodes, while only partial, is discarded completely. In this paper, we propose an approach to parallelizing synchronous SGD that exploits the work completed by all workers. The central idea is to fix the computation time of each worker and then to combine distinct contributions of all workers. We provide a convergence analysis and optimize the combination function. Our numerical results demonstrate an improvement of several factors of magnitude in comparison to existing methods.
        △ Less
",paper focu approach parallel stochast gradient descent sgd wherein data farm set worker result number updat combin central master node although synchron sgd approach parallel well ideal comput environ often fail realiz promis comput acceler practic set one caus slow worker term straggler caus fusion step master node stall greatli slow converg mani straggler mitig approach work complet node partial discard complet paper propos approach parallel synchron sgd exploit work complet worker central idea fix comput time worker combin distinct contribut worker provid converg analysi optim combin function numer result demonstr improv sever factor magnitud comparison exist method less
314,1810.02974,"
        Data centres that use consumer-grade disks drives and distributed peer-to-peer systems are unreliable environments to archive data without enough redundancy. Most redundancy schemes are not completely effective for providing high availability, durability and integrity in the long-term. We propose alpha entanglement codes, a mechanism that creates a virtual layer of highly interconnected storage devices to propagate redundant information across a large scale storage system. Our motivation is to design flexible and practical erasure codes with high fault-tolerance to improve data durability and availability even in catastrophic scenarios. By flexible and practical, we mean code settings that can be adapted to future requirements and practical implementations with reasonable trade-offs between security, resource usage and performance. The codes have three parameters. Alpha increases storage overhead linearly but increases the possible paths to recover data exponentially. Two other parameters increase fault-tolerance even further without the need of additional storage. As a result, an entangled storage system can provide high availability, durability and offer additional integrity: it is more difficult to modify data undetectably. We evaluate how several redundancy schemes perform in unreliable environments and show that alpha entanglement codes are flexible and practical codes. Remarkably, they excel at code locality, hence, they reduce repair costs and become less dependent on storage locations with poor availability. Our solution outperforms Reed-Solomon codes in many disaster recovery scenarios.
        △ Less
",data centr use consum grade disk drive distribut peer peer system unreli environ archiv data without enough redund redund scheme complet effect provid high avail durabl integr long term propos alpha entangl code mechan creat virtual layer highli interconnect storag devic propag redund inform across larg scale storag system motiv design flexibl practic erasur code high fault toler improv data durabl avail even catastroph scenario flexibl practic mean code set adapt futur requir practic implement reason trade off secur resourc usag perform code three paramet alpha increas storag overhead linearli increas possibl path recov data exponenti two paramet increas fault toler even without need addit storag result entangl storag system provid high avail durabl offer addit integr difficult modifi data undetect evalu sever redund scheme perform unreli environ show alpha entangl code flexibl practic code remark excel code local henc reduc repair cost becom less depend storag locat poor avail solut outperform reed solomon code mani disast recoveri scenario less
315,1810.02972,"
        Smartphone users and application behaviors add high pressure to apply smart techniques that stabilize the network capacity and consequently improve the end-user experience. The massive increase in smartphone penetration engenders signalling load, which exceeds the network capacity in terms of signaling. The signalling load leads to network congestion, degradation in the network KPIs. The classical way to tackle the signalling is by network expansion. However, this approach is not efficient in terms of capital expenditure (CAPEX) and also in terms of efficient utilization of the network resources. More specifically, the signaling domain becomes overloaded while the data domain are underutilized. In this paper, two UMTS air-interface features; Cell-PCH (paging channel) and enhanced fast dormancy (E-FD) are analyzed to mitigate the signalling load. Practical performance analysis is conducted based on results from commercial UMTS networks. The deployment of these features offers major improvement in network KPIs and significant relief in the signaling load. It is concluded that these two features in addition to several optimization techniques, discussed in this paper, provide solution to the smartphone signaling load and efficiently utilize the available network and spectrum resources while providing users with better always-on connectivity and improved battery consumption.
        △ Less
",smartphon user applic behavior add high pressur appli smart techniqu stabil network capac consequ improv end user experi massiv increas smartphon penetr engend signal load exce network capac term signal signal load lead network congest degrad network kpi classic way tackl signal network expans howev approach effici term capit expenditur capex also term effici util network resourc specif signal domain becom overload data domain underutil paper two umt air interfac featur cell pch page channel enhanc fast dormanc e fd analyz mitig signal load practic perform analysi conduct base result commerci umt network deploy featur offer major improv network kpi signific relief signal load conclud two featur addit sever optim techniqu discuss paper provid solut smartphon signal load effici util avail network spectrum resourc provid user better alway connect improv batteri consumpt less
316,1810.02970,"
        This paper covers the practical aspects of commercial long term evolution (LTE) network design and deployment. The end-to-end architecture of the LTE network and different deployment scenarios are presented. Moreover, the LTE coverage and link budget aspects are discussed in details. Theoretical and practical throughputs of LTE system are analyzed. In addition, capacity dimensioning of LTE system is explained in details and compared with the evolved high-speed packet access (HSPA+) system. Additionally, the quality of service (QoS) of the LTE system and the end-to-end implementation scenarios along with testing results are presented. Finally, the latency of the LTE system is analyzed and compared with the HSPA+ system. This paper can be used as a reference for best practices in LTE network design and deployment.
        △ Less
",paper cover practic aspect commerci long term evolut lte network design deploy end end architectur lte network differ deploy scenario present moreov lte coverag link budget aspect discuss detail theoret practic throughput lte system analyz addit capac dimens lte system explain detail compar evolv high speed packet access hspa system addit qualiti servic qo lte system end end implement scenario along test result present final latenc lte system analyz compar hspa system paper use refer best practic lte network design deploy less
317,1810.02968,"
        Voice over Long-Term Evolution (VoLTE) has been witnessing a rapid deployment by network carriers worldwide. During the phases of VoLTE deployments, carriers would typically face challenges in understanding the factors affecting the VoLTE performance and then optimizing it to meet or exceed the performance of the legacy circuit switched (CS) network (i.e., 2G/3G). The main challenge of VoLTE service quality is the LTE network optimization and the performance aspects of the service in different LTE deployment scenarios. In this paper, we present a detailed practical performance analysis of VoLTE based on commercially deployed 3GPP Release-10 LTE networks. The analysis evaluates VoLTE performance in terms of real-time transport protocol (RTP) error rate, RTP jitter and delays, block error rate (BLER) in different radio conditions and VoLTE voice quality in terms of mean opinion score (MOS). In addition, the paper evaluates key VoLTE features such as RObust Header Compression (ROHC) and transmission time interval (TTI) bundling. This paper provides guidelines for best practices of VoLTE deployment as well as practical performance evaluation based on field measurement data from commercial LTE networks.
        △ Less
",voic long term evolut volt wit rapid deploy network carrier worldwid phase volt deploy carrier would typic face challeng understand factor affect volt perform optim meet exceed perform legaci circuit switch cs network e g g main challeng volt servic qualiti lte network optim perform aspect servic differ lte deploy scenario paper present detail practic perform analysi volt base commerci deploy gpp releas lte network analysi evalu volt perform term real time transport protocol rtp error rate rtp jitter delay block error rate bler differ radio condit volt voic qualiti term mean opinion score mo addit paper evalu key volt featur robust header compress rohc transmiss time interv tti bundl paper provid guidelin best practic volt deploy well practic perform evalu base field measur data commerci lte network less
318,1810.02966,"
        At present, the state-of-the-art computational models across a range of sequential data processing tasks, including language modeling, are based on recurrent neural network architectures. This paper begins with the observation that most research on developing computational models capable of processing sequential data fails to explicitly analyze the long distance dependencies (LDDs) within the datasets the models process. In this context, in this paper, we make five research contributions. First, we argue that a key step in modeling sequential data is to understand the characteristics of the LDDs within the data. Second, we present a method to compute and analyze the LDD characteristics of any sequential dataset, and demonstrate this method on a number of sequential datasets that are frequently used for model benchmarking. Third, based on the analysis of the LDD characteristics within the benchmarking datasets, we observe that LDDs are far more complex than previously assumed, and depend on at least four factors: (i) the number of unique symbols in a dataset, (ii) size of the dataset, (iii) the number of interacting symbols within an LDD, and (iv) the distance between the interacting symbols. Fourth, we verify these factors by using synthetic datasets generated using Strictly k-Piecewise (SPk) languages. We then demonstrate how SPk languages can be used to generate benchmarking datasets with varying degrees of LDDs. The advantage of these synthesized datasets being that they enable the targeted testing of recurrent neural architectures. Finally, we demonstrate how understanding the characteristics of the LDDs in a dataset can inform better hyper-parameter selection for current state-of-the-art recurrent neural architectures and also aid in understanding them...
        △ Less
",present state art comput model across rang sequenti data process task includ languag model base recurr neural network architectur paper begin observ research develop comput model capabl process sequenti data fail explicitli analyz long distanc depend ldd within dataset model process context paper make five research contribut first argu key step model sequenti data understand characterist ldd within data second present method comput analyz ldd characterist sequenti dataset demonstr method number sequenti dataset frequent use model benchmark third base analysi ldd characterist within benchmark dataset observ ldd far complex previous assum depend least four factor number uniqu symbol dataset ii size dataset iii number interact symbol within ldd iv distanc interact symbol fourth verifi factor use synthet dataset gener use strictli k piecewis spk languag demonstr spk languag use gener benchmark dataset vari degre ldd advantag synthes dataset enabl target test recurr neural architectur final demonstr understand characterist ldd dataset inform better hyper paramet select current state art recurr neural architectur also aid understand less
319,1810.02964,"
        In this paper we break the protocol based on the Diffie-Hellman Decomposition problem and ElGamal Decomposition problem over the matrix ring $E_p^{(m)}$. Our attack terminates in a provable running time of $O(m^{10})$.
        △ Less
",paper break protocol base diffi hellman decomposit problem elgam decomposit problem matrix ring e p attack termin provabl run time less
320,1810.02959,"
        Higher-order connectivity patterns such as small induced sub-graphs called graphlets (network motifs) are vital to understand the important components (modules/functional units) governing the configuration and behavior of complex networks. Existing work in higher-order clustering has focused on simple homogeneous graphs with a single node/edge type. However, heterogeneous graphs consisting of nodes and edges of different types are seemingly ubiquitous in the real-world. In this work, we introduce the notion of typed-graphlet that explicitly captures the rich (typed) connectivity patterns in heterogeneous networks. Using typed-graphlets as a basis, we develop a general principled framework for higher-order clustering in heterogeneous networks. The framework provides mathematical guarantees on the optimality of the higher-order clustering obtained. The experiments demonstrate the effectiveness of the framework quantitatively for three important applications including (i) clustering, (ii) link prediction, and (iii) graph compression. In particular, the approach achieves a mean improvement of 43x over all methods and graphs for clustering while achieving a 18.7% and 20.8% improvement for link prediction and graph compression, respectively.
        △ Less
",higher order connect pattern small induc sub graph call graphlet network motif vital understand import compon modul function unit govern configur behavior complex network exist work higher order cluster focus simpl homogen graph singl node edg type howev heterogen graph consist node edg differ type seemingli ubiquit real world work introduc notion type graphlet explicitli captur rich type connect pattern heterogen network use type graphlet basi develop gener principl framework higher order cluster heterogen network framework provid mathemat guarante optim higher order cluster obtain experi demonstr effect framework quantit three import applic includ cluster ii link predict iii graph compress particular approach achiev mean improv x method graph cluster achiev improv link predict graph compress respect less
321,1810.02953,"
        We show that the shuffle $L \unicode{x29E2} F$ of a piecewise-testable language $L$ and a finite language $F$ is piecewise-testable. The proof relies on a classic but little-used automata-theoretic characterization of piecewise-testable languages. We also discuss some mild generalizations of the main result, and provide bounds on the complexity of $L \unicode{x29E2} F$.
        △ Less
",show shuffl l unicod x e f piecewis testabl languag l finit languag f piecewis testabl proof reli classic littl use automata theoret character piecewis testabl languag also discuss mild gener main result provid bound complex l unicod x e f less
322,1810.02950,"
        In many domains, there is significant interest in capturing novel relationships between time series that represent activities recorded at different nodes of a highly complex system. In this paper, we introduce multipoles, a novel class of linear relationships between more than two time series. A multipole is a set of time series that have strong linear dependence among themselves, with the requirement that each time series makes a significant contribution to the linear dependence. We demonstrate that most interesting multipoles can be identified as cliques of negative correlations in a correlation network. Such cliques are typically rare in a real-world correlation network, which allows us to find almost all multipoles efficiently using a clique-enumeration approach. Using our proposed framework, we demonstrate the utility of multipoles in discovering new physical phenomena in two scientific domains: climate science and neuroscience. In particular, we discovered several multipole relationships that are reproducible in multiple other independent datasets and lead to novel domain insights.
        △ Less
",mani domain signific interest captur novel relationship time seri repres activ record differ node highli complex system paper introduc multipol novel class linear relationship two time seri multipol set time seri strong linear depend among requir time seri make signific contribut linear depend demonstr interest multipol identifi cliqu neg correl correl network cliqu typic rare real world correl network allow us find almost multipol effici use cliqu enumer approach use propos framework demonstr util multipol discov new physic phenomena two scientif domain climat scienc neurosci particular discov sever multipol relationship reproduc multipl independ dataset lead novel domain insight less
323,1810.02938,"
        Learning a matching function between two text sequences is a long standing problem in NLP research. This task enables many potential applications such as question answering and paraphrase identification. This paper proposes Co-Stack Residual Affinity Networks (CSRAN), a new and universal neural architecture for this problem. CSRAN is a deep architecture, involving stacked (multi-layered) recurrent encoders. Stacked/Deep architectures are traditionally difficult to train, due to the inherent weaknesses such as difficulty with feature propagation and vanishing gradients. CSRAN incorporates two novel components to take advantage of the stacked architecture. Firstly, it introduces a new bidirectional alignment mechanism that learns affinity weights by fusing sequence pairs across stacked hierarchies. Secondly, it leverages a multi-level attention refinement component between stacked recurrent layers. The key intuition is that, by leveraging information across all network hierarchies, we can not only improve gradient flow but also improve overall performance. We conduct extensive experiments on six well-studied text sequence matching datasets, achieving state-of-the-art performance on all.
        △ Less
",learn match function two text sequenc long stand problem nlp research task enabl mani potenti applic question answer paraphras identif paper propos co stack residu affin network csran new univers neural architectur problem csran deep architectur involv stack multi layer recurr encod stack deep architectur tradit difficult train due inher weak difficulti featur propag vanish gradient csran incorpor two novel compon take advantag stack architectur firstli introduc new bidirect align mechan learn affin weight fuse sequenc pair across stack hierarchi secondli leverag multi level attent refin compon stack recurr layer key intuit leverag inform across network hierarchi improv gradient flow also improv overal perform conduct extens experi six well studi text sequenc match dataset achiev state art perform less
324,1810.02937,"
        Decentralization, which has backed the hyper growth of many blockchains, comes at the cost of scalability. To understand this fundamental limitation, this paper proposes a quantitative measure of blockchain decentralization, and discusses its implications to various trust models and consensus algorithms. Further, we identify the major challenges in blockchain decentralization. Our key findings are that true decentralization is hard to achieve due to the skewed mining power and that a fully decentralized blockchain inherently limits scalability as it incurs a throughput upper bound and prevents scaling smart contract execution. To address these challenges, we outline three research directions to explore the trade-offs between decentralization and scalability.
        △ Less
",decentr back hyper growth mani blockchain come cost scalabl understand fundament limit paper propos quantit measur blockchain decentr discuss implic variou trust model consensu algorithm identifi major challeng blockchain decentr key find true decentr hard achiev due skew mine power fulli decentr blockchain inher limit scalabl incur throughput upper bound prevent scale smart contract execut address challeng outlin three research direct explor trade off decentr scalabl less
325,1810.02936,"
        Person re-identification (reID) is an important task that requires to retrieve a person's images from an image dataset, given one image of the person of interest. For learning robust person features, the pose variation of person images is one of the key challenges. Existing works targeting the problem either perform human alignment, or learn human-region-based representations. Extra pose information and computational cost is generally required for inference. To solve this issue, a Feature Distilling Generative Adversarial Network (FD-GAN) is proposed for learning identity-related and pose-unrelated representations. It is a novel framework based on a Siamese structure with multiple novel discriminators on human poses and identities. In addition to the discriminators, a novel same-pose loss is also integrated, which requires appearance of a same person's generated images to be similar. After learning pose-unrelated person features with pose guidance, no auxiliary pose information and additional computational cost is required during testing. Our proposed FD-GAN achieves state-of-the-art performance on three person reID datasets, which demonstrates that the effectiveness and robust feature distilling capability of the proposed FD-GAN.
        △ Less
",person identif reid import task requir retriev person imag imag dataset given one imag person interest learn robust person featur pose variat person imag one key challeng exist work target problem either perform human align learn human region base represent extra pose inform comput cost gener requir infer solv issu featur distil gener adversari network fd gan propos learn ident relat pose unrel represent novel framework base siames structur multipl novel discrimin human pose ident addit discrimin novel pose loss also integr requir appear person gener imag similar learn pose unrel person featur pose guidanc auxiliari pose inform addit comput cost requir test propos fd gan achiev state art perform three person reid dataset demonstr effect robust featur distil capabl propos fd gan less
326,1810.02935,"
        Recent years, many applications have been driven advances by the use of Machine Learning (ML). Nowadays, it is common to see industrial-strength machine learning jobs that involve millions of model parameters, terabytes of training data, and weeks of training. Good efficiency, i.e., fast completion time of running a specific ML job, therefore, is a key feature of a successful ML system. While the completion time of a long- running ML job is determined by the time required to reach model convergence, practically that is also largely influenced by the values of various system settings. In this paper, we contribute techniques towards building self-tuning parameter servers. Parameter Server (PS) is a popular system architecture for large-scale machine learning systems; and by self-tuning we mean while a long-running ML job is iteratively training the expert-suggested model, the system is also iteratively learning which system setting is more efficient for that job and applies it online. While our techniques are general enough to various PS- style ML systems, we have prototyped our techniques on top of TensorFlow. Experiments show that our techniques can reduce the completion times of a variety of long-running TensorFlow jobs from 1.4x to 18x.
        △ Less
",recent year mani applic driven advanc use machin learn ml nowaday common see industri strength machin learn job involv million model paramet terabyt train data week train good effici e fast complet time run specif ml job therefor key featur success ml system complet time long run ml job determin time requir reach model converg practic also larg influenc valu variou system set paper contribut techniqu toward build self tune paramet server paramet server ps popular system architectur larg scale machin learn system self tune mean long run ml job iter train expert suggest model system also iter learn system set effici job appli onlin techniqu gener enough variou ps style ml system prototyp techniqu top tensorflow experi show techniqu reduc complet time varieti long run tensorflow job x x less
327,1810.02930,"
        Mechanism design is studied for aggregating renewable power producers (RPPs) in a two-settlement power market. Employing an indirect mechanism design framework, a payoff allocation mechanism (PAM) is derived from the competitive equilibrium (CE) of a specially formulated market with transferrable payoff. Given the designed mechanism, the strategic behaviors of the participating RPPs entail a non-cooperative game: It is proven that a unique pure Nash equilibrium (NE) exists among the RPPs, for which a closed-form expression is found. Moreover, it is proven that the designed mechanism achieves a number of key desirable properties at the NE: these include efficiency (i.e., an ideal ""Price of Anarchy"" of one), stability (i.e., ""in the core"" from a coalitional game theoretic perspective), and no collusion. In addition, it is shown that a set of desirable ""ex-post"" properties are also achieved by the designed mechanism. Extensive simulations are conducted and corroborate the theoretical results.
        △ Less
",mechan design studi aggreg renew power produc rpp two settlement power market employ indirect mechan design framework payoff alloc mechan pam deriv competit equilibrium ce special formul market transferr payoff given design mechan strateg behavior particip rpp entail non cooper game proven uniqu pure nash equilibrium ne exist among rpp close form express found moreov proven design mechan achiev number key desir properti ne includ effici e ideal price anarchi one stabil e core coalit game theoret perspect collus addit shown set desir ex post properti also achiev design mechan extens simul conduct corrobor theoret result less
328,1810.02929,"
        This paper discusses system consequence, a central idea in the project to lift the theory of information flow to the abstract level of universal logic and the theory of institutions. The theory of information flow is a theory of distributed logic. The theory of institutions is abstract model theory. A system is a collection of interconnected parts, where the whole may have properties that cannot be known from an analysis of the constituent parts in isolation. In an information system, the parts represent information resources and the interconnections represent constraints between the parts. System consequence, which is the extension of the consequence operator from theories to systems, models the available regularities represented by an information system as a whole. System consequence (without part-to-part constraints) is defined for a specific logical system (institution) in the theory of information flow. This paper generalizes the idea of system consequence to arbitrary logical systems.
        △ Less
",paper discuss system consequ central idea project lift theori inform flow abstract level univers logic theori institut theori inform flow theori distribut logic theori institut abstract model theori system collect interconnect part whole may properti cannot known analysi constitu part isol inform system part repres inform resourc interconnect repres constraint part system consequ extens consequ oper theori system model avail regular repres inform system whole system consequ without part part constraint defin specif logic system institut theori inform flow paper gener idea system consequ arbitrari logic system less
329,1810.02927,"
        Goal-oriented learning has become a core concept in reinforcement learning (RL), extending the reward signal as a sole way to define tasks. However, as parameterizing value functions with goals increases the learning complexity, efficiently reusing past experience to update estimates towards several goals at once becomes desirable but usually requires independent updates per goal. Considering that a significant number of RL environments can support spatial coordinates as goals, such as on-screen location of the character in ATARI or SNES games, we propose a novel goal-oriented agent called Q-map that utilizes an autoencoder-like neural network to predict the minimum number of steps towards each coordinate in a single forward pass. This architecture is similar to Horde with parameter sharing and allows the agent to discover correlations between visual patterns and navigation. For example learning how to use a ladder in a game could be transferred to other ladders later. We show how this network can be efficiently trained with a 3D variant of Q-learning to update the estimates towards all goals at once. While the Q-map agent could be used for a wide range of applications, we propose a novel exploration mechanism in place of epsilon-greedy that relies on goal selection at a desired distance followed by several steps taken towards it, allowing long and coherent exploratory steps in the environment. We demonstrate the accuracy and generalization qualities of the Q-map agent on a grid-world environment and then demonstrate the efficiency of the proposed exploration mechanism on the notoriously difficult Montezuma's Revenge and Super Mario All-Stars games.
        △ Less
",goal orient learn becom core concept reinforc learn rl extend reward signal sole way defin task howev parameter valu function goal increas learn complex effici reus past experi updat estim toward sever goal becom desir usual requir independ updat per goal consid signific number rl environ support spatial coordin goal screen locat charact atari sne game propos novel goal orient agent call q map util autoencod like neural network predict minimum number step toward coordin singl forward pass architectur similar hord paramet share allow agent discov correl visual pattern navig exampl learn use ladder game could transfer ladder later show network effici train variant q learn updat estim toward goal q map agent could use wide rang applic propos novel explor mechan place epsilon greedi reli goal select desir distanc follow sever step taken toward allow long coher exploratori step environ demonstr accuraci gener qualiti q map agent grid world environ demonstr effici propos explor mechan notori difficult montezuma reveng super mario star game less
330,1810.02920,"
        A novel framework is presented that combines Mean Field Game (MFG) theory and Hybrid Optimal Control (HOC) theory to obtain a unique $ε$-Nash equilibrium for a non-cooperative game with stopping times. We consider the case where there exists one major agent with a significant influence on the system together with a large number of minor agents constituting two subpopulations, each with individually asymptotically negligible effect on the whole system. Each agent has stochastic linear dynamics with quadratic costs, and the agents are coupled in their dynamics by the average state of minor agents (i.e. the empirical mean field). The hybrid feature enters via the indexing by discrete states: (i) the switching of the major agent between alternative dynamics or (ii) the termination of the agents' trajectories in one or both of the subpopulations of minor agents. Optimal switchings and stopping time strategies together with best response control actions for, respectively, the major agent and all minor agents are established with respect to their individual cost criteria by an application of LQG HOC theory.
        △ Less
",novel framework present combin mean field game mfg theori hybrid optim control hoc theori obtain uniqu nash equilibrium non cooper game stop time consid case exist one major agent signific influenc system togeth larg number minor agent constitut two subpopul individu asymptot neglig effect whole system agent stochast linear dynam quadrat cost agent coupl dynam averag state minor agent e empir mean field hybrid featur enter via index discret state switch major agent altern dynam ii termin agent trajectori one subpopul minor agent optim switch stop time strategi togeth best respons control action respect major agent minor agent establish respect individu cost criteria applic lqg hoc theori less
331,1810.02919,"
        Efforts are underway at UT Austin to build autonomous robot systems that address the challenges of long-term deployments in office environments and of the more prescribed domestic service tasks of the RoboCup@Home competition. We discuss the contrasts and synergies of these efforts, highlighting how our work to build a RoboCup@Home Domestic Standard Platform League entry led us to identify an integrated software architecture that could support both projects. Further, naturalistic deployments of our office robot platform as part of the Building-Wide Intelligence project have led us to identify and research new problems in a traditional laboratory setting.
        △ Less
",effort underway ut austin build autonom robot system address challeng long term deploy offic environ prescrib domest servic task robocup home competit discuss contrast synergi effort highlight work build robocup home domest standard platform leagu entri led us identifi integr softwar architectur could support project naturalist deploy offic robot platform part build wide intellig project led us identifi research new problem tradit laboratori set less
332,1810.02912,"
        Reinforcement learning in multi-agent scenarios is important for real-world applications but presents challenges beyond those seen in single-agent settings. We present an actor-critic algorithm that trains decentralized policies in multi-agent settings, using centrally computed critics that share an attention mechanism which selects relevant information for each agent at every timestep. This attention mechanism enables more effective and scalable learning in complex multi-agent environments, when compared to recent approaches. Our approach is applicable not only to cooperative settings with shared rewards, but also individualized reward settings, including adversarial settings, and it makes no assumptions about the action spaces of the agents. As such, it is flexible enough to be applied to most multi-agent learning problems.
        △ Less
",reinforc learn multi agent scenario import real world applic present challeng beyond seen singl agent set present actor critic algorithm train decentr polici multi agent set use central comput critic share attent mechan select relev inform agent everi timestep attent mechan enabl effect scalabl learn complex multi agent environ compar recent approach approach applic cooper set share reward also individu reward set includ adversari set make assumpt action space agent flexibl enough appli multi agent learn problem less
333,1810.02911,"
        We propose a software platform that integrates methods and tools for multi-objective parameter auto- tuning in tissue image segmentation workflows. The goal of our work is to provide an approach for improving the accuracy of nucleus/cell segmentation pipelines by tuning their input parameters. The shape, size and texture features of nuclei in tissue are important biomarkers for disease prognosis, and accurate computation of these features depends on accurate delineation of boundaries of nuclei. Input parameters in many nucleus segmentation workflows affect segmentation accuracy and have to be tuned for optimal performance. This is a time-consuming and computationally expensive process; automating this step facilitates more robust image segmentation workflows and enables more efficient application of image analysis in large image datasets. Our software platform adjusts the parameters of a nuclear segmentation algorithm to maximize the quality of image segmentation results while minimizing the execution time. It implements several optimization methods to search the parameter space efficiently. In addition, the methodology is developed to execute on high performance computing systems to reduce the execution time of the parameter tuning phase. Our results using three real-world image segmentation workflows demonstrate that the proposed solution is able to (1) search a small fraction (about 100 points) of the parameter space, which contains billions to trillions of points, and improve the quality of segmentation output by 1.20x, 1.29x, and 1.29x, on average; (2) decrease the execution time of a segmentation workflow by up to 11.79x while improving output quality; and (3) effectively use parallel systems to accelerate parameter tuning and segmentation phases.
        △ Less
",propos softwar platform integr method tool multi object paramet auto tune tissu imag segment workflow goal work provid approach improv accuraci nucleu cell segment pipelin tune input paramet shape size textur featur nuclei tissu import biomark diseas prognosi accur comput featur depend accur delin boundari nuclei input paramet mani nucleu segment workflow affect segment accuraci tune optim perform time consum comput expens process autom step facilit robust imag segment workflow enabl effici applic imag analysi larg imag dataset softwar platform adjust paramet nuclear segment algorithm maxim qualiti imag segment result minim execut time implement sever optim method search paramet space effici addit methodolog develop execut high perform comput system reduc execut time paramet tune phase result use three real world imag segment workflow demonstr propos solut abl search small fraction point paramet space contain billion trillion point improv qualiti segment output x x x averag decreas execut time segment workflow x improv output qualiti effect use parallel system acceler paramet tune segment phase less
334,1810.02907,"
        Text mining and analytics software has become popular, but little attention has been paid to the software architectures of such systems. Often they are built from scratch using special-purpose software and data structures, which increases their cost and complexity. This demo paper describes Sifaka, a new open-source text mining application constructed above a standard search engine index using existing application programmer interface (API) calls. Indexing integrates popular annotation software libraries to augment the full-text index with noun phrase and named-entities; n-grams are also provided. Sifaka enables a person to quickly explore and analyze large text collections using search, frequency analysis, and co-occurrence analysis; and import existing document labels or interactively construct document sets that are positive or negative examples of new concepts, perform feature selection, and export feature vectors compatible with popular machine learning software. Sifaka demonstrates that search engines are good platforms for text mining applications while also making common IR text mining capabilities accessible to researchers in disciplines where programming skills are less common.
        △ Less
",text mine analyt softwar becom popular littl attent paid softwar architectur system often built scratch use special purpos softwar data structur increas cost complex demo paper describ sifaka new open sourc text mine applic construct standard search engin index use exist applic programm interfac api call index integr popular annot softwar librari augment full text index noun phrase name entiti n gram also provid sifaka enabl person quickli explor analyz larg text collect use search frequenc analysi co occurr analysi import exist document label interact construct document set posit neg exampl new concept perform featur select export featur vector compat popular machin learn softwar sifaka demonstr search engin good platform text mine applic also make common ir text mine capabl access research disciplin program skill less common less
335,1810.02904,"
        We present a new approach to testing file-system crash consistency: bounded black-box crash testing (B3). B3 tests the file system in a black-box manner using workloads of file-system operations. Since the space of possible workloads is infinite, B3 bounds this space based on parameters such as the number of file-system operations or which operations to include, and exhaustively generates workloads within this bounded space. Each workload is tested on the target file system by simulating power-loss crashes while the workload is being executed, and checking if the file system recovers to a correct state after each crash. B3 builds upon insights derived from our study of crash-consistency bugs reported in Linux file systems in the last five years. We observed that most reported bugs can be reproduced using small workloads of three or fewer file-system operations on a newly-created file system, and that all reported bugs result from crashes after fsync() related system calls. We build two tools, CrashMonkey and ACE, to demonstrate the effectiveness of this approach. Our tools are able to find 24 out of the 26 crash-consistency bugs reported in the last five years. Our tools also revealed 10 new crash-consistency bugs in widely-used, mature Linux file systems, seven of which existed in the kernel since 2014. Our tools also found a crash-consistency bug in a verified file system, FSCQ. The new bugs result in severe consequences like broken rename atomicity and loss of persisted files.
        △ Less
",present new approach test file system crash consist bound black box crash test b b test file system black box manner use workload file system oper sinc space possibl workload infinit b bound space base paramet number file system oper oper includ exhaust gener workload within bound space workload test target file system simul power loss crash workload execut check file system recov correct state crash b build upon insight deriv studi crash consist bug report linux file system last five year observ report bug reproduc use small workload three fewer file system oper newli creat file system report bug result crash fsync relat system call build two tool crashmonkey ace demonstr effect approach tool abl find crash consist bug report last five year tool also reveal new crash consist bug wide use matur linux file system seven exist kernel sinc tool also found crash consist bug verifi file system fscq new bug result sever consequ like broken renam atom loss persist file less
336,1810.02899,"
        Cloud operators require real-time identification of Heavy Hitters (HH) and Hierarchical Heavy Hitters (HHH) for applications such as load balancing, traffic engineering, and attack mitigation. However, existing techniques are slow in detecting new heavy hitters.
  In this paper, we make the case for identifying heavy hitters through \textit{sliding windows}. Sliding windows detect heavy hitters quicker and more accurately than current methods, but to date had no practical algorithms. Accordingly, we introduce, design and analyze the \textit{Memento} family of sliding window algorithms for the HH and HHH problems in the single-device and network-wide settings. Using extensive evaluations, we show that our single-device solutions attain similar accuracy and are by up to $273\times$ faster than existing window-based techniques. Furthermore, we exemplify our network-wide HHH detection capabilities on a realistic testbed. To that end, we implemented Memento as an open-source extension to the popular HAProxy cloud load-balancer. In our evaluations, using an HTTP flood by 50 subnets, our network-wide approach detected the new subnets faster, and reduced the number of undetected flood requests by up to $37\times$ compared to the alternatives.
        △ Less
",cloud oper requir real time identif heavi hitter hh hierarch heavi hitter hhh applic load balanc traffic engin attack mitig howev exist techniqu slow detect new heavi hitter paper make case identifi heavi hitter textit slide window slide window detect heavi hitter quicker accur current method date practic algorithm accordingli introduc design analyz textit memento famili slide window algorithm hh hhh problem singl devic network wide set use extens evalu show singl devic solut attain similar accuraci time faster exist window base techniqu furthermor exemplifi network wide hhh detect capabl realist testb end implement memento open sourc extens popular haproxi cloud load balanc evalu use http flood subnet network wide approach detect new subnet faster reduc number undetect flood request time compar altern less
337,1810.02898,"
        This note studies (practical) asymptotic stability of nonlinear networked control systems whose protocols are not necessarily uniformly globally exponentially stable. In particular, we propose a Lyapunov-based approach to establish (practical) asymptotic stability of the networked control systems. Considering so-called modified Round Robin and Try-Once-Discard protocols, which are only uniformly globally asymptotically stable, we explicitly construct Lyapunov functions for these two protocols, which fit our proposed setting. In order to optimize the usage of communication resource, we exploit the following transmission policy: wait for a certain minimum amount of time after the last sampling instant and then check a state-dependent criterion. When the latter condition is violated, a transmission occurs. In that way, the existence of the minimum amount of time between two consecutive transmission is established and so-called Zeno phenomenon, therefore, is avoided. Finally, illustrative examples are given to verify the effectiveness of our results.
        △ Less
",note studi practic asymptot stabil nonlinear network control system whose protocol necessarili uniformli global exponenti stabl particular propos lyapunov base approach establish practic asymptot stabil network control system consid call modifi round robin tri discard protocol uniformli global asymptot stabl explicitli construct lyapunov function two protocol fit propos set order optim usag commun resourc exploit follow transmiss polici wait certain minimum amount time last sampl instant check state depend criterion latter condit violat transmiss occur way exist minimum amount time two consecut transmiss establish call zeno phenomenon therefor avoid final illustr exampl given verifi effect result less
338,1810.02897,"
        Many distance-based algorithms exhibit bias towards dense clusters in inhomogeneous datasets (i.e., those which contain clusters in both dense and sparse regions of the space). For example, density-based clustering algorithms tend to join neighbouring dense clusters together into a single group in the presence of a sparse cluster; while distance-based anomaly detectors exhibit difficulty in detecting local anomalies which are close to a dense cluster in datasets also containing sparse clusters. In this paper, we propose the CDF Transform-Shift (CDF-TS) algorithm which is based on a multi-dimensional Cumulative Distribution Function (CDF) transformation. It effectively converts a dataset with clusters of inhomogeneous density to one with clusters of homogeneous density, i.e., the data distribution is converted to one in which all locally low/high-density locations become globally low/high-density locations. Thus, after performing the proposed Transform-Shift, a single global density threshold can be used to separate the data into clusters and their surrounding noise points. Our empirical evaluations show that CDF-TS overcomes the shortcomings of existing density-based clustering and distance-based anomaly detection algorithms and significantly improves their performance.
        △ Less
",mani distanc base algorithm exhibit bia toward dens cluster inhomogen dataset e contain cluster dens spars region space exampl densiti base cluster algorithm tend join neighbour dens cluster togeth singl group presenc spars cluster distanc base anomali detector exhibit difficulti detect local anomali close dens cluster dataset also contain spars cluster paper propos cdf transform shift cdf ts algorithm base multi dimension cumul distribut function cdf transform effect convert dataset cluster inhomogen densiti one cluster homogen densiti e data distribut convert one local low high densiti locat becom global low high densiti locat thu perform propos transform shift singl global densiti threshold use separ data cluster surround nois point empir evalu show cdf ts overcom shortcom exist densiti base cluster distanc base anomali detect algorithm significantli improv perform less
339,1810.02895,"
        Consumer genetic testing has become immensely popular in recent years and has lead to the creation of large scale genetic databases containing millions of dense autosomal genotype profiles. One of the most used features offered by genetic databases is the ability to find distant relatives using a technique called relative matching (or DNA matching). Recently, novel uses of relative matching were discovered that combined matching results with genealogical information to solve criminal cold cases. New estimates suggest that relative matching, combined with simple demographic information, could be used to re-identify a significant percentage of US Caucasian individuals. In this work we attempt to systematize computer security and privacy risks from relative matching and describe new security problems that can occur if an attacker uploads manipulated or forged genetic profiles. For example, forged profiles can be used by criminals to misdirect investigations, con-artists to defraud victims, or political operatives to blackmail opponents. We discuss solutions to mitigate these threats, including existing proposals to use digital signatures, and encourage the consumer genetics community to consider the broader security implications of relative matching now that it is becoming so prominent.
        △ Less
",consum genet test becom immens popular recent year lead creation larg scale genet databas contain million dens autosom genotyp profil one use featur offer genet databas abil find distant rel use techniqu call rel match dna match recent novel use rel match discov combin match result genealog inform solv crimin cold case new estim suggest rel match combin simpl demograph inform could use identifi signific percentag us caucasian individu work attempt systemat comput secur privaci risk rel match describ new secur problem occur attack upload manipul forg genet profil exampl forg profil use crimin misdirect investig con artist defraud victim polit oper blackmail oppon discuss solut mitig threat includ exist propos use digit signatur encourag consum genet commun consid broader secur implic rel match becom promin less
340,1810.02891,"
        Reading comprehension tasks test the ability of models to process long-term context and remember salient information. Recent work has shown that relatively simple neural methods such as the Attention Sum-Reader can perform well on these tasks; however, these systems still significantly trail human performance. Analysis suggests that many of the remaining hard instances are related to the inability to track entity-references throughout documents. This work focuses on these hard entity tracking cases with two extensions: (1) additional entity features, and (2) training with a multi-task tracking objective. We show that these simple modifications improve performance both independently and in combination, and we outperform the previous state of the art on the LAMBADA dataset, particularly on difficult entity examples.
        △ Less
",read comprehens task test abil model process long term context rememb salient inform recent work shown rel simpl neural method attent sum reader perform well task howev system still significantli trail human perform analysi suggest mani remain hard instanc relat inabl track entiti refer throughout document work focus hard entiti track case two extens addit entiti featur train multi task track object show simpl modif improv perform independ combin outperform previou state art lambada dataset particularli difficult entiti exampl less
341,1810.02890,"
        Imitation learning has proven to be useful for many real-world problems, but approaches such as behavioral cloning suffer from data mismatch and compounding error issues. One attempt to address these limitations is the DAgger algorithm, which uses the state distribution induced by the novice to sample corrective actions from the expert. Such sampling schemes, however, require the expert to provide action labels without being fully in control of the system. This can decrease safety and, when using humans as experts, is likely to degrade the quality of the collected labels due to perceived actuator lag. In this work, we propose HG-DAgger, a variant of DAgger that is more suitable for interactive imitation learning from human experts in real-world systems. In addition to training a novice policy, HG-DAgger also learns a safety threshold for a model-uncertainty-based risk metric that can be used to predict the performance of the fully trained novice in different regions of the state space. We evaluate our method on both a simulated and real-world autonomous driving task, and demonstrate improved performance over both DAgger and behavioral cloning.
        △ Less
",imit learn proven use mani real world problem approach behavior clone suffer data mismatch compound error issu one attempt address limit dagger algorithm use state distribut induc novic sampl correct action expert sampl scheme howev requir expert provid action label without fulli control system decreas safeti use human expert like degrad qualiti collect label due perceiv actuat lag work propos hg dagger variant dagger suitabl interact imit learn human expert real world system addit train novic polici hg dagger also learn safeti threshold model uncertainti base risk metric use predict perform fulli train novic differ region state space evalu method simul real world autonom drive task demonstr improv perform dagger behavior clone less
342,1810.02889,"
        We present a framework for generating natural language description from structured data such as tables. Motivated by the need to approach this problem in a manner that is scalable and easily adaptable to newer domains, unlike existing related systems, our system does not require parallel data; it rather relies on monolingual corpora and basic NLP tools which are easily accessible. The system employs a 3-staged pipeline that: (i) converts entries in the structured data to canonical form, (ii) generates simple sentences for each atomic entry in the canonicalized representation, and (iii) combines the sentences to produce a coherent, fluent and adequate paragraph description through sentence compounding and co-reference replacement modules. Experiments on a benchmark mixed-domain dataset curated for paragraph description from tables reveals the superiority of our system over existing data-to-text approaches. We also demonstrate the robustness of our system in accepting other data types such as Knowledge-Graphs and Key-Value dictionaries.
        △ Less
",present framework gener natur languag descript structur data tabl motiv need approach problem manner scalabl easili adapt newer domain unlik exist relat system system requir parallel data rather reli monolingu corpora basic nlp tool easili access system employ stage pipelin convert entri structur data canon form ii gener simpl sentenc atom entri canonic represent iii combin sentenc produc coher fluent adequ paragraph descript sentenc compound co refer replac modul experi benchmark mix domain dataset curat paragraph descript tabl reveal superior system exist data text approach also demonstr robust system accept data type knowledg graph key valu dictionari less
343,1810.02880,"
        In this paper, we introduce a novel framework for combining scientific knowledge within physics-based models and recurrent neural networks to advance scientific discovery in many dynamical systems. We will first describe the use of outputs from physics-based models in learning a hybrid-physics-data model. Then, we further incorporate physical knowledge in real-world dynamical systems as additional constraints for training recurrent neural networks. We will apply this approach on modeling lake temperature and quality where we take into account the physical constraints along both the depth dimension and time dimension. By using scientific knowledge to guide the construction and learning the data-driven model, we demonstrate that this method can achieve better prediction accuracy as well as scientific consistency of results.
        △ Less
",paper introduc novel framework combin scientif knowledg within physic base model recurr neural network advanc scientif discoveri mani dynam system first describ use output physic base model learn hybrid physic data model incorpor physic knowledg real world dynam system addit constraint train recurr neural network appli approach model lake temperatur qualiti take account physic constraint along depth dimens time dimens use scientif knowledg guid construct learn data driven model demonstr method achiev better predict accuraci well scientif consist result less
344,1810.02869,"
        This work is done as part of a master's thesis project. The goal is to integrate two or more ontologies (of the same or close domains) in a new consistent and coherent OWL ontology to insure semantic interoperability between them. To do this, we have chosen to create a bridge ontology that includes all source ontologies and their bridging axioms in a customized way. In addition, we introduced a new criterion for obtaining an ontology of better quality (having the minimum of semantic/logical conflicts). We have also proposed new terminology and definitions that clarify the unclear and misplaced ""integration"" and ""merging"" notions that are randomly used in state-of-the-art works. Finally, we tested and evaluated our OIA2R tool using ontologies and reference alignments of the OAEI campaign. It turned out that it is generic, efficient and powerful enough.
        △ Less
",work done part master thesi project goal integr two ontolog close domain new consist coher owl ontolog insur semant interoper chosen creat bridg ontolog includ sourc ontolog bridg axiom custom way addit introduc new criterion obtain ontolog better qualiti minimum semant logic conflict also propos new terminolog definit clarifi unclear misplac integr merg notion randomli use state art work final test evalu oia r tool use ontolog refer align oaei campaign turn gener effici power enough less
345,1810.02862,"
        Haze and smog are among the most common environmental factors impacting image quality and, therefore, image analysis. This paper proposes an end-to-end generative method for image dehazing. It is based on designing a fully convolutional neural network to recognize haze structures in input images and restore clear, haze-free images. The proposed method is agnostic in the sense that it does not explore the atmosphere scattering model. Somewhat surprisingly, it achieves superior performance relative to all existing state-of-the-art methods for image dehazing even on SOTS outdoor images, which are synthesized using the atmosphere scattering model.
        △ Less
",haze smog among common environment factor impact imag qualiti therefor imag analysi paper propos end end gener method imag dehaz base design fulli convolut neural network recogn haze structur input imag restor clear haze free imag propos method agnost sens explor atmospher scatter model somewhat surprisingli achiev superior perform rel exist state art method imag dehaz even sot outdoor imag synthes use atmospher scatter model less
346,1810.02854,"
        We show that discrete distributions on the $d$-dimensional non-negative integer lattice can be approximated arbitrarily well via the marginals of stationary distributions for various classes of stochastic chemical reaction networks. We begin by providing a class of detailed balanced networks and prove that they can approximate any discrete distribution to any desired accuracy. However, these detailed balanced constructions rely on the ability to initialize a system precisely, and are therefore susceptible to perturbations in the initial conditions. We therefore provide another construction based on the ability to approximate point mass distributions and prove that this construction is capable of approximating arbitrary discrete distributions for any choice of initial condition. In particular, the developed models are ergodic, so their limit distributions are robust to a finite number of perturbations over time in the counts of molecules.
        △ Less
",show discret distribut dimension non neg integ lattic approxim arbitrarili well via margin stationari distribut variou class stochast chemic reaction network begin provid class detail balanc network prove approxim discret distribut desir accuraci howev detail balanc construct reli abil initi system precis therefor suscept perturb initi condit therefor provid anoth construct base abil approxim point mass distribut prove construct capabl approxim arbitrari discret distribut choic initi condit particular develop model ergod limit distribut robust finit number perturb time count molecul less
347,1810.02851,"
        Auto-encoders compress input data into a latent-space representation and reconstruct the original data from the representation. This latent representation is not easily interpreted by humans. In this paper, we propose training an auto-encoder that encodes input text into human-readable sentences, and unpaired abstractive summarization is thereby achieved. The auto-encoder is composed of a generator and a reconstructor. The generator encodes the input text into a shorter word sequence, and the reconstructor recovers the generator input from the generator output. To make the generator output human-readable, a discriminator restricts the output of the generator to resemble human-written sentences. By taking the generator output as the summary of the input text, abstractive summarization is achieved without document-summary pairs as training data. Promising results are shown on both English and Chinese corpora.
        △ Less
",auto encod compress input data latent space represent reconstruct origin data represent latent represent easili interpret human paper propos train auto encod encod input text human readabl sentenc unpair abstract summar therebi achiev auto encod compos gener reconstructor gener encod input text shorter word sequenc reconstructor recov gener input gener output make gener output human readabl discrimin restrict output gener resembl human written sentenc take gener output summari input text abstract summar achiev without document summari pair train data promis result shown english chines corpora less
348,1810.02848,"
        In this paper, we study fault-tolerant distributed consensus in wireless systems. In more detail, we produce two new randomized algorithms that solve this problem in the abstract MAC layer model, which captures the basic interface and communication guarantees provided by most wireless MAC layers. Our algorithms work for any number of failures, require no advance knowledge of the network participants or network size, and guarantee termination with high probability after a number of broadcasts that are polynomial in the network size. Our first algorithm satisfies the standard agreement property, while our second trades a faster termination guarantee in exchange for a looser agreement property in which most nodes agree on the same value. These are the first known fault-tolerant consensus algorithms for this model. In addition to our main upper bound results, we explore the gap between the abstract MAC layer and the standard asynchronous message passing model by proving fault-tolerant consensus is impossible in the latter in the absence of information regarding the network participants, even if we assume no faults, allow randomized solutions, and provide the algorithm a constant-factor approximation of the network size.
        △ Less
",paper studi fault toler distribut consensu wireless system detail produc two new random algorithm solv problem abstract mac layer model captur basic interfac commun guarante provid wireless mac layer algorithm work number failur requir advanc knowledg network particip network size guarante termin high probabl number broadcast polynomi network size first algorithm satisfi standard agreement properti second trade faster termin guarante exchang looser agreement properti node agre valu first known fault toler consensu algorithm model addit main upper bound result explor gap abstract mac layer standard asynchron messag pass model prove fault toler consensu imposs latter absenc inform regard network particip even assum fault allow random solut provid algorithm constant factor approxim network size less
349,1810.02845,"
        We propose a variational inference approach to deep probabilistic video compression. Our model uses advances in variational autoencoders (VAEs) for sequential data and combines it with recent work on neural image compression. The approach jointly learns to transform the original video into a lower-dimensional representation as well as to entropy code this representation according to a temporally-conditioned probabilistic model. We split the latent space into local (per frame) and global (per segment) variables, and show that training the VAE to utilize both representations leads to an improved rate-distortion performance. Evaluation on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content. Extreme compression performance is achieved for videos with specialized content if the model is trained on similar videos.
        △ Less
",propos variat infer approach deep probabilist video compress model use advanc variat autoencod vae sequenti data combin recent work neural imag compress approach jointli learn transform origin video lower dimension represent well entropi code represent accord tempor condit probabilist model split latent space local per frame global per segment variabl show train vae util represent lead improv rate distort perform evalu small video public data set vari complex divers show model yield competit result train gener video content extrem compress perform achiev video special content model train similar video less
350,1810.02837,"
        Often times, in many design problems, there is a need to select a small set of informative or representative elements from a large ground set of entities in an optimal fashion. Submodular optimization that provides for a formal way to solve such problems, has recently received significant attention from the controls community where such subset selection problems are abound. However, scaling these approaches to large systems can be challenging because of the high computational complexity of the overall flow, in-part due to the high-complexity compute-oracles used to determine the objective function values. In this work, we explore a well-known paradigm, namely leader-selection in a multi-agent networked environment to illustrate strategies for scalable submodular optimization. We study the performance of the state-of-the-art stochastic and distributed greedy algorithms as well as explore techniques that accelerate the computation oracles within the optimization loop. We finally present results combining accelerated greedy algorithms with accelerated computation oracles and demonstrate significant speedups with little loss of optimality when compared to the baseline ordinary greedy algorithm.
        △ Less
",often time mani design problem need select small set inform repres element larg ground set entiti optim fashion submodular optim provid formal way solv problem recent receiv signific attent control commun subset select problem abound howev scale approach larg system challeng high comput complex overal flow part due high complex comput oracl use determin object function valu work explor well known paradigm name leader select multi agent network environ illustr strategi scalabl submodular optim studi perform state art stochast distribut greedi algorithm well explor techniqu acceler comput oracl within optim loop final present result combin acceler greedi algorithm acceler comput oracl demonstr signific speedup littl loss optim compar baselin ordinari greedi algorithm less
351,1810.02835,"
        The objective of this paper is to compare the performance of three background-modeling algorithms in segmenting and detecting vehicles in highway traffic videos. All algorithms are available in OpenCV and were all coded in Python. We analyzed seven videos, totaling 2 hours of recording. To compare the algorithms, we created 35 ground-truth images, five from each video, and we used three different metrics: accuracy rate, precision rate, and processing time. By using accuracy and precision, we aim to identify how well the algorithms perform in detection and segmentation, while using the processing time to evaluate the impact on the computational system. Results indicate that all three algorithms had more than 90% of precision rate, while obtaining an average of 80% on accuracy. The algorithm with the lowest impact on processing time allowed the computation of 60 frames per second.
        △ Less
",object paper compar perform three background model algorithm segment detect vehicl highway traffic video algorithm avail opencv code python analyz seven video total hour record compar algorithm creat ground truth imag five video use three differ metric accuraci rate precis rate process time use accuraci precis aim identifi well algorithm perform detect segment use process time evalu impact comput system result indic three algorithm precis rate obtain averag accuraci algorithm lowest impact process time allow comput frame per second less
352,1810.02833,"
        We propose a new recurrent generative model for generating images from text captions while attending on specific parts of text captions. Our model creates images by incrementally adding patches on a ""canvas"" while attending on words from text caption at each timestep. Finally, the canvas is passed through an upscaling network to generate images. We also introduce a new method for generating visual-semantic sentence embeddings based on self-attention over text. We compare our model's generated images with those generated Reed et. al.'s model and show that our model is a stronger baseline for text to image generation tasks.
        △ Less
",propos new recurr gener model gener imag text caption attend specif part text caption model creat imag increment ad patch canva attend word text caption timestep final canva pass upscal network gener imag also introduc new method gener visual semant sentenc embed base self attent text compar model gener imag gener reed et al model show model stronger baselin text imag gener task less
353,1810.02832,"
        Recruitment of appropriate people for certain positions is critical for any companies or organizations. Manually screening to select appropriate candidates from large amounts of resumes can be exhausted and time-consuming. However, there is no public tool that can be directly used for automatic resume quality assessment (RQA). This motivates us to develop a method for automatic RQA. Since there is also no public dataset for model training and evaluation, we build a dataset for RQA by collecting around 10K resumes, which are provided by a private resume management company. By investigating the dataset, we identify some factors or features that could be useful to discriminate good resumes from bad ones, e.g., the consistency between different parts of a resume. Then a neural-network model is designed to predict the quality of each resume, where some text processing techniques are incorporated. To deal with the label deficiency issue in the dataset, we propose several variants of the model by either utilizing the pair/triplet-based loss, or introducing some semi-supervised learning technique to make use of the abundant unlabeled data. Both the presented baseline model and its variants are general and easy to implement. Various popular criteria including the receiver operating characteristic (ROC) curve, F-measure and ranking-based average precision (AP) are adopted for model evaluation. We compare the different variants with our baseline model. Since there is no public algorithm for RQA, we further compare our results with those obtained from a website that can score a resume. Experimental results in terms of different criteria demonstrate the effectiveness of the proposed method. We foresee that our approach would transform the way of future human resources management.
        △ Less
",recruit appropri peopl certain posit critic compani organ manual screen select appropri candid larg amount resum exhaust time consum howev public tool directli use automat resum qualiti assess rqa motiv us develop method automat rqa sinc also public dataset model train evalu build dataset rqa collect around k resum provid privat resum manag compani investig dataset identifi factor featur could use discrimin good resum bad one e g consist differ part resum neural network model design predict qualiti resum text process techniqu incorpor deal label defici issu dataset propos sever variant model either util pair triplet base loss introduc semi supervis learn techniqu make use abund unlabel data present baselin model variant gener easi implement variou popular criteria includ receiv oper characterist roc curv f measur rank base averag precis ap adopt model evalu compar differ variant baselin model sinc public algorithm rqa compar result obtain websit score resum experiment result term differ criteria demonstr effect propos method forese approach would transform way futur human resourc manag less
354,1810.02815,"
        It is well-known that demand response can improve the system efficiency as well as lower consumers' (prosumers') electricity bills. However, it is not clear how we can either qualitatively identify the prosumer with the most impact potential or quantitatively estimate each prosumer's contribution to the total social welfare improvement when additional resource capacity/flexibility is introduced to the system with demand response, such as allowing net-selling behavior. In this work, we build upon existing literature on the electricity market, which consists of price-taking prosumers each with various appliances, an electric utility company and a social welfare optimizing distribution system operator, to design a general sensitivity analysis approach (GSAA) that can estimate the potential of each consumer's contribution to the social welfare when given more resource capacity. GSAA is based on existence of an efficient competitive equilibrium, which we establish in the paper. When prosumers' utility functions are quadratic, GSAA can give closed forms characterization on social welfare improvement based on duality analysis. Furthermore, we extend GSAA to a general convex settings, i.e., utility functions with strong convexity and Lipschitz continuous gradient. Even without knowing the specific forms the utility functions, we can derive upper and lower bounds of the social welfare improvement potential of each prosumer, when extra resource is introduced. For both settings, several applications and numerical examples are provided: including extending AC comfort zone, ability of EV to discharge and net selling. The estimation results show that GSAA can be used to decide how to allocate potentially limited market resources in the most impactful way.
        △ Less
",well known demand respons improv system effici well lower consum prosum electr bill howev clear either qualit identifi prosum impact potenti quantit estim prosum contribut total social welfar improv addit resourc capac flexibl introduc system demand respons allow net sell behavior work build upon exist literatur electr market consist price take prosum variou applianc electr util compani social welfar optim distribut system oper design gener sensit analysi approach gsaa estim potenti consum contribut social welfar given resourc capac gsaa base exist effici competit equilibrium establish paper prosum util function quadrat gsaa give close form character social welfar improv base dualiti analysi furthermor extend gsaa gener convex set e util function strong convex lipschitz continu gradient even without know specif form util function deriv upper lower bound social welfar improv potenti prosum extra resourc introduc set sever applic numer exampl provid includ extend ac comfort zone abil ev discharg net sell estim result show gsaa use decid alloc potenti limit market resourc impact way less
355,1810.02810,"
        We study the problem of estimating a set of $d$ linear queries with respect to some unknown distribution $\mathbf{p}$ over a domain $\mathcal{J}=[J]$ based on a sensitive data set of $n$ individuals under the constraint of local differential privacy. This problem subsumes a wide range of estimation tasks, e.g., distribution estimation and $d$-dimensional mean estimation. We provide new algorithms for both the offline (non-adaptive) and adaptive versions of this problem.
  In the offline setting, the set of queries are fixed before the algorithm starts. In the regime where $n\lesssim d^2/\log(J)$, our algorithms attain $L_2$ estimation error that is independent of $d$, and is tight up to a factor of $\tilde{O}\left(\log^{1/4}(J)\right)$. For the special case of distribution estimation, we show that projecting the output estimate of an algorithm due to [Acharya et al. 2018] on the probability simplex yields an $L_2$ error that depends only sub-logarithmically on $J$ in the regime where $n\lesssim J^2/\log(J)$. These results show the possibility of accurate estimation of linear queries in the high-dimensional settings under the $L_2$ error criterion.
  In the adaptive setting, the queries are generated over $d$ rounds; one query at a time. In each round, a query can be chosen adaptively based on all the history of previous queries and answers. We give an algorithm for this problem with optimal $L_{\infty}$ estimation error (worst error in the estimated values for the queries w.r.t. the data distribution). Our bound matches a lower bound on the $L_{\infty}$ error for the offline version of this problem [Duchi et al. 2013].
        △ Less
",studi problem estim set linear queri respect unknown distribut mathbf p domain mathcal j j base sensit data set n individu constraint local differenti privaci problem subsum wide rang estim task e g distribut estim dimension mean estim provid new algorithm offlin non adapt adapt version problem offlin set set queri fix algorithm start regim n lesssim log j algorithm attain l estim error independ tight factor tild left log j right special case distribut estim show project output estim algorithm due acharya et al probabl simplex yield l error depend sub logarithm j regim n lesssim j log j result show possibl accur estim linear queri high dimension set l error criterion adapt set queri gener round one queri time round queri chosen adapt base histori previou queri answer give algorithm problem optim l infti estim error worst error estim valu queri w r data distribut bound match lower bound l infti error offlin version problem duchi et al less
356,1810.02802,"
        Many services that perform information retrieval for Points of Interest (POI) utilize a Lucene-based setup with spatial filtering. While this type of system is easy to implement it does not make use of semantics but relies on direct word matches between a query and reviews leading to a loss in both precision and recall. To study the challenging task of semantically enriching POIs from unstructured data in order to support open-domain search and question answering (QA), we introduce a new dataset POIReviewQA. It consists of 20k questions (e.g.""is this restaurant dog friendly?"") for 1022 Yelp business types. For each question we sampled 10 reviews, and annotated each sentence in the reviews whether it answers the question and what the corresponding answer is. To test a system's ability to understand the text we adopt an information retrieval evaluation by ranking all the review sentences for a question based on the likelihood that they answer this question. We build a Lucene-based baseline model, which achieves 77.0% AUC and 48.8% MAP. A sentence embedding-based model achieves 79.2% AUC and 41.8% MAP, indicating that the dataset presents a challenging problem for future research by the GIR community. The result technology can help exploit the thematic content of web documents and social media for characterisation of locations.
        △ Less
",mani servic perform inform retriev point interest poi util lucen base setup spatial filter type system easi implement make use semant reli direct word match queri review lead loss precis recal studi challeng task semant enrich poi unstructur data order support open domain search question answer qa introduc new dataset poireviewqa consist k question e g restaur dog friendli yelp busi type question sampl review annot sentenc review whether answer question correspond answer test system abil understand text adopt inform retriev evalu rank review sentenc question base likelihood answer question build lucen base baselin model achiev auc map sentenc embed base model achiev auc map indic dataset present challeng problem futur research gir commun result technolog help exploit themat content web document social media characteris locat less
357,1810.02797,"
        Efficient and precise classification of histological cell nuclei is of utmost importance due to its potential applications in the field of medical image analysis. It would facilitate the medical practitioners to better understand and explore various factors for cancer treatment. The classification of histological cell nuclei is a challenging task due to the cellular heterogeneity. This paper proposes an efficient Convolutional Neural Network (CNN) based architecture for classification of histological routine colon cancer nuclei named as RCCNet. The main objective of this network is to keep the CNN model as simple as possible. The proposed RCCNet model consists of only 1,512,868 learnable parameters which are significantly less compared to the popular CNN models such as AlexNet, CIFARVGG, GoogLeNet, and WRN. The experiments are conducted over publicly available routine colon cancer histological dataset ""CRCHistoPhenotypes"". The results of the proposed RCCNet model are compared with five state-of-the-art CNN models in terms of the accuracy, weighted average F1 score and training time. The proposed method has achieved a classification accuracy of 80.61% and 0.7887 weighted average F1 score. The proposed RCCNet is more efficient and generalized terms of the training time and data over-fitting, respectively.
        △ Less
",effici precis classif histolog cell nuclei utmost import due potenti applic field medic imag analysi would facilit medic practition better understand explor variou factor cancer treatment classif histolog cell nuclei challeng task due cellular heterogen paper propos effici convolut neural network cnn base architectur classif histolog routin colon cancer nuclei name rccnet main object network keep cnn model simpl possibl propos rccnet model consist learnabl paramet significantli less compar popular cnn model alexnet cifarvgg googlenet wrn experi conduct publicli avail routin colon cancer histolog dataset crchistophenotyp result propos rccnet model compar five state art cnn model term accuraci weight averag f score train time propos method achiev classif accuraci weight averag f score propos rccnet effici gener term train time data fit respect less
358,1810.02786,"
        The model parameters of convolutional neural networks (CNNs) are determined by backpropagation (BP). In this work, we propose an interpretable feedforward (FF) design without any BP as a reference. The FF design adopts a data-centric approach. It derives network parameters of the current layer based on data statistics from the output of the previous layer in a one-pass manner. To construct convolutional layers, we develop a new signal transform, called the Saab (Subspace Approximation with Adjusted Bias) transform. It is a variant of the principal component analysis (PCA) with an added bias vector to annihilate activation's nonlinearity. Multiple Saab transforms in cascade yield multiple convolutional layers. As to fully-connected (FC) layers, we construct them using a cascade of multi-stage linear least squared regressors (LSRs). The classification and robustness (against adversarial attacks) performances of BP- and FF-designed CNNs applied to the MNIST and the CIFAR-10 datasets are compared. Finally, we comment on the relationship between BP and FF designs.
        △ Less
",model paramet convolut neural network cnn determin backpropag bp work propos interpret feedforward ff design without bp refer ff design adopt data centric approach deriv network paramet current layer base data statist output previou layer one pass manner construct convolut layer develop new signal transform call saab subspac approxim adjust bia transform variant princip compon analysi pca ad bia vector annihil activ nonlinear multipl saab transform cascad yield multipl convolut layer fulli connect fc layer construct use cascad multi stage linear least squar regressor lsr classif robust adversari attack perform bp ff design cnn appli mnist cifar dataset compar final comment relationship bp ff design less
359,1810.02784,"
        A rainbow $q$-coloring of a $k$-uniform hypergraph is a $q$-coloring of the vertex set such that every hyperedge contains all $q$ colors.
  We prove that given a rainbow $(k - 2\lfloor \sqrt{k}\rfloor)$-colorable $k$-uniform hypergraph, it is NP-hard to find a normal $2$-coloring. Previously, this was only known for rainbow $\lfloor k/2 \rfloor$-colorable hypergraphs (Guruswami and Lee, SODA 2015).
  We also study a generalization which we call rainbow $(q, p)$-coloring, defined as a coloring using $q$ colors such that every hyperedge contains at least $p$ colors. We prove that given a rainbow $(k - \lfloor \sqrt{kc} \rfloor, k- \lfloor3\sqrt{kc} \rfloor)$-colorable $k$ uniform hypergraph, it is NP-hard to find a normal $c$-coloring for any constant $c < k/10$.
  The proof of our second result relies on two combinatorial theorems. One of the theorems was proved by Sarkaria (J. Comb. Theory. 1990) using topological methods and the other theorem we prove using a generalized Borsuk-Ulam theorem.
        △ Less
",rainbow q color k uniform hypergraph q color vertex set everi hyperedg contain q color prove given rainbow k lfloor sqrt k rfloor color k uniform hypergraph np hard find normal color previous known rainbow lfloor k rfloor color hypergraph guruswami lee soda also studi gener call rainbow q p color defin color use q color everi hyperedg contain least p color prove given rainbow k lfloor sqrt kc rfloor k lfloor sqrt kc rfloor color k uniform hypergraph np hard find normal c color constant c k proof second result reli two combinatori theorem one theorem prove sarkaria j comb theori use topolog method theorem prove use gener borsuk ulam theorem less
360,1810.02781,"
        Graphs are found in a plethora of domains, including online social networks, the World Wide Web and the study of epidemics, to name a few. With the advent of greater volumes of information and the need for continuously updated results under temporal constraints, it is necessary to explore novel approaches that further enable performance improvements. In the scope of stream processing over graphs, we research the trade-offs between result accuracy and the speedup of approximate computation techniques. We believe this to be a natural path towards these performance improvements. Herein we present GraphBolt, through which we conducted our research. It is an innovative model for approximate graph processing, implemented in Apache Flink. We analyze our model and evaluate it with the case study of the PageRank algorithm, perhaps the most famous measure of vertex centrality used to rank websites in search engine results. In light of our model, we discuss the challenges driven by relations between result accuracy and potential performance gains. Our experiments show that GraphBolt can reduce computational time by over 50% while achieving result quality above 95% when compared to results of the traditional version of PageRank without any summarization or approximation techniques.
        △ Less
",graph found plethora domain includ onlin social network world wide web studi epidem name advent greater volum inform need continu updat result tempor constraint necessari explor novel approach enabl perform improv scope stream process graph research trade off result accuraci speedup approxim comput techniqu believ natur path toward perform improv herein present graphbolt conduct research innov model approxim graph process implement apach flink analyz model evalu case studi pagerank algorithm perhap famou measur vertex central use rank websit search engin result light model discuss challeng driven relat result accuraci potenti perform gain experi show graphbolt reduc comput time achiev result qualiti compar result tradit version pagerank without summar approxim techniqu less
361,1810.02780,"
        Transient stability simulation of a large-scale and interconnected electric power system involves solving a large set of differential algebraic equations (DAEs) at every simulation time-step. With the ever-growing size and complexity of power grids, dynamic simulation becomes more time-consuming and computationally difficult using conventional sequential simulation techniques. To cope with this challenge, this paper aims to develop a fully distributed approach intended for implementation on High Performance Computer (HPC) clusters. A novel, relaxation-based domain decomposition algorithm known as Parallel-General-Norton with Multiple-port Equivalent (PGNME) is proposed as the core technique of a two-stage decomposition approach to divide the overall dynamic simulation problem into a set of subproblems that can be solved concurrently to exploit parallelism and scalability. While the convergence property has traditionally been a concern for relaxation-based decomposition, an estimation mechanism based on multiple-port network equivalent is adopted as the preconditioner to enhance the convergence of the proposed algorithm. The proposed algorithm is illustrated using rigorous mathematics and validated both in terms of speed-up and capability. Moreover, a complexity analysis is performed to support the observation that PGNME scales well when the size of the subproblems are sufficiently large.
        △ Less
",transient stabil simul larg scale interconnect electr power system involv solv larg set differenti algebra equat dae everi simul time step ever grow size complex power grid dynam simul becom time consum comput difficult use convent sequenti simul techniqu cope challeng paper aim develop fulli distribut approach intend implement high perform comput hpc cluster novel relax base domain decomposit algorithm known parallel gener norton multipl port equival pgnme propos core techniqu two stage decomposit approach divid overal dynam simul problem set subproblem solv concurr exploit parallel scalabl converg properti tradit concern relax base decomposit estim mechan base multipl port network equival adopt precondition enhanc converg propos algorithm propos algorithm illustr use rigor mathemat valid term speed capabl moreov complex analysi perform support observ pgnme scale well size subproblem suffici larg less
362,1810.02769,"
        Dynamic epistemic logics which model abilities of agents to make various announcements and influence each other's knowledge have been studied extensively in recent years. Two notable examples of such logics are Group Announcement Logic and Coalition Announcement Logic. They allow us to reason about what groups of agents can achieve through joint announcements in non-competitive and competitive environments. In this paper, we consider a combination of these logics -- Coalition and Relativised Group Announcement Logic and provide its complete axiomatisation. Moreover, we partially answer the question of how group and coalition announcement operators interact, and settle some other open problems.
        △ Less
",dynam epistem logic model abil agent make variou announc influenc knowledg studi extens recent year two notabl exampl logic group announc logic coalit announc logic allow us reason group agent achiev joint announc non competit competit environ paper consid combin logic coalit relativis group announc logic provid complet axiomatis moreov partial answer question group coalit announc oper interact settl open problem less
363,1810.02766,"
        Generating a robust representation of the environment is a crucial ability of learning agents. Deep learning based methods have greatly improved perception systems but still fail in challenging situations. These failures are often not solvable on the basis of a single image. In this work, we present a parameter-efficient temporal filtering concept which extends an existing single-frame segmentation model to work with multiple frames. The resulting recurrent architecture temporally filters representations on all abstraction levels in a hierarchical manner, while decoupling temporal dependencies from scene representation. Using a synthetic dataset, we show the ability of our model to cope with data perturbations and highlight the importance of recurrent and hierarchical filtering.
        △ Less
",gener robust represent environ crucial abil learn agent deep learn base method greatli improv percept system still fail challeng situat failur often solvabl basi singl imag work present paramet effici tempor filter concept extend exist singl frame segment model work multipl frame result recurr architectur tempor filter represent abstract level hierarch manner decoupl tempor depend scene represent use synthet dataset show abil model cope data perturb highlight import recurr hierarch filter less
364,1810.02765,"
        According to the principles articulated in the agile manifesto, motivated and empowered software developers relying on technical excellence and simple designs, create business value by delivering working software to users at regular short intervals. These principles have spawned many practices. At the core of these practices is the idea of autonomous, self-managing, or self-organizing teams whose members work at a pace that sustains their creativity and productivity. This article summarizes the main challenges faced when implementing autonomous teams and the topics and research questions that future research should address.
        △ Less
",accord principl articul agil manifesto motiv empow softwar develop reli technic excel simpl design creat busi valu deliv work softwar user regular short interv principl spawn mani practic core practic idea autonom self manag self organ team whose member work pace sustain creativ product articl summar main challeng face implement autonom team topic research question futur research address less
365,1810.02762,"
        Anomaly detection is an important step in the management and monitoring of data centers and cloud computing platforms. The ability to detect anomalous virtual machines before real failures occur results in reduced downtime while operations engineers urgently recover malfunctioning virtual machines, efficient root cause analysis, and improved customer optics in the event said malfunction lead to an outage. Virtual machines could fail at any time, whether in a lab or production system. If there is no anomaly detection system, and a virtual machine in a lab environment fails, the QA and DEV team will have to switch to another environment while the OPS team fixes the failure. The potential impact of failing to detect anomalous virtual machines can result in financial ramifications, both when developing new features and servicing existing ones. This paper presents a model that can efficiently detect anomalous virtual machines both in production and testing environments.
        △ Less
",anomali detect import step manag monitor data center cloud comput platform abil detect anomal virtual machin real failur occur result reduc downtim oper engin urgent recov malfunct virtual machin effici root caus analysi improv custom optic event said malfunct lead outag virtual machin could fail time whether lab product system anomali detect system virtual machin lab environ fail qa dev team switch anoth environ op team fix failur potenti impact fail detect anomal virtual machin result financi ramif develop new featur servic exist one paper present model effici detect anomal virtual machin product test environ less
366,1810.02758,"
        One of the most celebrated results in mechanism design is Myerson's characterization of the revenue optimal auction for selling a single item. However, this result relies heavily on the assumption that buyers are indifferent to risk. In this paper we investigate the case where the buyers are risk-loving, i.e. they prefer gambling to being rewarded deterministically. We use the standard model for risk from expected utility theory, where risk-loving behavior is represented by a convex utility function.
  We focus our attention on the special case of exponential utility functions. We characterize the optimal auction and show that randomization can be used to extract more revenue than when buyers are risk-neutral. Most importantly, we show that the optimal auction is simple: the optimal revenue can be extracted using a randomized take-it-or-leave-it price for a single buyer and using a loser-pay auction, a variant of the all-pay auction, for multiple buyers. Finally, we show that these results no longer hold for convex utility functions beyond exponential.
        △ Less
",one celebr result mechan design myerson character revenu optim auction sell singl item howev result reli heavili assumpt buyer indiffer risk paper investig case buyer risk love e prefer gambl reward determinist use standard model risk expect util theori risk love behavior repres convex util function focu attent special case exponenti util function character optim auction show random use extract revenu buyer risk neutral importantli show optim auction simpl optim revenu extract use random take leav price singl buyer use loser pay auction variant pay auction multipl buyer final show result longer hold convex util function beyond exponenti less
367,1810.02749,"
        Today we see the use of the Internet of Things (IoT) in various application domains such as healthcare, smart homes, smart cars, and smart-x applications in smart cities. The number of applications based on IoT and cloud computing is projected to increase rapidly over the next few years. IoT-based services must meet the guaranteed levels of quality of service (QoS) to match users' expectations. Ensuring QoS through specifying the QoS constraints using Service Level Agreements (SLAs) is crucial. Therefore, as a first step toward SLA management, it is essential to provide an SLA specification in a machine-readable format. In this paper, we demonstrate a toolkit for creating SLA specifications for IoT applications. The toolkit is used to simplify the process of capturing the requirements of IoT applications. We present a demonstration of the toolkit using a Remote Health Monitoring Service (RHMS) usecase. The toolkit supports the following: (1) specifying the Service-Level Objectives (SLO) of an IoT application at the application level; (2) specifying the workflow activities of the IoT application; (3) mapping each activity to the required software and hardware resources and specifying the constraints of SLOs and other configuration- related metrics of the required hardware and software; and (4) creating the composed SLA in JSON format.
        △ Less
",today see use internet thing iot variou applic domain healthcar smart home smart car smart x applic smart citi number applic base iot cloud comput project increas rapidli next year iot base servic must meet guarante level qualiti servic qo match user expect ensur qo specifi qo constraint use servic level agreement sla crucial therefor first step toward sla manag essenti provid sla specif machin readabl format paper demonstr toolkit creat sla specif iot applic toolkit use simplifi process captur requir iot applic present demonstr toolkit use remot health monitor servic rhm usecas toolkit support follow specifi servic level object slo iot applic applic level specifi workflow activ iot applic map activ requir softwar hardwar resourc specifi constraint slo configur relat metric requir hardwar softwar creat compos sla json format less
368,1810.02741,"
        In essence, the two tagging methods (direct tagging and tagging with sentences compression) are to tag the information we need by using regular expression which basing on the inherent language patterns of the natural language. Though it has many advantages in extracting regular data, Direct tagging is not applicable to some situations. if the data we need extract is not regular and its surrounding words are regular is relatively regular, then we can use information compression to cut the information we do not need before we tagging the data we need. In this way we can increase the precision of the data while not undermine the recall of the data.
        △ Less
",essenc two tag method direct tag tag sentenc compress tag inform need use regular express base inher languag pattern natur languag though mani advantag extract regular data direct tag applic situat data need extract regular surround word regular rel regular use inform compress cut inform need tag data need way increas precis data undermin recal data less
369,1810.02726,"
        The visual scoring of arousals during sleep routinely conducted by sleep experts is a challenging task warranting an automatic approach. This paper presents an algorithm for automatic detection of arousals during sleep. Using the Physionet/CinC Challenge dataset, an 80-20% subject-level split was performed to create in-house training and test sets, respectively. The data for each subject in the training set was split to 30-second epochs with no overlap. A total of 428 features from EEG, EMG, EOG, airflow, and SaO2 in each epoch were extracted and used for creating subject-specific models based on an ensemble of bagged classification trees, resulting in 943 models. For marking arousal and non-arousal regions in the test set, the data in the test set was split to 30-second epochs with 50% overlaps. The average of arousal probabilities from different patient-specific models was assigned to each 30-second epoch and then a sample-wise probability vector with the same length as test data was created for model evaluation. Using the PhysioNet/CinC Challenge 2018 scoring criteria, AUPRCs of 0.25 and 0.21 were achieved for the in-house test and blind test sets, respectively.
        △ Less
",visual score arous sleep routin conduct sleep expert challeng task warrant automat approach paper present algorithm automat detect arous sleep use physionet cinc challeng dataset subject level split perform creat hous train test set respect data subject train set split second epoch overlap total featur eeg emg eog airflow sao epoch extract use creat subject specif model base ensembl bag classif tree result model mark arous non arous region test set data test set split second epoch overlap averag arous probabl differ patient specif model assign second epoch sampl wise probabl vector length test data creat model evalu use physionet cinc challeng score criteria auprc achiev hous test blind test set respect less
370,1810.02724,"
        It is possible to rely on current corporate law to grant legal personhood to Artificially Intelligent (AI) agents. In this paper, after introducing pathways to AI personhood, we analyze consequences of such AI empowerment on human dignity, human safety and AI rights. We emphasize possibility of creating selfish memes and legal system hacking in the context of artificial entities. Finally, we consider some potential solutions for addressing described problems.
        △ Less
",possibl reli current corpor law grant legal personhood artifici intellig ai agent paper introduc pathway ai personhood analyz consequ ai empower human digniti human safeti ai right emphas possibl creat selfish meme legal system hack context artifici entiti final consid potenti solut address describ problem less
371,1810.02720,"
        We present TRANX, a transition-based neural semantic parser that maps natural language (NL) utterances into formal meaning representations (MRs). TRANX uses a transition system based on the abstract syntax description language for the target MR, which gives it two major advantages: (1) it is highly accurate, using information from the syntax of the target MR to constrain the output space and model the information flow, and (2) it is highly generalizable, and can easily be applied to new types of MR by just writing a new abstract syntax description corresponding to the allowable structures in the MR. Experiments on four different semantic parsing and code generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers.
        △ Less
",present tranx transit base neural semant parser map natur languag nl utter formal mean represent mr tranx use transit system base abstract syntax descript languag target mr give two major advantag highli accur use inform syntax target mr constrain output space model inform flow highli generaliz easili appli new type mr write new abstract syntax descript correspond allow structur mr experi four differ semant pars code gener task show system generaliz extens effect regist strong result compar exist neural semant parser less
372,1810.02718,"
        Compilers are widely-used infrastructures in accelerating the software development, and expected to be trustworthy. In the literature, various testing technologies have been proposed to guarantee the quality of compilers. However, there remains an obstacle to comprehensively characterize and understand compiler testing. To overcome this obstacle, we propose a literature analysis framework to gain insights into the compiler testing area. First, we perform an extensive search to construct a dataset related to compiler testing papers. Then, we conduct a bibliometric analysis to analyze the productive authors, the influential papers, and the frequently tested compilers based on our dataset. Finally, we utilize association rules and collaboration networks to mine the authorships and the communities of interests among researchers and keywords. Some valuable results are reported. We find that the USA is the leading country that contains the most influential researchers and institutions. The most active keyword is ""random testing"". We also find that most researchers have broad interests within small-scale collaborators in the compiler testing area.
        △ Less
",compil wide use infrastructur acceler softwar develop expect trustworthi literatur variou test technolog propos guarante qualiti compil howev remain obstacl comprehens character understand compil test overcom obstacl propos literatur analysi framework gain insight compil test area first perform extens search construct dataset relat compil test paper conduct bibliometr analysi analyz product author influenti paper frequent test compil base dataset final util associ rule collabor network mine authorship commun interest among research keyword valuabl result report find usa lead countri contain influenti research institut activ keyword random test also find research broad interest within small scale collabor compil test area less
373,1810.02717,"
        Social media corpora pose unique challenges and opportunities, including typically short document lengths and rich meta-data such as author characteristics and relationships. This creates great potential for systematic analysis of the enormous body of the users and thus provides implications for industrial strategies such as targeted marketing. Here we propose a novel and statistically principled method, clust-LDA, which incorporates authorship structure into the topical modeling, thus accomplishing the task of the topical inferences across documents on the basis of authorship and, simultaneously, the identification of groupings between authors. We develop an inference procedure for clust-LDA and demonstrate its performance on simulated data, showing that clust-LDA out-performs the ""vanilla"" LDA on the topic identification task where authors exhibit distinctive topical preference. We also showcase the empirical performance of clust-LDA based on a real-world social media dataset from Reddit.
        △ Less
",social media corpora pose uniqu challeng opportun includ typic short document length rich meta data author characterist relationship creat great potenti systemat analysi enorm bodi user thu provid implic industri strategi target market propos novel statist principl method clust lda incorpor authorship structur topic model thu accomplish task topic infer across document basi authorship simultan identif group author develop infer procedur clust lda demonstr perform simul data show clust lda perform vanilla lda topic identif task author exhibit distinct topic prefer also showcas empir perform clust lda base real world social media dataset reddit less
374,1810.02716,"
        Consider the following class of learning schemes: \begin{equation} \label{eq:main-problem1}
  \hat{\boldsymbolβ} := \underset{\boldsymbolβ \in \mathcal{C}}{\arg\min} \;\sum_{j=1}^n \ell(\boldsymbol{x}_j^\top\boldsymbolβ; y_j) + λR(\boldsymbolβ), \qquad \qquad \qquad (1) \end{equation} where $\boldsymbol{x}_i \in \mathbb{R}^p$ and $y_i \in \mathbb{R}$ denote the $i^{\rm th}$ feature and response variable respectively. Let $\ell$ and $R$ be the convex loss function and regularizer, $\boldsymbolβ$ denote the unknown weights, and $λ$ be a regularization parameter. $\mathcal{C} \subset \mathbb{R}^{p}$ is a closed convex set. Finding the optimal choice of $λ$ is a challenging problem in high-dimensional regimes where both $n$ and $p$ are large. We propose three frameworks to obtain a computationally efficient approximation of the leave-one-out cross validation (LOOCV) risk for nonsmooth losses and regularizers. Our three frameworks are based on the primal, dual, and proximal formulations of (1). Each framework shows its strength in certain types of problems. We prove the equivalence of the three approaches under smoothness conditions. This equivalence enables us to justify the accuracy of the three methods under such conditions. We use our approaches to obtain a risk estimate for several standard problems, including generalized LASSO, nuclear norm regularization, and support vector machines. We empirically demonstrate the effectiveness of our results for non-differentiable cases.
        △ Less
",consid follow class learn scheme begin equat label eq main problem hat boldsymbol underset boldsymbol mathcal c arg min sum j n ell boldsymbol x j top boldsymbol j r boldsymbol qquad qquad qquad end equat boldsymbol x mathbb r p mathbb r denot rm th featur respons variabl respect let ell r convex loss function regular boldsymbol denot unknown weight regular paramet mathcal c subset mathbb r p close convex set find optim choic challeng problem high dimension regim n p larg propos three framework obtain comput effici approxim leav one cross valid loocv risk nonsmooth loss regular three framework base primal dual proxim formul framework show strength certain type problem prove equival three approach smooth condit equival enabl us justifi accuraci three method condit use approach obtain risk estim sever standard problem includ gener lasso nuclear norm regular support vector machin empir demonstr effect result non differenti case less
375,1810.02713,"
        In novel forms of the Social Internet of Things, any mobile user within communication range may help routing messages for another user in the network. The resulting message delivery rate depends both on the users' mobility patterns and the message load in the network. This new type of configuration, however, poses new challenges to security, amongst them, assessing the effect that a group of colluding malicious participants can have on the global message delivery rate in such a network is far from trivial. In this work, after modeling such a question as an optimization problem, we are able to find quite interesting results by coupling a network simulator with an evolutionary algorithm. The chosen algorithm is specifically designed to solve problems whose solutions can be decomposed into parts sharing the same structure. We demonstrate the effectiveness of the proposed approach on two medium-sized Delay-Tolerant Networks, realistically simulated in the urban contexts of two cities with very different route topology: Venice and San Francisco. In all experiments, our methodology produces attack patterns that greatly lower network performance with respect to previous studies on the subject, as the evolutionary core is able to exploit the specific weaknesses of each target configuration.
        △ Less
",novel form social internet thing mobil user within commun rang may help rout messag anoth user network result messag deliveri rate depend user mobil pattern messag load network new type configur howev pose new challeng secur amongst assess effect group collud malici particip global messag deliveri rate network far trivial work model question optim problem abl find quit interest result coupl network simul evolutionari algorithm chosen algorithm specif design solv problem whose solut decompos part share structur demonstr effect propos approach two medium size delay toler network realist simul urban context two citi differ rout topolog venic san francisco experi methodolog produc attack pattern greatli lower network perform respect previou studi subject evolutionari core abl exploit specif weak target configur less
376,1810.02711,"
        We present new integer linear programming (ILP) models for NP-hard optimisation problems in instances of the Stable Marriage problem with Ties and Incomplete lists (SMTI) and its many-to-one generalisation, the Hospitals / Residents problem with Ties (HRT). These models can be used to efficiently solve these optimisation problems when applied to (i) instances derived from real-world applications, and (ii) larger instances that are randomly-generated. In the case of SMTI, we consider instances arising from the pairing of children with adoptive families, where preferences are obtained from a quality measure of each possible pairing of child to family. In this case we seek a maximum weight stable matching. We present new algorithms for preprocessing instances of SMTI with ties on both sides, as well as new ILP models. Algorithms based on existing state-of-the-art models only solve 6 of our 22 real-world instances within an hour per instance, and our new models solve all 22 instances within a mean runtime of 60 seconds. For HRT, we consider instances derived from the problem of assigning junior doctors to foundation posts in Scottish hospitals. Here we seek a maximum size stable matching. We show how to extend our models for SMTI to the HRT case. For the real instances, we reduce the mean runtime from an average of 144 seconds when using state-of-the-art methods, to 3 seconds when using our new ILP-based algorithms. We also show that our models outperform considerably state-of-the-art models on larger randomly-generated instances of SMTI and HRT.
        △ Less
",present new integ linear program ilp model np hard optimis problem instanc stabl marriag problem tie incomplet list smti mani one generalis hospit resid problem tie hrt model use effici solv optimis problem appli instanc deriv real world applic ii larger instanc randomli gener case smti consid instanc aris pair children adopt famili prefer obtain qualiti measur possibl pair child famili case seek maximum weight stabl match present new algorithm preprocess instanc smti tie side well new ilp model algorithm base exist state art model solv real world instanc within hour per instanc new model solv instanc within mean runtim second hrt consid instanc deriv problem assign junior doctor foundat post scottish hospit seek maximum size stabl match show extend model smti hrt case real instanc reduc mean runtim averag second use state art method second use new ilp base algorithm also show model outperform consider state art model larger randomli gener instanc smti hrt less
377,1810.02702,"
        The performance of evolutionary algorithms can be heavily undermined when constraints limit the feasible areas of the search space. For instance, while Covariance Matrix Adaptation Evolution Strategy is one of the most efficient algorithms for unconstrained optimization problems, it cannot be readily applied to constrained ones. Here, we used concepts from Memetic Computing, i.e. the harmonious combination of multiple units of algorithmic information, and Viability Evolution, an alternative abstraction of artificial evolution, to devise a novel approach for solving optimization problems with inequality constraints. Viability Evolution emphasizes elimination of solutions not satisfying viability criteria, defined as boundaries on objectives and constraints. These boundaries are adapted during the search to drive a population of local search units, based on Covariance Matrix Adaptation Evolution Strategy, towards feasible regions. These units can be recombined by means of Differential Evolution operators. Of crucial importance for the performance of our method, an adaptive scheduler toggles between exploitation and exploration by selecting to advance one of the local search units and/or recombine them. The proposed algorithm can outperform several state-of-the-art methods on a diverse set of benchmark and engineering problems, both for quality of solutions and computational resources needed.
        △ Less
",perform evolutionari algorithm heavili undermin constraint limit feasibl area search space instanc covari matrix adapt evolut strategi one effici algorithm unconstrain optim problem cannot readili appli constrain one use concept memet comput e harmoni combin multipl unit algorithm inform viabil evolut altern abstract artifici evolut devis novel approach solv optim problem inequ constraint viabil evolut emphas elimin solut satisfi viabil criteria defin boundari object constraint boundari adapt search drive popul local search unit base covari matrix adapt evolut strategi toward feasibl region unit recombin mean differenti evolut oper crucial import perform method adapt schedul toggl exploit explor select advanc one local search unit recombin propos algorithm outperform sever state art method divers set benchmark engin problem qualiti solut comput resourc need less
378,1810.02695,"
        Depth prediction is one of the fundamental problems in computer vision. In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for various depth estimation tasks. Specifically, it is an efficient linear propagation model, in which the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN). We can append this module to any output from a state-of-the-art (SOTA) depth estimation networks to improve their performances. In practice, we further extend CSPN in two aspects: 1) take sparse depth map as additional input, which is useful for the task of depth completion; 2) similar to commonly used 3D convolution operation in CNNs, we propose 3D CSPN to handle features with one additional dimension, which is effective in the task of stereo matching using 3D cost volume. For the tasks of sparse to dense, a.k.a depth completion. We experimented the proposed CPSN conjunct algorithms over the popular NYU v2 and KITTI datasets, where we show that our proposed algorithms not only produce high quality (e.g., 30% more reduction in depth error), but also run faster (e.g., 2 to 5x faster) than previous SOTA spatial propagation network. We also evaluated our stereo matching algorithm on the Scene Flow and KITTI Stereo datasets, and rank 1st on both the KITTI Stereo 2012 and 2015 benchmarks, which demonstrates the effectiveness of the proposed module. The code of CSPN proposed in this work will be released at https://github.com/XinJCheng/CSPN.
        △ Less
",depth predict one fundament problem comput vision paper propos simpl yet effect convolut spatial propag network cspn learn affin matrix variou depth estim task specif effici linear propag model propag perform manner recurr convolut oper affin among neighbor pixel learn deep convolut neural network cnn append modul output state art sota depth estim network improv perform practic extend cspn two aspect take spars depth map addit input use task depth complet similar commonli use convolut oper cnn propos cspn handl featur one addit dimens effect task stereo match use cost volum task spars dens k depth complet experi propos cpsn conjunct algorithm popular nyu v kitti dataset show propos algorithm produc high qualiti e g reduct depth error also run faster e g x faster previou sota spatial propag network also evalu stereo match algorithm scene flow kitti stereo dataset rank st kitti stereo benchmark demonstr effect propos modul code cspn propos work releas http github com xinjcheng cspn less
379,1810.02690,"
        Robots state of insecurity is onstage. There is an emerging concern about major robot vulnerabilities and their adverse consequences. However, there is still a considerable gap between robotics and cybersecurity domains. For the purpose of filling that gap, the present technical report presents the Robotics CTF (RCTF), an online playground to challenge robot security from any browser. We describe the architecture of the RCTF and provide 9 scenarios where hackers can challenge the security of different robotic setups. Our work empowers security researchers to a) reproduce virtual robotic scenarios locally and b) change the networking setup to mimic real robot targets. We advocate for hacker powered security in robotics and contribute by open sourcing our scenarios.
        △ Less
",robot state insecur onstag emerg concern major robot vulner advers consequ howev still consider gap robot cybersecur domain purpos fill gap present technic report present robot ctf rctf onlin playground challeng robot secur browser describ architectur rctf provid scenario hacker challeng secur differ robot setup work empow secur research reproduc virtual robot scenario local b chang network setup mimic real robot target advoc hacker power secur robot contribut open sourc scenario less
380,1810.02689,"
        Evaluation has always been a key challenge in the development of artificial intelligence (AI) based software, due to the technical complexity of the software artifact and, often, its embedding in complex sociotechnical processes. Recent advances in machine learning (ML) enabled by deep neural networks has exacerbated the challenge of evaluating such software due to the opaque nature of these ML-based artifacts. A key related issue is the (in)ability of such systems to generate useful explanations of their outputs, and we argue that the explanation and evaluation problems are closely linked. The paper models the elements of a ML-based AI system in the context of public sector decision (PSD) applications involving both artificial and human intelligence, and maps these elements against issues in both evaluation and explanation, showing how the two are related. We consider a number of common PSD application patterns in the light of our model, and identify a set of key issues connected to explanation and evaluation in each case. Finally, we propose multiple strategies to promote wider adoption of AI/ML technologies in PSD, where each is distinguished by a focus on different elements of our model, allowing PSD policy makers to adopt an approach that best fits their context and concerns.
        △ Less
",evalu alway key challeng develop artifici intellig ai base softwar due technic complex softwar artifact often embed complex sociotechn process recent advanc machin learn ml enabl deep neural network exacerb challeng evalu softwar due opaqu natur ml base artifact key relat issu abil system gener use explan output argu explan evalu problem close link paper model element ml base ai system context public sector decis psd applic involv artifici human intellig map element issu evalu explan show two relat consid number common psd applic pattern light model identifi set key issu connect explan evalu case final propos multipl strategi promot wider adopt ai ml technolog psd distinguish focu differ element model allow psd polici maker adopt approach best fit context concern less
381,1810.02688,"
        Big data, data science, deep learning, artificial intelligence are the key words of intense hype related with a job market in full evolution, that impose  to adapt the contents of our university professional trainings. Which artificial intelligence is mostly concerned by the job offers? Which methodologies and technologies should be favored in the training pprograms? Which objectives, tools and educational resources do we needed to put in place to meet these pressing needs? We answer these questions in describing the contents and operational ressources in the Data Science  orientation of  the speciality  Applied Mathematics at INSA Toulouse.  We focus on basic mathematics training (Optimization, Probability, Statistics), associated with the practical implementation of the most performing statistical learning algorithms, with the most appropriate technologies and on real examples. Considering the huge volatility of the technologies, it is imperative to train students in seft-training, this will be their technological watch tool when they will be in professional activity. This  explains  the structuring of the educational site https://github.com/wikistat/ into a set of tutorials. Finally, to motivate the thorough practice of these tutorials, a serious game is organized each year in the form of a prediction contest between students of Master degrees in Applied Mathematics for IA.
        △ Less
",big data data scienc deep learn artifici intellig key word intens hype relat job market full evolut impos adapt content univers profession train artifici intellig mostli concern job offer methodolog technolog favor train pprogram object tool educ resourc need put place meet press need answer question describ content oper ressourc data scienc orient special appli mathemat insa toulous focu basic mathemat train optim probabl statist associ practic implement perform statist learn algorithm appropri technolog real exampl consid huge volatil technolog imper train student seft train technolog watch tool profession activ explain structur educ site http github com wikistat set tutori final motiv thorough practic tutori seriou game organ year form predict contest student master degre appli mathemat ia less
382,1810.02685,"
        Ethical systems are usually described as principles for distinguishing right from wrong and forming beliefs about proper conduct. Ethical topics are complex, with excessively verbose accounts of mental models and intensely ingrained philosophical assumptions. From practical experience, in teaching ethics for software engineering students, an explanation of ethics alone often cannot provide insights of behavior and thought for students. Additionally, it seems that there has been no exploration into the development of a conceptual presentation of ethics that appeals to computer engineers. This is particularly clear in the area of software engineering, which focuses on software and associated tools such as algorithms, diagramming, documentation, modeling and design as applied to various types of data and conceptual artifacts. It seems that software engineers look at ethical materials as a collection of ideas and notions that lack systemization and uniformity. Accordingly, this paper explores a thinging schematization for ethical theories that can serve a role similar to that of modeling languages (e.g., UML). In this approach, thinging means actualization (existence, presence, being) of things and mechanisms that define a boundary around some region of ethically related reality, separating it from everything else. The resultant diagrammatic representation then developed to model the process of making ethical decisions in that region.
        △ Less
",ethic system usual describ principl distinguish right wrong form belief proper conduct ethic topic complex excess verbos account mental model intens ingrain philosoph assumpt practic experi teach ethic softwar engin student explan ethic alon often cannot provid insight behavior thought student addit seem explor develop conceptu present ethic appeal comput engin particularli clear area softwar engin focus softwar associ tool algorithm diagram document model design appli variou type data conceptu artifact seem softwar engin look ethic materi collect idea notion lack system uniform accordingli paper explor thing schemat ethic theori serv role similar model languag e g uml approach thing mean actual exist presenc thing mechan defin boundari around region ethic relat realiti separ everyth els result diagrammat represent develop model process make ethic decis region less
383,1810.02684,"
        Physically-based overland flow models are computationally demanding, hindering their use for real-time applications. Therefore, the development of fast (and reasonably accurate) overland flow models is needed if they are to be used to support flood mitigation decision making. In this study, we investigate the potential of Self-Organizing Maps to rapidly generate water depth and flood extent results. To conduct the study, we developed a flood-simulation specific SOM, using cellular automata flood model results and a synthetic DEM and inflow hydrograph. The preliminary results showed that water depth and flood extent results produced by the SOM are reasonably accurate and obtained in a very short period of time. Based on this, it seems that SOMs have the potential to provide critical flood information to support real-time flood mitigation decisions. The findings presented would however require further investigations to obtain general conclusions; these further investigations may include the consideration of real terrain representations, real water supply networks and realistic inflows from pipe bursts.
        △ Less
",physic base overland flow model comput demand hinder use real time applic therefor develop fast reason accur overland flow model need use support flood mitig decis make studi investig potenti self organ map rapidli gener water depth flood extent result conduct studi develop flood simul specif som use cellular automata flood model result synthet dem inflow hydrograph preliminari result show water depth flood extent result produc som reason accur obtain short period time base seem som potenti provid critic flood inform support real time flood mitig decis find present would howev requir investig obtain gener conclus investig may includ consider real terrain represent real water suppli network realist inflow pipe burst less
384,1810.02683,"
        Diffusion magnetic resonance imaging (diffusion MRI) is a non-invasive microstructure assessment method. Scalar measures quantifying micro-structural tissue properties can be obtained using diffusion models and data processing pipelines. However, it is costly and time consuming to collect high quality diffusion data. We demonstrate how Generative Adversarial Networks (GANs) can be used to generate diffusion scalar measures from structural MR images in a single optimized step, without diffusion models and diffusion data. We show that the used Cycle-GAN model can synthesize visually realistic and quantitatively accurate diffusion-derived scalar measures.
        △ Less
",diffus magnet reson imag diffus mri non invas microstructur assess method scalar measur quantifi micro structur tissu properti obtain use diffus model data process pipelin howev costli time consum collect high qualiti diffus data demonstr gener adversari network gan use gener diffus scalar measur structur mr imag singl optim step without diffus model diffus data show use cycl gan model synthes visual realist quantit accur diffus deriv scalar measur less
385,1810.02679,"
        Wireless Sensor Networks (WSNs) is an emerging technology in several application domains, ranging from urban surveillance to environmental and structural monitoring. Computational Intelligence (CI) techniques are particularly suitable for enhancing these systems. However, when embedding CI into wireless sensors, severe hardware limitations must be taken into account. In this paper we investigate the possibility to perform an online, distributed optimization process within a WSN. Such a system might be used, for example, to implement advanced network features like distributed modelling, self-optimizing protocols, and anomaly detection, to name a few. The proposed approach, called DOWSN (Distributed Optimization for WSN) is an island-model infrastructure in which each node executes a simple, computationally cheap (both in terms of CPU and memory) optimization algorithm, and shares promising solutions with its neighbors. We perform extensive tests of different DOWSN configurations on a benchmark made up of continuous optimization problems; we analyze the influence of the network parameters (number of nodes, inter-node communication period and probability of accepting incoming solutions) on the optimization performance. Finally, we profile energy and memory consumption of DOWSN to show the efficient usage of the limited hardware resources available on the sensor nodes.
        △ Less
",wireless sensor network wsn emerg technolog sever applic domain rang urban surveil environment structur monitor comput intellig ci techniqu particularli suitabl enhanc system howev embed ci wireless sensor sever hardwar limit must taken account paper investig possibl perform onlin distribut optim process within wsn system might use exampl implement advanc network featur like distribut model self optim protocol anomali detect name propos approach call dowsn distribut optim wsn island model infrastructur node execut simpl comput cheap term cpu memori optim algorithm share promis solut neighbor perform extens test differ dowsn configur benchmark made continu optim problem analyz influenc network paramet number node inter node commun period probabl accept incom solut optim perform final profil energi memori consumpt dowsn show effici usag limit hardwar resourc avail sensor node less
386,1810.02678,"
        We introduce a method, KL-LIME, for explaining predictions of Bayesian predictive models by projecting the information in the predictive distribution locally to a simpler, interpretable explanation model. The proposed approach combines the recent Local Interpretable Model-agnostic Explanations (LIME) method with ideas from Bayesian projection predictive variable selection methods. The information theoretic basis helps in navigating the trade-off between explanation fidelity and complexity. We demonstrate the method in explaining MNIST digit classifications made by a Bayesian deep convolutional neural network.
        △ Less
",introduc method kl lime explain predict bayesian predict model project inform predict distribut local simpler interpret explan model propos approach combin recent local interpret model agnost explan lime method idea bayesian project predict variabl select method inform theoret basi help navig trade explan fidel complex demonstr method explain mnist digit classif made bayesian deep convolut neural network less
387,1810.02677,"
        Clustering is a fundamental problem in unsupervised learning. Popular methods like K-means, may suffer from poor performance as they are prone to get stuck in its local minima. Recently, the sum-of-norms (SON) model (also known as the clustering path) has been proposed in Pelckmans et al. (2005), Lindsten et al. (2011) and Hocking et al. (2011). The perfect recovery properties of the convex clustering model with uniformly weighted all pairwise-differences regularization have been proved by Zhu et al. (2014) and Panahi et al. (2017). However, no theoretical guarantee has been established for the general weighted convex clustering model, where better empirical results have been observed. In the numerical optimization aspect, although algorithms like the alternating direction method of multipliers (ADMM) and the alternating minimization algorithm (AMA) have been proposed to solve the convex clustering model (Chi and Lange, 2015), it still remains very challenging to solve large-scale problems. In this paper, we establish sufficient conditions for the perfect recovery guarantee of the general weighted convex clustering model, which include and improve existing theoretical results as special cases. In addition, we develop a semismooth Newton based augmented Lagrangian method for solving large-scale convex clustering problems. Extensive numerical experiments on both simulated and real data demonstrate that our algorithm is highly efficient and robust for solving large-scale problems. Moreover, the numerical results also show the superior performance and scalability of our algorithm comparing to the existing first-order methods. In particular, our algorithm is able to solve a convex clustering problem with 200,000 points in $\mathbb{R}^3$ in about 6 minutes.
        △ Less
",cluster fundament problem unsupervis learn popular method like k mean may suffer poor perform prone get stuck local minima recent sum norm son model also known cluster path propos pelckman et al lindsten et al hock et al perfect recoveri properti convex cluster model uniformli weight pairwis differ regular prove zhu et al panahi et al howev theoret guarante establish gener weight convex cluster model better empir result observ numer optim aspect although algorithm like altern direct method multipli admm altern minim algorithm ama propos solv convex cluster model chi lang still remain challeng solv larg scale problem paper establish suffici condit perfect recoveri guarante gener weight convex cluster model includ improv exist theoret result special case addit develop semismooth newton base augment lagrangian method solv larg scale convex cluster problem extens numer experi simul real data demonstr algorithm highli effici robust solv larg scale problem moreov numer result also show superior perform scalabl algorithm compar exist first order method particular algorithm abl solv convex cluster problem point mathbb r minut less
388,1810.02670,"
        We provide an algorithm for computing the nucleolus for an instance of a weighted voting game in pseudo-polynomial time. This resolves an open question posed by Elkind. et.al. 2007.
        △ Less
",provid algorithm comput nucleolu instanc weight vote game pseudo polynomi time resolv open question pose elkind et al less
389,1810.02659,"
        Research Proposal in Automated Fix Detection
        △ Less
",research propos autom fix detect less
390,1810.02653,"
        Tactile sensing is essential to the human perception system, so as to robot. In this paper, we develop a novel optical-based tactile sensor ""FingerVision"" with effective signal processing algorithms. This sensor is composed of soft skin with embedded marker array bonded to rigid frame, and a web camera with a fisheye lens. While being excited with contact force, the camera tracks the movements of markers and deformation field is obtained. Compared to existing tactile sensors, our sensor features compact footprint, high resolution, and ease of fabrication. Besides, utilizing the deformation field estimation, we propose a slip classification framework based on convolution Long Short Term Memory (convolutional LSTM) networks. The data collection process takes advantage of the human sense of slip, during which human hand holds 12 daily objects, interacts with sensor skin and labels data with a slip or non-slip identity based on human feeling of slip. Our slip classification framework performs high accuracy of 97.62% on the test dataset. It is expected to be capable of enhancing the stability of robot grasping significantly, leading to better contact force control, finer object interaction and more active sensing manipulation.
        △ Less
",tactil sens essenti human percept system robot paper develop novel optic base tactil sensor fingervis effect signal process algorithm sensor compos soft skin embed marker array bond rigid frame web camera fishey len excit contact forc camera track movement marker deform field obtain compar exist tactil sensor sensor featur compact footprint high resolut eas fabric besid util deform field estim propos slip classif framework base convolut long short term memori convolut lstm network data collect process take advantag human sens slip human hand hold daili object interact sensor skin label data slip non slip ident base human feel slip slip classif framework perform high accuraci test dataset expect capabl enhanc stabil robot grasp significantli lead better contact forc control finer object interact activ sens manipul less
391,1810.02650,"
        International migration implies the coexistence of different ethnic and cultural groups in the receiving country. The refugee crisis of 2015 has resulted in critical levels of opinion polarization on the question of whether welcome migrants, causing clashes in the receiving countries. This scenario emphasizes the need to better understand the dynamics of mutual adaptation between locals and migrants, and the conditions that favor successful integration. Agent-based simulations can be useful to this goal. In this work, we introduce our agent-based model MigrAgent and preliminary results. The model synthetizes the dynamics of migration intake and post-migration adaptation. In detail, it explores the different acculturation outcomes that can emerge from the mutual adaptation of a migrant and a local population depending on their degree of tolerance. With parameter sweeping, we detect how different acculturation strategies can coexist in a society and at different degree for various subgroups. We compare these observations at different time points and in different conditions of speed intake. Results show higher polarization effects between a local population and migrants for fast intake conditions. In slow intake of migrants, transitory conditions between acculturation outcomes emerge for subgroups, e.g. from assimilation to integration for liberal migrants and from marginalization to separation for conservative migrants. Additionally, relative groups sizes due to speed of intake illustrate counterintuitive scenarios as separation of liberal locals. We qualitatively compare the processes of our model with SCIP survey in Germany, finding preliminary confirmation of our assumptions and results.
        △ Less
",intern migrat impli coexist differ ethnic cultur group receiv countri refuge crisi result critic level opinion polar question whether welcom migrant caus clash receiv countri scenario emphas need better understand dynam mutual adapt local migrant condit favor success integr agent base simul use goal work introduc agent base model migrag preliminari result model synthet dynam migrat intak post migrat adapt detail explor differ accultur outcom emerg mutual adapt migrant local popul depend degre toler paramet sweep detect differ accultur strategi coexist societi differ degre variou subgroup compar observ differ time point differ condit speed intak result show higher polar effect local popul migrant fast intak condit slow intak migrant transitori condit accultur outcom emerg subgroup e g assimil integr liber migrant margin separ conserv migrant addit rel group size due speed intak illustr counterintuit scenario separ liber local qualit compar process model scip survey germani find preliminari confirm assumpt result less
392,1810.02649,"
        Collaborative predictive blacklisting (CPB) allows to forecast future attack sources based on logs and alerts contributed by multiple organizations. Unfortunately, however, research on CPB has only focused on increasing the number of predicted attacks but has not considered the impact on false positives and false negatives. Moreover, sharing alerts is often hindered by confidentiality, trust, and liability issues, which motivates the need for privacy-preserving approaches to the problem. In this paper, we present a measurement study of state-of-the-art CPB techniques, aiming to shed light on the actual impact of collaboration. To this end, we reproduce and measure two systems: a non privacy-friendly one that uses a trusted coordinating party with access to all alerts (Soldo et al., 2010) and a peer-to-peer one using privacy-preserving data sharing (Freudiger et al., 2015). We show that, while collaboration boosts the number of predicted attacks, it also yields high false positives, ultimately leading to poor accuracy. This motivates us to present a hybrid approach, using a semi-trusted central entity, aiming to increase utility from collaboration while, at the same time, limiting information disclosure and false positives. This leads to a better trade-off of true and false positive rates, while at the same time addressing privacy concerns.
        △ Less
",collabor predict blacklist cpb allow forecast futur attack sourc base log alert contribut multipl organ unfortun howev research cpb focus increas number predict attack consid impact fals posit fals neg moreov share alert often hinder confidenti trust liabil issu motiv need privaci preserv approach problem paper present measur studi state art cpb techniqu aim shed light actual impact collabor end reproduc measur two system non privaci friendli one use trust coordin parti access alert soldo et al peer peer one use privaci preserv data share freudig et al show collabor boost number predict attack also yield high fals posit ultim lead poor accuraci motiv us present hybrid approach use semi trust central entiti aim increas util collabor time limit inform disclosur fals posit lead better trade true fals posit rate time address privaci concern less
393,1810.02648,"
        We present the first real-time human performance capture approach that reconstructs dense, space-time coherent deforming geometry of entire humans in general everyday clothing from just a single RGB video. We propose a novel two-stage analysis-by-synthesis optimization whose formulation and implementation are designed for high performance. In the first stage, a skinned template model is jointly fitted to background subtracted input video, 2D and 3D skeleton joint positions found using a deep neural network, and a set of sparse facial landmark detections. In the second stage, dense non-rigid 3D deformations of skin and even loose apparel are captured based on a novel real-time capable algorithm for non-rigid tracking using dense photometric and silhouette constraints. Our novel energy formulation leverages automatically identified material regions on the template to model the differing non-rigid deformation behavior of skin and apparel. The two resulting non-linear optimization problems per-frame are solved with specially-tailored data-parallel Gauss-Newton solvers. In order to achieve real-time performance of over 25Hz, we design a pipelined parallel architecture using the CPU and two commodity GPUs. Our method is the first real-time monocular approach for full-body performance capture. Our method yields comparable accuracy with off-line performance capture techniques, while being orders of magnitude faster.
        △ Less
",present first real time human perform captur approach reconstruct dens space time coher deform geometri entir human gener everyday cloth singl rgb video propos novel two stage analysi synthesi optim whose formul implement design high perform first stage skin templat model jointli fit background subtract input video skeleton joint posit found use deep neural network set spars facial landmark detect second stage dens non rigid deform skin even loos apparel captur base novel real time capabl algorithm non rigid track use dens photometr silhouett constraint novel energi formul leverag automat identifi materi region templat model differ non rigid deform behavior skin apparel two result non linear optim problem per frame solv special tailor data parallel gauss newton solver order achiev real time perform hz design pipelin parallel architectur use cpu two commod gpu method first real time monocular approach full bodi perform captur method yield compar accuraci line perform captur techniqu order magnitud faster less
394,1810.02647,"
        We describe a framework of hybrid cognition by formulating a hybrid cognitive agent that performs hierarchical active inference across a human and a machine part. We suggest that, in addition to enhancing human cognitive functions with an intelligent and adaptive interface, integrated cognitive processing could accelerate emergent properties within artificial intelligence. To establish this, a machine learning part learns to integrate into human cognition by explaining away multi-modal sensory measurements from the environment and physiology simultaneously with the brain signal. With ongoing training, the amount of predictable brain signal increases. This lends the agent the ability to self-supervise on increasingly high levels of cognitive processing in order to further minimize surprise in predicting the brain signal. Furthermore, with increasing level of integration, the access to sensory information about environment and physiology is substituted with access to their representation in the brain. While integrating into a joint embodiment of human and machine, human action and perception are treated as the machine's own. The framework can be implemented with invasive as well as non-invasive sensors for environment, body and brain interfacing. Online and offline training with different machine learning approaches are thinkable. Building on previous research on shared representation learning, we suggest a first implementation leading towards hybrid active inference with non-invasive brain interfacing and state of the art probabilistic deep learning methods. We further discuss how implementation might have effect on the meta-cognitive abilities of the described agent and suggest that with adequate implementation the machine part can continue to execute and build upon the learned cognitive processes autonomously.
        △ Less
",describ framework hybrid cognit formul hybrid cognit agent perform hierarch activ infer across human machin part suggest addit enhanc human cognit function intellig adapt interfac integr cognit process could acceler emerg properti within artifici intellig establish machin learn part learn integr human cognit explain away multi modal sensori measur environ physiolog simultan brain signal ongo train amount predict brain signal increas lend agent abil self supervis increasingli high level cognit process order minim surpris predict brain signal furthermor increas level integr access sensori inform environ physiolog substitut access represent brain integr joint embodi human machin human action percept treat machin framework implement invas well non invas sensor environ bodi brain interfac onlin offlin train differ machin learn approach thinkabl build previou research share represent learn suggest first implement lead toward hybrid activ infer non invas brain interfac state art probabilist deep learn method discuss implement might effect meta cognit abil describ agent suggest adequ implement machin part continu execut build upon learn cognit process autonom less
395,1810.02643,"
        Low resolution image enhancement is a classical computer vision problem. Selecting the best method to reconstruct an image to a higher resolution with the limited data available in the low-resolution image is quite a challenge. A major drawback from the existing enlargement techniques is the introduction of color bleeding while interpolating pixels over the edges that separate distinct colors in an image. The color bleeding causes to accentuate the edges with new colors as a result of blending multiple colors over adjacent regions. This paper proposes a novel approach to mitigate the color bleeding by segmenting the homogeneous color regions of the image using Simple Linear Iterative Clustering (SLIC) and applying a higher order interpolation technique separately on the isolated segments. The interpolation at the boundaries of each of the isolated segments is handled by using a morphological operation. The approach is evaluated by comparing against several frequently used image enlargement methods such as bilinear and bicubic interpolation by means of Peak Signal-to-Noise-Ratio (PSNR) value. The results obtained exhibit that the proposed method outperforms the baseline methods by means of PSNR and also mitigates the color bleeding at the edges which improves the overall appearance.
        △ Less
",low resolut imag enhanc classic comput vision problem select best method reconstruct imag higher resolut limit data avail low resolut imag quit challeng major drawback exist enlarg techniqu introduct color bleed interpol pixel edg separ distinct color imag color bleed caus accentu edg new color result blend multipl color adjac region paper propos novel approach mitig color bleed segment homogen color region imag use simpl linear iter cluster slic appli higher order interpol techniqu separ isol segment interpol boundari isol segment handl use morpholog oper approach evalu compar sever frequent use imag enlarg method bilinear bicub interpol mean peak signal nois ratio psnr valu result obtain exhibit propos method outperform baselin method mean psnr also mitig color bleed edg improv overal appear less
396,1810.02623,"
        With the development of real-time networks such as reactive embedded systems, there is a need to compute deterministic performance bounds. This paper focuses on the performance guarantees and stability conditions in networks with cyclic dependencies in the network calculus framework. We first propose an algorithm that computes tight backlog bounds in tree networks for any set of flows crossing a server. Then, we show how this algorithm can be applied to improve bounds from the literature fir any topology, including cyclic networks. In particular, we show that the ring is stable in the network calculus framework.
        △ Less
",develop real time network reactiv embed system need comput determinist perform bound paper focus perform guarante stabil condit network cyclic depend network calculu framework first propos algorithm comput tight backlog bound tree network set flow cross server show algorithm appli improv bound literatur fir topolog includ cyclic network particular show ring stabl network calculu framework less
397,1810.02620,"
        This paper proposes a computational methodology for the integration of Computer Aided Design (CAD) and the Finite Cell Method (FCM) for models with ""dirty geometries"". FCM, being a fictitious domain approach based on higher order finite elements, embeds the physical model into a fictitious domain, which can be discretized without having to take into account the boundary of the physical domain. The true geometry is captured by a precise numerical integration of elements cut by the boundary. Thus, an effective Point Membership Classification algorithm that determines the inside-outside state of an integration point with respect to the physical domain is a core operation in FCM. To treat also ""dirty geometries"", i.e. imprecise or flawed geometric models, a combination of a segment-triangle intersection algorithm and a flood fill algorithm being insensitive to most CAD model flaws is proposed to identify the affiliation of the integration points. The present method thus allows direct computations on geometrically and topologically flawed models. The potential and merit for practical applications of the proposed method is demonstrated by several numerical examples.
        △ Less
",paper propos comput methodolog integr comput aid design cad finit cell method fcm model dirti geometri fcm fictiti domain approach base higher order finit element emb physic model fictiti domain discret without take account boundari physic domain true geometri captur precis numer integr element cut boundari thu effect point membership classif algorithm determin insid outsid state integr point respect physic domain core oper fcm treat also dirti geometri e imprecis flaw geometr model combin segment triangl intersect algorithm flood fill algorithm insensit cad model flaw propos identifi affili integr point present method thu allow direct comput geometr topolog flaw model potenti merit practic applic propos method demonstr sever numer exampl less
398,1810.02614,"
        This paper demonstrates that word sense disambiguation (WSD) can improve neural machine translation (NMT) by widening the source context considered when modeling the senses of potentially ambiguous words. We first introduce three adaptive clustering algorithms for WSD, based on k-means, Chinese restaurant processes, and random walks, which are then applied to large word contexts represented in a low-rank space and evaluated on SemEval shared-task data. We then learn word vectors jointly with sense vectors defined by our best WSD method, within a state-of-the-art NMT system. We show that the concatenation of these vectors, and the use of a sense selection mechanism based on the weighted average of sense vectors, outperforms several baselines including sense-aware ones. This is demonstrated by translation on five language pairs. The improvements are above one BLEU point over strong NMT baselines, +4% accuracy over all ambiguous nouns and verbs, or +20% when scored manually over several challenging words.
        △ Less
",paper demonstr word sens disambigu wsd improv neural machin translat nmt widen sourc context consid model sens potenti ambigu word first introduc three adapt cluster algorithm wsd base k mean chines restaur process random walk appli larg word context repres low rank space evalu semev share task data learn word vector jointli sens vector defin best wsd method within state art nmt system show concaten vector use sens select mechan base weight averag sens vector outperform sever baselin includ sens awar one demonstr translat five languag pair improv one bleu point strong nmt baselin accuraci ambigu noun verb score manual sever challeng word less
399,1810.02612,"
        Linear temporal logic and automaton-based run-time verification provide a powerful framework for designing task and motion planning algorithms for autonomous agents. The drawback to this approach is the computational cost of operating on high resolution discrete abstractions of continuous dynamical systems. In particular, the computational bottleneck that arises is converting perceived environment variables into a labeling function on the states of a Kripke structure or analogously the transitions of a labeled transition system. This paper presents the design and empirical evaluation of an approach to constructing the labeling function that exposes a large degree of parallelism in the operation as well as efficient memory access patterns. The approach is implemented on a commodity GPU and empirical results demonstrate the efficacy of the labeling technique for real-time planning and decision-making.
        △ Less
",linear tempor logic automaton base run time verif provid power framework design task motion plan algorithm autonom agent drawback approach comput cost oper high resolut discret abstract continu dynam system particular comput bottleneck aris convert perceiv environ variabl label function state kripk structur analog transit label transit system paper present design empir evalu approach construct label function expos larg degre parallel oper well effici memori access pattern approach implement commod gpu empir result demonstr efficaci label techniqu real time plan decis make less
400,1810.02607,"
        Many types of anomaly detection methods have been proposed recently, and applied to a wide variety of fields including medical screening and production quality checking. Some methods have utilized images, and, in some cases, a part of the anomaly images is known beforehand. However, this kind of information is dismissed by previous methods, because the methods can only utilize a normal pattern. Moreover, the previous methods suffer a decrease in accuracy due to negative effects from surrounding noises. In this study, we propose a spatially-weighted anomaly detection method (SPADE) that utilizes all of the known patterns and lessens the vulnerability to ambient noises by applying Grad-CAM, which is the visualization method of a CNN. We evaluated our method quantitatively using two datasets, the MNIST dataset with noise and a dataset based on a brief screening test for dementia.
        △ Less
",mani type anomali detect method propos recent appli wide varieti field includ medic screen product qualiti check method util imag case part anomali imag known beforehand howev kind inform dismiss previou method method util normal pattern moreov previou method suffer decreas accuraci due neg effect surround nois studi propos spatial weight anomali detect method spade util known pattern lessen vulner ambient nois appli grad cam visual method cnn evalu method quantit use two dataset mnist dataset nois dataset base brief screen test dementia less
401,1810.02600,"
        Optical camera communication (OCC) exhibits considerable importance nowadays in various indoor camera based services such as smart home and robot-based automation. An android smart phone camera that is mounted on a mobile robot (MR) offers a uniform communication distance when the camera remains at the same level that can reduce the communication error rate. Indoor mobile robot navigation (MRN) is considered to be a promising OCC application in which the white light emitting diodes (LEDs) and an MR camera are used as transmitters and receiver respectively. Positioning is a key issue in MRN systems in terms of accuracy, data rate, and distance. We propose an indoor navigation and positioning combined algorithm and further evaluate its performance. An android application is developed to support data acquisition from multiple simultaneous transmitter links. Experimentally, we received data from four links which are required to ensure a higher positioning accuracy.
        △ Less
",optic camera commun occ exhibit consider import nowaday variou indoor camera base servic smart home robot base autom android smart phone camera mount mobil robot mr offer uniform commun distanc camera remain level reduc commun error rate indoor mobil robot navig mrn consid promis occ applic white light emit diod led mr camera use transmitt receiv respect posit key issu mrn system term accuraci data rate distanc propos indoor navig posit combin algorithm evalu perform android applic develop support data acquisit multipl simultan transmitt link experiment receiv data four link requir ensur higher posit accuraci less
402,1810.02599,"
        In the paper of Gohar M. Kyureghyan and Alexander Pott (Designs, Codes and Cryptography, 29, 149-164, 2003), the linear feedback polynomials of the Sidel'nikov-Lempel-Cohn-Eastman sequences were determined for some special cases. When referring to that paper, we found that Corollary 4 and Theorem 2 of that paper are wrong because there exist many counterexamples for these two results. In this note, we give some counterexamples of Corollary 4 and Theorem 2 of that paper.
        △ Less
",paper gohar kyureghyan alexand pott design code cryptographi linear feedback polynomi sidel nikov lempel cohn eastman sequenc determin special case refer paper found corollari theorem paper wrong exist mani counterexampl two result note give counterexampl corollari theorem paper less
403,1810.02597,"
        Communications based solely on radio frequency (RF) networks cannot provide adequate quality of service for the rapidly growing demands of wireless connectivity. Since devices operating in the optical spectrum do not interfere with those using the RF spectrum, wireless networks based on the optical spectrum can be added to existing RF networks to fulfill this demand. Hence, optical wireless communication (OWC) technology can be an excellent complement to RF-based technology to provide improved service. Promising OWC systems include light fidelity (LiFi), visible light communication, optical camera communication (OCC), and free-space optical communication (FSOC). OWC and RF systems have differing limitations, and the integration of RF and optical wireless networks can overcome the limitations of both systems. This paper describes an LiFi/femtocell hybrid network system for indoor environments. Low signal-to-interference-plus-noise ratios and the shortage bandwidth problems of existing RF femtocell networks can be overcome with the proposed hybrid model. Moreover, we describe an integrated RF/optical wireless system that can be employed for users inside a vehicle, remote monitoring of ambulance patients, vehicle tracking, and vehicle-to-vehicle communications. We consider LiFi, OCC, and FSOC as the optical wireless technologies to be used for communication support in transportation, and assume macrocells, femtocells, and wireless fidelity to be the corresponding RF technologies. We describe handover management-including detailed call flow, interference management, link reliability improvement, and group handover provisioning-for integrated networks. Performance analyses demonstrate the significance of the proposed integrated RF/optical wireless systems.
        △ Less
",commun base sole radio frequenc rf network cannot provid adequ qualiti servic rapidli grow demand wireless connect sinc devic oper optic spectrum interfer use rf spectrum wireless network base optic spectrum ad exist rf network fulfil demand henc optic wireless commun owc technolog excel complement rf base technolog provid improv servic promis owc system includ light fidel lifi visibl light commun optic camera commun occ free space optic commun fsoc owc rf system differ limit integr rf optic wireless network overcom limit system paper describ lifi femtocel hybrid network system indoor environ low signal interfer plu nois ratio shortag bandwidth problem exist rf femtocel network overcom propos hybrid model moreov describ integr rf optic wireless system employ user insid vehicl remot monitor ambul patient vehicl track vehicl vehicl commun consid lifi occ fsoc optic wireless technolog use commun support transport assum macrocel femtocel wireless fidel correspond rf technolog describ handov manag includ detail call flow interfer manag link reliabl improv group handov provis integr network perform analys demonstr signific propos integr rf optic wireless system less
404,1810.02596,"
        Cellular networks are constantly lagging in terms of the bandwidth needed to support the growing high data rate demands. The system needs to efficiently allocate its frequency spectrum such that the spectrum utilization can be maximized while ensuring the quality of service (QoS) level. Owing to the coexistence of different types of traffic (e.g., real-time (RT) and non-real-time (nRT)) and different types of networks (e.g., small cell and macrocell), ensuring the QoS level for different types of users becomes a challenging issue in wireless networks. Fractional frequency reuse (FFR) is an effective approach for increasing spectrum utilization and reducing interference effects in orthogonal frequency division multiple access networks. In this paper, we propose a new FFR scheme in which bandwidth allocation is based on RT/nRT traffic classification. We consider the coexistence of small cells and macrocells. After applying FFR technique in macrocells, the remaining frequency bands are efficiently allocated among the small cells overlaid by a macrocell. In our proposed scheme, total frequency-band allocations for different macrocells are decided on the basis of the traffic intensity. The transmitted power levels for different frequency bands are controlled based on the level of interference from a nearby frequency band. Frequency bands with a lower level of interference are assigned to the RT traffic to ensure a higher QoS level for the RT traffic. RT traffic calls in macrocell networks are also given a higher priority compared with nRT traffic calls to ensure the low call-blocking rate. Performance analyses show significant improvement under the proposed scheme compared with conventional FFR schemes.
        △ Less
",cellular network constantli lag term bandwidth need support grow high data rate demand system need effici alloc frequenc spectrum spectrum util maxim ensur qualiti servic qo level owe coexist differ type traffic e g real time rt non real time nrt differ type network e g small cell macrocel ensur qo level differ type user becom challeng issu wireless network fraction frequenc reus ffr effect approach increas spectrum util reduc interfer effect orthogon frequenc divis multipl access network paper propos new ffr scheme bandwidth alloc base rt nrt traffic classif consid coexist small cell macrocel appli ffr techniqu macrocel remain frequenc band effici alloc among small cell overlaid macrocel propos scheme total frequenc band alloc differ macrocel decid basi traffic intens transmit power level differ frequenc band control base level interfer nearbi frequenc band frequenc band lower level interfer assign rt traffic ensur higher qo level rt traffic rt traffic call macrocel network also given higher prioriti compar nrt traffic call ensur low call block rate perform analys show signific improv propos scheme compar convent ffr scheme less
405,1810.02594,"
        New high-data-rate multimedia services and applications are evolving continuously and exponentially increasing the demand for wireless capacity of fifth-generation (5G) and beyond. The existing radio frequency (RF) communication spectrum is insufficient to meet the demands of future high-datarate 5G services. Optical wireless communication (OWC), which uses an ultra-wide range of unregulated spectrum, has emerged as a promising solution to overcome the RF spectrum crisis. It has attracted growing research interest worldwide in the last decade for indoor and outdoor applications. OWC offloads huge data traffic applications from RF networks. A 100 Gb/s data rate has already been demonstrated through OWC. It offers services indoors as well as outdoors, and communication distances range from several nm to more than 10000 km. This paper provides a technology overview and a review on optical wireless technologies, such as visible light communication, light fidelity, optical camera communication, free space optical communication, and light detection and ranging. We survey the key technologies for understanding OWC and present state-of-the-art criteria in aspects, such as classification, spectrum use, architecture, and applications. The key contribution of this paper is to clarify the differences among different promising optical wireless technologies and between these technologies and their corresponding similar existing RF technologies
        △ Less
",new high data rate multimedia servic applic evolv continu exponenti increas demand wireless capac fifth gener g beyond exist radio frequenc rf commun spectrum insuffici meet demand futur high datar g servic optic wireless commun owc use ultra wide rang unregul spectrum emerg promis solut overcom rf spectrum crisi attract grow research interest worldwid last decad indoor outdoor applic owc offload huge data traffic applic rf network gb data rate alreadi demonstr owc offer servic indoor well outdoor commun distanc rang sever nm km paper provid technolog overview review optic wireless technolog visibl light commun light fidel optic camera commun free space optic commun light detect rang survey key technolog understand owc present state art criteria aspect classif spectrum use architectur applic key contribut paper clarifi differ among differ promis optic wireless technolog technolog correspond similar exist rf technolog less
406,1810.02592,"
        For the current 5G era, the deployment of small cells in residential and commercial areas plays an imperious preamble in improving network coverage and the quality of service (QoS). Major technical problems associated with the mass deployment of small cells, such as femtocells are interference management and QoS provisioning. These are important for service-providing operators because the system capacity and achievable data rates mainly depend on interference. Future generation wireless networks will use autonomous and distributed architecture for ameliorating the efficacy and flexibility of communication systems. In this paper, we propose a game theory-based model along with dynamic channel allocation and self-optimizing power control scheme for resolving priority-based access exposure by applying the concept of primary and secondary users. It is expected that the consumers will experience better QoS with reduced interference levels, and the service-providing operators will be able to increase their revenue, while ensuring optimal price for the consumers. We assimilated extensive numerical results to demonstrate the efficacy of our proposed model.
        △ Less
",current g era deploy small cell residenti commerci area play imperi preambl improv network coverag qualiti servic qo major technic problem associ mass deploy small cell femtocel interfer manag qo provis import servic provid oper system capac achiev data rate mainli depend interfer futur gener wireless network use autonom distribut architectur amelior efficaci flexibl commun system paper propos game theori base model along dynam channel alloc self optim power control scheme resolv prioriti base access exposur appli concept primari secondari user expect consum experi better qo reduc interfer level servic provid oper abl increas revenu ensur optim price consum assimil extens numer result demonstr efficaci propos model less
407,1810.02589,"
        The demand for autonomous vehicles is increasing gradually owing to their enormous potential benefits. However, several challenges, such as vehicle localization, are involved in the development of autonomous vehicles. A simple and secure algorithm for vehicle positioning is proposed herein without massively modifying the existing transportation infrastructure. For vehicle localization, vehicles on the road are classified into two categories: host vehicles (HVs) are the ones used to estimate other vehicles' positions and forwarding vehicles (FVs) are the ones that move in front of the HVs. The FV transmits modulated data from the tail (or back) light, and the camera of the HV receives that signal using optical camera communication (OCC). In addition, the streetlight (SL) data are considered to ensure the position accuracy of the HV. Determining the HV position minimizes the relative position variation between the HV and FV. Using photogrammetry, the distance between FV or SL and the camera of the HV is calculated by measuring the occupied image area on the image sensor. Comparing the change in distance between HV and SLs with the change in distance between HV and FV, the positions of FVs are determined. The performance of the proposed technique is analyzed, and the results indicate a significant improvement in performance. The experimental distance measurement validated the feasibility of the proposed scheme.
        △ Less
",demand autonom vehicl increas gradual owe enorm potenti benefit howev sever challeng vehicl local involv develop autonom vehicl simpl secur algorithm vehicl posit propos herein without massiv modifi exist transport infrastructur vehicl local vehicl road classifi two categori host vehicl hv one use estim vehicl posit forward vehicl fv one move front hv fv transmit modul data tail back light camera hv receiv signal use optic camera commun occ addit streetlight sl data consid ensur posit accuraci hv determin hv posit minim rel posit variat hv fv use photogrammetri distanc fv sl camera hv calcul measur occupi imag area imag sensor compar chang distanc hv sl chang distanc hv fv posit fv determin perform propos techniqu analyz result indic signific improv perform experiment distanc measur valid feasibl propos scheme less
408,1810.02583,"
        In this work, we propose a self-supervised learning method for affine image registration on 3D medical images. Unlike optimisation-based methods, our affine image registration network (AIRNet) is designed to directly estimate the transformation parameters between two input images without using any metric, which represents the quality of the registration, as the optimising function. But since it is costly to manually identify the transformation parameters between any two images, we leverage the abundance of cheap unlabelled data to generate a synthetic dataset for the training of the model. Additionally, the structure of AIRNet enables us to learn the discriminative features of the images which are useful for registration purpose. Our proposed method was evaluated on magnetic resonance images of the axial view of human brain and compared with the performance of a conventional image registration method. Experiments demonstrate that our approach achieves better overall performance on registration of images from different patients and modalities with 100x speed-up in execution time.
        △ Less
",work propos self supervis learn method affin imag registr medic imag unlik optimis base method affin imag registr network airnet design directli estim transform paramet two input imag without use metric repres qualiti registr optimis function sinc costli manual identifi transform paramet two imag leverag abund cheap unlabel data gener synthet dataset train model addit structur airnet enabl us learn discrimin featur imag use registr purpos propos method evalu magnet reson imag axial view human brain compar perform convent imag registr method experi demonstr approach achiev better overal perform registr imag differ patient modal x speed execut time less
409,1810.02582,"
        With the proliferation of mobile phone users, interference management is a big concern in this neoteric years. To cope with this problem along with ensuring better Quality of Service (QoS), femtocell plays an imperious preamble in heterogeneous networks (HetNets) for some of its noteworthy characteristics. In this paper, we propose a game theoretic algorithm along with dynamic channel allocation and hybrid access mechanism with self-organizing power control scheme. With a view to resolving prioritized access issue, the concept of primary and secondary users is applied. Existence of pure strategy Nash equilibrium (NE) has been investigated and come to a perfection that our proposed scheme can be adopted both increasing capacity and increasing revenue of operators considering optimal price for consumers.
        △ Less
",prolifer mobil phone user interfer manag big concern neoter year cope problem along ensur better qualiti servic qo femtocel play imperi preambl heterogen network hetnet noteworthi characterist paper propos game theoret algorithm along dynam channel alloc hybrid access mechan self organ power control scheme view resolv priorit access issu concept primari secondari user appli exist pure strategi nash equilibrium ne investig come perfect propos scheme adopt increas capac increas revenu oper consid optim price consum less
410,1810.02579,"
        As the web expands in data volume and in geographical distribution, centralized search methods become inefficient, leading to increasing interest in cooperative information retrieval, e.g., federated text retrieval (FTR). Different from existing centralized information retrieval (IR) methods, in which search is done on a logically centralized document collection, FTR is composed of a number of peers, each of which is a complete search engine by itself. To process a query, FTR requires firstly the identification of promising peers that host the relevant documents and secondly the retrieval of the most relevant documents from the selected peers. Most of the existing methods only apply traditional IR techniques that treat each text collection as a single large document and utilize term matching to rank the collections. In this paper, we formalize the problem and identify the properties of FTR, and analyze the feasibility of extending LSI with clustering to adapt to FTR, based on which a novel approach called Cluster-based Distributed Latent Semantic Indexing (C-DLSI) is proposed. C-DLSI distinguishes the topics of a peer with clustering, captures the local LSI spaces within the clusters, and consider the relations among these LSI spaces, thus providing more precise characterization of the peer. Accordingly, novel descriptors of the peers and a compatible local text retrieval are proposed. The experimental results show that C-DLSI outperforms existing methods.
        △ Less
",web expand data volum geograph distribut central search method becom ineffici lead increas interest cooper inform retriev e g feder text retriev ftr differ exist central inform retriev ir method search done logic central document collect ftr compos number peer complet search engin process queri ftr requir firstli identif promis peer host relev document secondli retriev relev document select peer exist method appli tradit ir techniqu treat text collect singl larg document util term match rank collect paper formal problem identifi properti ftr analyz feasibl extend lsi cluster adapt ftr base novel approach call cluster base distribut latent semant index c dlsi propos c dlsi distinguish topic peer cluster captur local lsi space within cluster consid relat among lsi space thu provid precis character peer accordingli novel descriptor peer compat local text retriev propos experiment result show c dlsi outperform exist method less
411,1810.02575,"
        This work addresses the problem of semantic image segmentation of nighttime scenes. Although considerable progress has been made in semantic image segmentation, it is mainly related to daytime scenarios. This paper proposes a novel method to progressive adapt the semantic models trained on daytime scenes, along with large-scale annotations therein, to nighttime scenes via the bridge of twilight time -- the time between dawn and sunrise, or between sunset and dusk. The goal of the method is to alleviate the cost of human annotation for nighttime images by transferring knowledge from standard daytime conditions. In addition to the method, a new dataset of road scenes is compiled; it consists of 35,000 images ranging from daytime to twilight time and to nighttime. Also, a subset of the nighttime images are densely annotated for method evaluation. Our experiments show that our method is effective for model adaptation from daytime scenes to nighttime scenes, without using extra human annotation.
        △ Less
",work address problem semant imag segment nighttim scene although consider progress made semant imag segment mainli relat daytim scenario paper propos novel method progress adapt semant model train daytim scene along larg scale annot therein nighttim scene via bridg twilight time time dawn sunris sunset dusk goal method allevi cost human annot nighttim imag transfer knowledg standard daytim condit addit method new dataset road scene compil consist imag rang daytim twilight time nighttim also subset nighttim imag dens annot method evalu experi show method effect model adapt daytim scene nighttim scene without use extra human annot less
412,1810.02572,"
        Wireless networks employing small cells like femtocells are considered to be the choice of network deployment for 4G or advanced networks. This hierarchical deployment of cells introduces the necessity of effective frequency planning for mitigation of interference between different layers of network. As the scarce spectrum resources are likely to be reused to increase spectral efficiency, interference free signal reception has to be guaranteed to ensure better quality of service (QoS). In this paper we propose a dynamic frequency reuse scheme for the deployment of femtocells within a macrocell with the femtocells reusing the spectrum of neighbouring macrocells. We also provide a protective scheme for cell edge femtocell users as they are vulnerable to interference signals from neighbouring macrocells. A detailed frequency planning is provided to maximize spectral reuse while providing maximum throughput. We compare our proposed scheme with other frequency allocation schemes already described in literature. Simulation results shows that our scheme provides better throughput and ensures lower outage probability.
        △ Less
",wireless network employ small cell like femtocel consid choic network deploy g advanc network hierarch deploy cell introduc necess effect frequenc plan mitig interfer differ layer network scarc spectrum resourc like reus increas spectral effici interfer free signal recept guarante ensur better qualiti servic qo paper propos dynam frequenc reus scheme deploy femtocel within macrocel femtocel reus spectrum neighbour macrocel also provid protect scheme cell edg femtocel user vulner interfer signal neighbour macrocel detail frequenc plan provid maxim spectral reus provid maximum throughput compar propos scheme frequenc alloc scheme alreadi describ literatur simul result show scheme provid better throughput ensur lower outag probabl less
413,1810.02570,"
        The ubiquitous services of wireless communication networks are growing rapidly by the development of wireless communication technologies. While a user is roaming from one cell to another cell, an intelligent decision mechanism and network selection is extremely crucial to maintain the quality of service (QoS) during handover. Handover decision must be made precisely to avoid any unnecessary phenomenon like ping-pong, corner effect, call blocking, and call dropping etc. This work considered service types like voice, video, and data and their QoS requirements for handover decision using fuzzy logic in heterogeneous network environment. Service is an important factor for the users and particular service requires respective QoS. This paper provides all the cases of handover decisions between macrocell and femtocell networks considering service type. The proposed system models regarding these handover decisions using fuzzy logic considering several input parameters e.g. received signal strength indicator (RSSI), data rate, user's velocity, and interference level (signal-to-noise plus interference ratio) to make handover from femtocell to macrocell, macrocell to femtocell or femtocell to femtocell. The performance of different parameters are shown based on service type are analyzed.
        △ Less
",ubiquit servic wireless commun network grow rapidli develop wireless commun technolog user roam one cell anoth cell intellig decis mechan network select extrem crucial maintain qualiti servic qo handov handov decis must made precis avoid unnecessari phenomenon like ping pong corner effect call block call drop etc work consid servic type like voic video data qo requir handov decis use fuzzi logic heterogen network environ servic import factor user particular servic requir respect qo paper provid case handov decis macrocel femtocel network consid servic type propos system model regard handov decis use fuzzi logic consid sever input paramet e g receiv signal strength indic rssi data rate user veloc interfer level signal nois plu interfer ratio make handov femtocel macrocel macrocel femtocel femtocel femtocel perform differ paramet shown base servic type analyz less
414,1810.02569,"
        We propose a method for the weakly supervised detection of objects in paintings. At training time, only image-level annotations are needed. This, combined with the efficiency of our multiple-instance learning method, enables one to learn new classes on-the-fly from globally annotated databases, avoiding the tedious task of manually marking objects. We show on several databases that dropping the instance-level annotations only yields mild performance losses. We also introduce a new database, IconArt, on which we perform detection experiments on classes that could not be learned on photographs, such as Jesus Child or Saint Sebastian. To the best of our knowledge, these are the first experiments dealing with the automatic (and in our case weakly supervised) detection of iconographic elements in paintings. We believe that such a method is of great benefit for helping art historians to explore large digital databases.
        △ Less
",propos method weakli supervis detect object paint train time imag level annot need combin effici multipl instanc learn method enabl one learn new class fli global annot databas avoid tediou task manual mark object show sever databas drop instanc level annot yield mild perform loss also introduc new databas iconart perform detect experi class could learn photograph jesu child saint sebastian best knowledg first experi deal automat case weakli supervis detect iconograph element paint believ method great benefit help art historian explor larg digit databas less
415,1810.02561,"
        GPdoemd is an open-source python package for design of experiments for model discrimination that uses Gaussian process surrogate models to approximate and maximise the divergence between marginal predictive distributions of rival mechanistic models. GPdoemd uses the divergence prediction to suggest a maximally informative next experiment.
        △ Less
",gpdoemd open sourc python packag design experi model discrimin use gaussian process surrog model approxim maximis diverg margin predict distribut rival mechanist model gpdoemd use diverg predict suggest maxim inform next experi less
416,1810.02559,"
        The energy efficiency aspect of cellular networks is a vital topic of research over the recent days. As energy consumption is on the rise and the price of electricity is increasing very rapidly, necessity to reduce electricity usage in every aspect is becoming much more significant. The power grid infrastructure, from which the cellular networks attain the required electricity for operation is considering a significant change from the traditional electricity grid to the smart grid. The base stations, which are the main candidates for energy consumption in cellular networks, remain in operation even when there are a very few users or no user at all. In this paper, we propose an effective application scenario of the femtocell technology for power saving purpose of cellular networks. Effective deployment of femto-access-points in the required places can reduce the power usage of cellular networks by turning off the redundant base stations. Moreover, the signal to noise plus interference ratio is also enhanced in comparison to the traditional scheme. The base station turn off probability and the energy consumption in the overall network are analyzed. The comparison results show that the proposed energy efficient model can significantly reduce energy consumption and increase the service quality of the cellular users.
        △ Less
",energi effici aspect cellular network vital topic research recent day energi consumpt rise price electr increas rapidli necess reduc electr usag everi aspect becom much signific power grid infrastructur cellular network attain requir electr oper consid signific chang tradit electr grid smart grid base station main candid energi consumpt cellular network remain oper even user user paper propos effect applic scenario femtocel technolog power save purpos cellular network effect deploy femto access point requir place reduc power usag cellular network turn redund base station moreov signal nois plu interfer ratio also enhanc comparison tradit scheme base station turn probabl energi consumpt overal network analyz comparison result show propos energi effici model significantli reduc energi consumpt increas servic qualiti cellular user less
417,1810.02558,"
        We consider a scenario in which a DoS attacker with the limited power resource jams a wireless network through which the packet from a sensor is sent to a remote estimator to estimate the system state. To degrade the estimation quality with power constraint, the attacker aims to solve how much power to obstruct the channel each time, which is the recently proposed optimal attack energy management problem. The existing works are built on an ideal link model in which the packet dropout never occurs without attack. To encompass wireless transmission losses, we introduce the SINR-based link. First, we focus on the case when the attacker employs the constant power level. To maximize the terminal error at the remote estimator, we provide some sufficient conditions for the existence of an explicit solution to the optimal static attack energy management problem and the solution is constructed. Compared with the existing result in which corresponding sufficient conditions work only when the system matrix is normal, the obtained conditions in this paper are viable for a general system and shown to be more relaxed. For the other system index, the average error, the associated sufficient conditions are also derived based on different analysis with the existing work. And a feasible method is presented for both indexes when the system cannot meet the sufficient conditions. Then when the real-time ACK information can be acquired, an MDP based algorithm is designed to solve the optimal dynamic attack energy management problem. We further study the optimal tradeoff between attack power and system degradation. By moving power constraint into the objective function to maximize system index and minimize energy consumption, the other MDP based algorithm is proposed to find the optimal attack policy which is further shown to have a monotone structure. The theoretical results are illustrated by simulations.
        △ Less
",consid scenario do attack limit power resourc jam wireless network packet sensor sent remot estim estim system state degrad estim qualiti power constraint attack aim solv much power obstruct channel time recent propos optim attack energi manag problem exist work built ideal link model packet dropout never occur without attack encompass wireless transmiss loss introduc sinr base link first focu case attack employ constant power level maxim termin error remot estim provid suffici condit exist explicit solut optim static attack energi manag problem solut construct compar exist result correspond suffici condit work system matrix normal obtain condit paper viabl gener system shown relax system index averag error associ suffici condit also deriv base differ analysi exist work feasibl method present index system cannot meet suffici condit real time ack inform acquir mdp base algorithm design solv optim dynam attack energi manag problem studi optim tradeoff attack power system degrad move power constraint object function maxim system index minim energi consumpt mdp base algorithm propos find optim attack polici shown monoton structur theoret result illustr simul less
418,1810.02557,"
        Technological advancement has brought revolutionary change in the converged wireless networks. Due to the existence of different types of traffic, provisioning of Quality of Service (QoS) becomes a challenge in the wireless networks. In case of a congested network, resource allocation has emerged as an effective way to provide the excessive users with desirable QoS. Since QoS for non-real-time traffic are not as strict as for real-time traffic, the unoccupied channels of the adjacent cells can be assigned to the non-real-time traffic to retain QoS for real-time traffic. This results in the intensified bandwidth utilization as well as less interference for the real-time traffic. In this paper, we propose an effective radio resource management scheme that relies on the dynamically assigned bandwidth allocation process. In case of interference management, we classify the traffic into real-time traffic and non-real-time traffic and give priority to the real-time traffic. According to our scheme, the real-time traffic among the excessive number of users are reassigned to the original channels which have been occupied by non-real-time traffic and the non-real-time traffic are allocated to the assigned channels of those real-time traffic. The architecture allows improved signal to interference plus noise ratio (SINR) for real-time traffic along with intensification in the bandwidth utilization of the network. Besides, the increased system capacity and lower outage probability of the network bear the significance of the proposed scheme.
        △ Less
",technolog advanc brought revolutionari chang converg wireless network due exist differ type traffic provis qualiti servic qo becom challeng wireless network case congest network resourc alloc emerg effect way provid excess user desir qo sinc qo non real time traffic strict real time traffic unoccupi channel adjac cell assign non real time traffic retain qo real time traffic result intensifi bandwidth util well less interfer real time traffic paper propos effect radio resourc manag scheme reli dynam assign bandwidth alloc process case interfer manag classifi traffic real time traffic non real time traffic give prioriti real time traffic accord scheme real time traffic among excess number user reassign origin channel occupi non real time traffic non real time traffic alloc assign channel real time traffic architectur allow improv signal interfer plu nois ratio sinr real time traffic along intensif bandwidth util network besid increas system capac lower outag probabl network bear signific propos scheme less
419,1810.02555,"
        Stochastic optimization techniques are standard in variational inference algorithms. These methods estimate gradients by approximating expectations with independent Monte Carlo samples. In this paper, we explore a technique that uses correlated, but more representative , samples to reduce estimator variance. Specifically, we show how to generate antithetic samples that match sample moments with the true moments of an underlying importance distribution. Combining a differentiable antithetic sampler with modern stochastic variational inference, we showcase the effectiveness of this approach for learning a deep generative model.
        △ Less
",stochast optim techniqu standard variat infer algorithm method estim gradient approxim expect independ mont carlo sampl paper explor techniqu use correl repres sampl reduc estim varianc specif show gener antithet sampl match sampl moment true moment underli import distribut combin differenti antithet sampler modern stochast variat infer showcas effect approach learn deep gener model less
420,1810.02553,"
        The availability of different paths to communicate to a user or device introduces several benefits, from boosting enduser performance to improving network utilization. Hybrid access is a first step in enabling convergence of mobile and fixed networks, however, despite traffic optimization, this approach is limited as fixed and mobile are still two separate core networks inter-connected through an aggregation point. On the road to 5G networks, the design trend is moving towards an aggregated network, where different access technologies share a common anchor point in the core. This enables further network optimization in addition to hybrid access, examples are userspecific policies for aggregation and improved traffic balancing across different accesses according to user, network, and service context. This paper aims to discuss the ongoing work around hybrid access and network convergence by Broadband Forum and 3GPP. We present some testbed results on hybrid access and analyze some primary performance indicators such as achievable data rates, link utilization for aggregated traffic and session setup latency. We finally discuss the future directions for network convergence to enable future scenarios with enhanced configuration capabilities for fixed and mobile convergence.
        △ Less
",avail differ path commun user devic introduc sever benefit boost endus perform improv network util hybrid access first step enabl converg mobil fix network howev despit traffic optim approach limit fix mobil still two separ core network inter connect aggreg point road g network design trend move toward aggreg network differ access technolog share common anchor point core enabl network optim addit hybrid access exampl userspecif polici aggreg improv traffic balanc across differ access accord user network servic context paper aim discuss ongo work around hybrid access network converg broadband forum gpp present testb result hybrid access analyz primari perform indic achiev data rate link util aggreg traffic session setup latenc final discuss futur direct network converg enabl futur scenario enhanc configur capabl fix mobil converg less
421,1810.02552,"
        To ensure the maximum utilization of the limited bandwidth resources and improved quality of service (QoS) is the key issue for wireless communication networks. Excessive call blocking is a constraint to attain the desired QoS. In cellular network, as the traffic arrival rate increases, call blocking probability (CBP) increases considerably. Paying profound concern, we proposed a scheme that reduces the call blocking probability with approximately steady call dropping probability (CDP). Our proposed scheme also introduces the acceptance factor in specific guard channel where originating calls get access according to the acceptance factor. The analytical performance proves better performance than the conventional new-call bounding scheme in case of higher and lower traffic arrival rate.
        △ Less
",ensur maximum util limit bandwidth resourc improv qualiti servic qo key issu wireless commun network excess call block constraint attain desir qo cellular network traffic arriv rate increas call block probabl cbp increas consider pay profound concern propos scheme reduc call block probabl approxim steadi call drop probabl cdp propos scheme also introduc accept factor specif guard channel origin call get access accord accept factor analyt perform prove better perform convent new call bound scheme case higher lower traffic arriv rate less
422,1810.02550,"
        In this paper, a dynamic channel assigning along with dynamic cell sectoring model has been proposed that focuses on the Fractional Frequency Reuse (FFR) not only for interference mitigation but also for enhancement of overall system capacity in wireless networks. We partition the cells in a cluster into two part named centre user part (CUP) and edge user part (EUP). Instead of huge traffic, there may be unoccupied channels in the EUPs of the cells. These unoccupied channels of the EUPs can assist the excessive number of users if these channels are assigned with proper interference management. If the number of traffic of a cell surpasses the number of channels of the EUP, then the cell assigns the channels from the EUP of other cells in the cluster. To alleviate the interference, we propose a dynamic cell sectoring scheme. The scheme sectors the EUP of the cell which assigns channels that the assigned channels are provided to the sectored part where these channels receive negligible interference. The performance analysis illustrates reduced call blocking probability as well as better signal to interference plus noise ratio (SINR) without sacrificing bandwidth utilization. Besides, the proposed model ensures lower outage probability.
        △ Less
",paper dynam channel assign along dynam cell sector model propos focus fraction frequenc reus ffr interfer mitig also enhanc overal system capac wireless network partit cell cluster two part name centr user part cup edg user part eup instead huge traffic may unoccupi channel eup cell unoccupi channel eup assist excess number user channel assign proper interfer manag number traffic cell surpass number channel eup cell assign channel eup cell cluster allevi interfer propos dynam cell sector scheme scheme sector eup cell assign channel assign channel provid sector part channel receiv neglig interfer perform analysi illustr reduc call block probabl well better signal interfer plu nois ratio sinr without sacrif bandwidth util besid propos model ensur lower outag probabl less
423,1810.02542,"
        In modern days, users in the wireless networks are increasing drastically. It has become the major concern for researchers to manage the maximum users with limited radio resource. Interference is one of the biggest hindrances to reach the goal. In this paper, being deep apprehension of the issue, an efficient dynamic channel borrowing scheme is proposed that ensures better Quality of Service (QoS) with interference declination. We propose that if channels are borrowed from adjacent cells, cell bifurcation will be introduced that ensures interference declination when the borrowed channels have same frequency band. We also propose a scheme that inactivates the unoccupied interfering channels of adjacent cells, instead of cell bifurcation for interference declination. The simulation outcomes show acceptable performances in terms of SINR level, system capacity, and outage probability compared to conventional scheme without interference declination that may attract the considerable interest for the users.
        △ Less
",modern day user wireless network increas drastic becom major concern research manag maximum user limit radio resourc interfer one biggest hindranc reach goal paper deep apprehens issu effici dynam channel borrow scheme propos ensur better qualiti servic qo interfer declin propos channel borrow adjac cell cell bifurc introduc ensur interfer declin borrow channel frequenc band also propos scheme inactiv unoccupi interf channel adjac cell instead cell bifurc interfer declin simul outcom show accept perform term sinr level system capac outag probabl compar convent scheme without interfer declin may attract consider interest user less
424,1810.02541,"
        Proximal Policy Optimization (PPO) is a highly popular model-free reinforcement learning (RL) approach. However, in continuous state and actions spaces and a Gaussian policy -- common in computer animation and robotics -- PPO is prone to getting stuck in local optima. In this paper, we observe a tendency of PPO to prematurely shrink the exploration variance, which naturally leads to slow progress. Motivated by this, we borrow ideas from CMA-ES, a black-box optimization method designed for intelligent adaptive Gaussian exploration, to derive PPO-CMA, a novel proximal policy optimization approach that can expand the exploration variance on objective function slopes and shrink the variance when close to the optimum. This is implemented by using separate neural networks for policy mean and variance and training the mean and variance in separate passes. Our experiments demonstrate a clear improvement over vanilla PPO in many difficult OpenAI Gym MuJoCo tasks.
        △ Less
",proxim polici optim ppo highli popular model free reinforc learn rl approach howev continu state action space gaussian polici common comput anim robot ppo prone get stuck local optima paper observ tendenc ppo prematur shrink explor varianc natur lead slow progress motiv borrow idea cma es black box optim method design intellig adapt gaussian explor deriv ppo cma novel proxim polici optim approach expand explor varianc object function slope shrink varianc close optimum implement use separ neural network polici mean varianc train mean varianc separ pass experi demonstr clear improv vanilla ppo mani difficult openai gym mujoco task less
425,1810.02539,"
        Provisioning of Quality of Service (QoS) is the key concern for Radio Resource Management now-a-days. In this paper, an efficient dynamic channel borrowing architecture has been proposed that ensures better QoS. The proposed scheme lessens the problem of excessive overall call blocking probability without sacrificing bandwidth utilization. If a channel is borrowed from adjacent cells and causing interference, we also propose architecture that diminishes the interference problem. The numerical results show comparison between the proposed scheme and the conventional scheme before channel borrowing process. The results show a satisfactory performance that are in favor of the proposed scheme, in case of overall call blocking probability, bandwidth utilization and interference management.
        △ Less
",provis qualiti servic qo key concern radio resourc manag day paper effici dynam channel borrow architectur propos ensur better qo propos scheme lessen problem excess overal call block probabl without sacrif bandwidth util channel borrow adjac cell caus interfer also propos architectur diminish interfer problem numer result show comparison propos scheme convent scheme channel borrow process result show satisfactori perform favor propos scheme case overal call block probabl bandwidth util interfer manag less
426,1810.02537,"
        The femto-access-point (FAP), a low power small cellular base station provides better signal quality for the indoor users as to provide high data-rate communications with improved coverage, access network capacity and quality of service. Highly dense femtocellular deployments-the ultimate goal of the femtocellular technology-will require significant degree of selforganization in lieu of manual configuration as to mitigate unwanted handovers and interference effects. The deployment of femtocells with our proposed on-demand scheme in the multistoried building or home environment can solve the low level signal-to-noise plus interference ratio and throughput problems. In this paper, we propose new application of the femtocell technology with a new idea of femto-idle-mode and femto-active-mode system. The signal-to-noise plus interference ratio level and throughput are analyzed. The simulation results show that the proposed on-demand scheme in femtocell deployment significantly enhances the signal-to-noise plus interference ratio level and throughput in the multistoried building or home environment.
        △ Less
",femto access point fap low power small cellular base station provid better signal qualiti indoor user provid high data rate commun improv coverag access network capac qualiti servic highli dens femtocellular deploy ultim goal femtocellular technolog requir signific degre selforgan lieu manual configur mitig unwant handov interfer effect deploy femtocel propos demand scheme multistori build home environ solv low level signal nois plu interfer ratio throughput problem paper propos new applic femtocel technolog new idea femto idl mode femto activ mode system signal nois plu interfer ratio level throughput analyz simul result show propos demand scheme femtocel deploy significantli enhanc signal nois plu interfer ratio level throughput multistori build home environ less
427,1810.02536,"
        It has been suggested that direct reciprocity operates well within small groups of people where it would be hard to get away with cheating one another but no research has been done yet to show how exactly the mechanism of direct reciprocity fails to operate as the group size increases. Unlike previous models that have neglected the role of memory, our model takes into account the memory capacity of the agents as well as the cost of having such memory. We have shown that the optimal memory capacity for handling the exploiters grows with the group size in a similar way as the relative size of the neocortex grows with the group size of the primates as it was found by Robin Dunbar. The time required for reaching the relative fitness of the defectors increases rapidly with the group size which points to the conclusion that there is an upper group size limit over which the mechanism of direct reciprocity is insufficient to maintain the cooperation.
        △ Less
",suggest direct reciproc oper well within small group peopl would hard get away cheat one anoth research done yet show exactli mechan direct reciproc fail oper group size increas unlik previou model neglect role memori model take account memori capac agent well cost memori shown optim memori capac handl exploit grow group size similar way rel size neocortex grow group size primat found robin dunbar time requir reach rel fit defector increas rapidli group size point conclus upper group size limit mechan direct reciproc insuffici maintain cooper less
428,1810.02535,"
        We consider an energy harvesting (EH) multi-antenna relay based cooperative cognitive radio network (CCRN), and investigate its outage and throughput performance with the peak-interference type of power control for both time-switching (TS) and power-splitting (PS) protocols. We assume that the relay uses maximum ratio combining and transmit antenna selection in the first and second hop respectively. Unlike other literature on this topic, we optimally combine the direct and relayed signals at the destination, and demonstrate that due to random nature of the channels and the power control used, there is considerable improvement in performance. We also analyze the performance of EH-CCRN with incremental relaying, and { analytically quantify the gain in throughput {over} EH-CCRN}. For the small number of antennas at the relay, throughput performance of EH-CCRN is actually inferior to one that uses the direct link without the relay, whereas that of the incremental scheme is always superior. We establish that there exist optimum values of EH parameters that result in maximum throughput with TS and PS EH protocols. We derive closed-form expressions for these in some special cases. Computer simulations are used to validate the derived expressions.
        △ Less
",consid energi harvest eh multi antenna relay base cooper cognit radio network ccrn investig outag throughput perform peak interfer type power control time switch ts power split ps protocol assum relay use maximum ratio combin transmit antenna select first second hop respect unlik literatur topic optim combin direct relay signal destin demonstr due random natur channel power control use consider improv perform also analyz perform eh ccrn increment relay analyt quantifi gain throughput eh ccrn small number antenna relay throughput perform eh ccrn actual inferior one use direct link without relay wherea increment scheme alway superior establish exist optimum valu eh paramet result maximum throughput ts ps eh protocol deriv close form express special case comput simul use valid deriv express less
429,1810.02534,"
        In this correspondence, we correct an erroneous argument in the proof of Theorem 1 of the paper above, which is a statement generalizing that for Wyner's common information.
        △ Less
",correspond correct erron argument proof theorem paper statement gener wyner common inform less
430,1810.02533,"
        Orthogonal frequency division multiplexing with index modulation (OFDM-IM) is a novel multicarrier scheme, which uses the indices of the active subcarriers to transmit data. OFDM-IM also inherits the high peak-to-average power ratio (PAPR) problem, which induces in-band distortion and out-of-band radiation when OFDM-IM signal passes through high power amplifier (HPA). In this letter, dither signals are added in the idle subcarriers to reduce the PAPR. Unlike the previous work using single level dither signals, multilevel dither signals are used by exploiting that the amplitudes of the active subcarriers are variously distributed for different subblocks. As a result, the proposed scheme gives the dither signals more freedom (a larger radius of dithering) in average. Simulation results show that the proposed scheme can achieve better PAPR reduction performance than the previous work.
        △ Less
",orthogon frequenc divis multiplex index modul ofdm im novel multicarri scheme use indic activ subcarri transmit data ofdm im also inherit high peak averag power ratio papr problem induc band distort band radiat ofdm im signal pass high power amplifi hpa letter dither signal ad idl subcarri reduc papr unlik previou work use singl level dither signal multilevel dither signal use exploit amplitud activ subcarri various distribut differ subblock result propos scheme give dither signal freedom larger radiu dither averag simul result show propos scheme achiev better papr reduct perform previou work less
431,1810.02531,"
        This paper is concerned with developing a novel distributed Kalman filtering algorithm over wireless sensor networks based on randomized consensus strategy. Compared with the centralized algorithm, distributed filtering techniques require less computation per sensor and lead to more robust estimation since they simply use the information from the neighboring nodes in the network. However, poor local sensor estimation caused by limited observability and network topology changes which interfere the global consensus are challenging issues. Motivated by this observation, we propose a novel randomized gossip-based distributed Kalman filtering algorithm. Information exchange and computation in the proposed algorithm can be carried out in an arbitrarily connected network of nodes. In addition, the computational burden can be distributed for a sensor which communicates with a stochastically selected neighbor at each clock step under schemes of gossip algorithm. In this case, the error covariance matrix changes stochastically at every clock step, thus the convergence is considered in a probabilistic sense. We provide the mean square convergence analysis of the proposed algorithm. Under a sufficient condition, we show that the proposed algorithm is quite appealing as it achieves better mean square error performance theoretically than the noncooperative decentralized Kalman filtering algorithm. Besides, considering the limited computation, communication, and energy resources in the wireless sensor networks, we propose an optimization problem which minimizes the average expected state estimation error based on the proposed algorithm. To solve the proposed problem efficiently, we transform it into a convex optimization problem. And a sub-optimal solution is attained. Examples and simulations are provided to illustrate the theoretical results.
        △ Less
",paper concern develop novel distribut kalman filter algorithm wireless sensor network base random consensu strategi compar central algorithm distribut filter techniqu requir less comput per sensor lead robust estim sinc simpli use inform neighbor node network howev poor local sensor estim caus limit observ network topolog chang interfer global consensu challeng issu motiv observ propos novel random gossip base distribut kalman filter algorithm inform exchang comput propos algorithm carri arbitrarili connect network node addit comput burden distribut sensor commun stochast select neighbor clock step scheme gossip algorithm case error covari matrix chang stochast everi clock step thu converg consid probabilist sens provid mean squar converg analysi propos algorithm suffici condit show propos algorithm quit appeal achiev better mean squar error perform theoret noncoop decentr kalman filter algorithm besid consid limit comput commun energi resourc wireless sensor network propos optim problem minim averag expect state estim error base propos algorithm solv propos problem effici transform convex optim problem sub optim solut attain exampl simul provid illustr theoret result less
432,1810.02528,"
        Wasserstein GAN(WGAN) is a model that minimizes the Wasserstein distance between a data distribution and sample distribution. Recent studies have proposed stabilizing the training process for the WGAN and implementing the Lipschitz constraint. In this study, we prove the local stability of optimizing the simple gradient penalty $μ$-WGAN(SGP $μ$-WGAN) under suitable assumptions regarding the equilibrium and penalty measure $μ$. The measure valued differentiation concept is employed to deal with the derivative of the penalty terms, which is helpful for handling abstract singular measures with lower dimensional support. Based on this analysis, we claim that penalizing the data manifold or sample manifold is the key to regularizing the original WGAN with a gradient penalty. Experimental results obtained with unintuitive penalty measures that satisfy our assumptions are also provided to support our theoretical results.
        △ Less
",wasserstein gan wgan model minim wasserstein distanc data distribut sampl distribut recent studi propos stabil train process wgan implement lipschitz constraint studi prove local stabil optim simpl gradient penalti wgan sgp wgan suitabl assumpt regard equilibrium penalti measur measur valu differenti concept employ deal deriv penalti term help handl abstract singular measur lower dimension support base analysi claim penal data manifold sampl manifold key regular origin wgan gradient penalti experiment result obtain unintuit penalti measur satisfi assumpt also provid support theoret result less
433,1810.02525,"
        Recent analyses of certain gradient descent optimization methods have shown that performance can degrade in some settings - such as with stochasticity or implicit momentum. In deep reinforcement learning (Deep RL), such optimization methods are often used for training neural networks via the temporal difference error or policy gradient. As an agent improves over time, the optimization target changes and thus the loss landscape (and local optima) change. Due to the failure modes of those methods, the ideal choice of optimizer for Deep RL remains unclear. As such, we provide an empirical analysis of the effects that a wide range of gradient descent optimizers and their hyperparameters have on policy gradient methods, a subset of Deep RL algorithms, for benchmark continuous control tasks. We find that adaptive optimizers have a narrow window of effective learning rates, diverging in other cases, and that the effectiveness of momentum varies depending on the properties of the environment. Our analysis suggests that there is significant interplay between the dynamics of the environment and Deep RL algorithm properties which aren't necessarily accounted for by traditional adaptive gradient methods. We provide suggestions for optimal settings of current methods and further lines of research based on our findings.
        △ Less
",recent analys certain gradient descent optim method shown perform degrad set stochast implicit momentum deep reinforc learn deep rl optim method often use train neural network via tempor differ error polici gradient agent improv time optim target chang thu loss landscap local optima chang due failur mode method ideal choic optim deep rl remain unclear provid empir analysi effect wide rang gradient descent optim hyperparamet polici gradient method subset deep rl algorithm benchmark continu control task find adapt optim narrow window effect learn rate diverg case effect momentum vari depend properti environ analysi suggest signific interplay dynam environ deep rl algorithm properti necessarili account tradit adapt gradient method provid suggest optim set current method line research base find less
434,1810.02518,"
        There is a growing need for discrete choice models that account for the complex nature of human choices, escaping traditional behavioral assumptions such as the transitivity of pairwise preferences. Recently, several parametric models of intransitive comparisons have been proposed, but in all cases the maximum likelihood problem is non-concave, making inference difficult. In this work we generalize this trend, showing that there cannot exist any parametric model with a concave log-likelihood function that can exhibit intransitive preferences. Given this result, we motivate a new model for analyzing intransitivity in pairwise comparisons, taking inspiration from the Condorcet method (majority vote) in social choice theory. The Majority Vote model we analyze is defined as a voting process over independent Random Utility Models (RUMs). We infer a multidimensional embedding of each object or player, in contrast to the traditional one-dimensional embedding used by models such as the Thurstone or Bradley-Terry-Luce (BTL) models. We show that a three-dimensional majority vote model is capable of modeling arbitrarily strong and long intransitive cycles, and can also represent arbitrary pairwise comparison probabilities on any triplet. We provide experimental results that substantiate our claims regarding the effectiveness of our model in capturing intransitivity for various pairwise choice tasks such as predicting choices in recommendation systems, winners in online video games, and elections.
        △ Less
",grow need discret choic model account complex natur human choic escap tradit behavior assumpt transit pairwis prefer recent sever parametr model intransit comparison propos case maximum likelihood problem non concav make infer difficult work gener trend show cannot exist parametr model concav log likelihood function exhibit intransit prefer given result motiv new model analyz intransit pairwis comparison take inspir condorcet method major vote social choic theori major vote model analyz defin vote process independ random util model rum infer multidimension embed object player contrast tradit one dimension embed use model thurston bradley terri luce btl model show three dimension major vote model capabl model arbitrarili strong long intransit cycl also repres arbitrari pairwis comparison probabl triplet provid experiment result substanti claim regard effect model captur intransit variou pairwis choic task predict choic recommend system winner onlin video game elect less
435,1810.02517,"
        This paper addresses the problem of planning time-optimal trajectories for multiple cooperative agents along specified paths through a static road network. Vehicle interactions at intersections create non-trivial decisions, with complex flow-on effects for subsequent interactions. A globally optimal, minimum time trajectory is found for all vehicles using Mixed Integer Linear Programming (MILP). Computational performance is improved by minimising binary variables using iteratively applied targeted collision constraints, and efficient goal constraints. Simulation results in an open-pit mining scenario compare the proposed method against a fast heuristic method and a reactive approach based on site practices. The heuristic is found to scale better with problem size while the MILP is able to avoid local minima.
        △ Less
",paper address problem plan time optim trajectori multipl cooper agent along specifi path static road network vehicl interact intersect creat non trivial decis complex flow effect subsequ interact global optim minimum time trajectori found vehicl use mix integ linear program milp comput perform improv minimis binari variabl use iter appli target collis constraint effici goal constraint simul result open pit mine scenario compar propos method fast heurist method reactiv approach base site practic heurist found scale better problem size milp abl avoid local minima less
436,1810.02513,"
        Simulation is a useful tool in situations where training data for machine learning models is costly to annotate or even hard to acquire. In this work, we propose a reinforcement learning-based method for automatically adjusting the parameters of any (non-differentiable) simulator, thereby controlling the distribution of synthesized data in order to maximize the accuracy of a model trained on that data. In contrast to prior art that hand-crafts these simulation parameters or adjusts only parts of the available parameters, our approach fully controls the simulator with the actual underlying goal of maximizing accuracy, rather than mimicking the real data distribution or randomly generating a large volume of data. We find that our approach (i) quickly converges to the optimal simulation parameters in controlled experiments and (ii) can indeed discover good sets of parameters for an image rendering simulator in actual computer vision applications.
        △ Less
",simul use tool situat train data machin learn model costli annot even hard acquir work propos reinforc learn base method automat adjust paramet non differenti simul therebi control distribut synthes data order maxim accuraci model train data contrast prior art hand craft simul paramet adjust part avail paramet approach fulli control simul actual underli goal maxim accuraci rather mimick real data distribut randomli gener larg volum data find approach quickli converg optim simul paramet control experi ii inde discov good set paramet imag render simul actual comput vision applic less
437,1810.02510,"
        In this paper, we investigate a multi-cell millimeter wave (mmWave) massive multiple-input multiple-output (MIMO) network with low-precision analog-to-digital converters (ADCs) at the base station (BS). Each cell serves multiple users and each user is equipped with multiple antennas but driven by a single RF chain. We first introduce a channel estimation strategy for the mmWave massive MIMO network and analyze the achievable rate with imperfect channel state information. Then, we derive an insightful lower bound for the achievable rate, which becomes tight with a growing number of users. The bound clearly demonstrates the impacts of the number of antennas and the ADC precision, especially for a single-cell mmWave network at low signal-to-noise ratio (SNR). It characterizes the tradeoff among various system parameters. Our analytical results are finally confirmed by extensive computer simulations.
        △ Less
",paper investig multi cell millimet wave mmwave massiv multipl input multipl output mimo network low precis analog digit convert adc base station bs cell serv multipl user user equip multipl antenna driven singl rf chain first introduc channel estim strategi mmwave massiv mimo network analyz achiev rate imperfect channel state inform deriv insight lower bound achiev rate becom tight grow number user bound clearli demonstr impact number antenna adc precis especi singl cell mmwave network low signal nois ratio snr character tradeoff among variou system paramet analyt result final confirm extens comput simul less
438,1810.02509,"
        In order to mitigate the long processing delay and high energy consumption of mobile augmented reality (AR) applications, mobile edge computing (MEC) has been recently proposed and is envisioned as a promising means to deliver better quality of experience (QoE) for AR consumers. In this article, we first present a comprehensive AR overview, including the indispensable components of general AR applications, fashionable AR devices, and several existing techniques for overcoming the thorny latency and energy consumption problems. Then, we propose a novel hierarchical computation architecture by inserting an edge layer between the conventional user layer and cloud layer. Based on the proposed architecture, we further develop an innovated operation mechanism to improve the performance of mobile AR applications. Three key technologies are also discussed to further assist the proposed AR architecture. Simulation results are finally provided to verify that our proposals can significantly improve the latency and energy performance as compared against existing baseline schemes.
        △ Less
",order mitig long process delay high energi consumpt mobil augment realiti ar applic mobil edg comput mec recent propos envis promis mean deliv better qualiti experi qoe ar consum articl first present comprehens ar overview includ indispens compon gener ar applic fashion ar devic sever exist techniqu overcom thorni latenc energi consumpt problem propos novel hierarch comput architectur insert edg layer convent user layer cloud layer base propos architectur develop innov oper mechan improv perform mobil ar applic three key technolog also discuss assist propos ar architectur simul result final provid verifi propos significantli improv latenc energi perform compar exist baselin scheme less
439,1810.02508,"
        Emotion recognition in conversations is a challenging Artificial Intelligence (AI) task. Recently, it has gained popularity due to its potential applications in many interesting AI tasks such as empathetic dialogue generation, user behavior understanding, and so on. To the best of our knowledge, there is no multimodal multi-party conversational dataset available, which contains more than two speakers in a dialogue. In this work, we propose the Multimodal EmotionLines Dataset (MELD), which we created by enhancing and extending the previously introduced EmotionLines dataset. MELD contains 13,708 utterances from 1433 dialogues of Friends TV series. MELD is superior to other conversational emotion recognition datasets SEMAINE and IEMOCAP as it consists of multiparty conversations and number of utterances in MELD is almost twice as these two datasets. Every utterance in MELD is associated with an emotion and a sentiment label. Utterances in MELD are multimodal encompassing audio and visual modalities along with the text. We have also addressed several shortcomings in EmotionLines and proposed a strong multimodal baseline. The baseline results show that both contextual and multimodal information play important role in emotion recognition in conversations.
        △ Less
",emot recognit convers challeng artifici intellig ai task recent gain popular due potenti applic mani interest ai task empathet dialogu gener user behavior understand best knowledg multimod multi parti convers dataset avail contain two speaker dialogu work propos multimod emotionlin dataset meld creat enhanc extend previous introduc emotionlin dataset meld contain utter dialogu friend tv seri meld superior convers emot recognit dataset semain iemocap consist multiparti convers number utter meld almost twice two dataset everi utter meld associ emot sentiment label utter meld multimod encompass audio visual modal along text also address sever shortcom emotionlin propos strong multimod baselin baselin result show contextu multimod inform play import role emot recognit convers less
440,1810.02496,"
        Users with limited use of their hands, such as people suffering from disabilities of the arm, shoulder, and hand (DASH), face challenges when authenticating with computer terminals, specially with publicly accessible terminals such as ATMs. A new glass wearable device was recently introduced by Google and it was immediately welcomed by groups of users, such as the ones described above, as Google Glass allows them to perform actions, like taking a photo, using only verbal commands. This paper investigates whether glass wearable devices can be used to authenticate users, both to grant access (one-time) and to maintain access (continuous), in similar hands-free fashion. We do so by designing and implementing Gauth, a system that enables users to authenticate with a service simply by issuing a voice command, while facing the computer terminal they are going to use to access the service. To achieve this goal, we create a physical communication channel from the terminal to the device using machine readable visual codes, like QR codes, and utilize the device's network adapter to communicate directly with a service. More importantly, we continuously authenticate the user accessing the terminal, exploiting the fact that a user operating a terminal is most likely facing it most of the time. We periodically issue authentication challenges, which are displayed as a QR code on the terminal, that cause the glass device to re-authenticate the user with an appropriate response. We evaluate our system to determine the technical limits of our approach.
        △ Less
",user limit use hand peopl suffer disabl arm shoulder hand dash face challeng authent comput termin special publicli access termin atm new glass wearabl devic recent introduc googl immedi welcom group user one describ googl glass allow perform action like take photo use verbal command paper investig whether glass wearabl devic use authent user grant access one time maintain access continu similar hand free fashion design implement gauth system enabl user authent servic simpli issu voic command face comput termin go use access servic achiev goal creat physic commun channel termin devic use machin readabl visual code like qr code util devic network adapt commun directli servic importantli continu authent user access termin exploit fact user oper termin like face time period issu authent challeng display qr code termin caus glass devic authent user appropri respons evalu system determin technic limit approach less
441,1810.02495,"
        This paper shows results of computer analysis of images in the purpose of finding differences between medical images in order of their classifications in terms of separation malign tissue from a normal and benign tissue. The diagnostics of malign tissue is of the crucial importance in medicine. Therefore, ascertainment of the correlation between multifractals parameters and ""chaotic"" cells could be of the great appliance. This paper shows the application of multifractal analysis for additional help in cancer diagnosis, as well as diminishing. of the subjective factor and error probability
        △ Less
",paper show result comput analysi imag purpos find differ medic imag order classif term separ malign tissu normal benign tissu diagnost malign tissu crucial import medicin therefor ascertain correl multifract paramet chaotic cell could great applianc paper show applic multifract analysi addit help cancer diagnosi well diminish subject factor error probabl less
442,1810.02492,"
        The analysis of multi-modality positron emission tomography and computed tomography (PET-CT) images requires combining the sensitivity of PET to detect abnormal regions with anatomical localization from CT. However, current methods for PET-CT image analysis either process the modalities separately or fuse information from each modality based on knowledge about the image analysis task. These methods generally do not consider the spatially varying visual characteristics that encode different information across the different modalities, which have different priorities at different locations. For example, a high abnormal PET uptake in the lungs is more meaningful for tumor detection than physiological PET uptake in the heart. Our aim is to improve fusion of the complementary information in multi-modality PET-CT with a new supervised convolutional neural network (CNN) that learns to fuse complementary information for multi-modality medical image analysis. Our CNN first encodes modality-specific features and then uses them to derive a spatially varying fusion map that quantifies the relative importance of each modality's features across different spatial locations. These fusion maps are then multiplied with the modality-specific feature maps to obtain a representation of the complementary multi-modality information at different locations, which can then be used for image analysis, e.g. region detection. We evaluated our CNN on a region detection problem using a dataset of PET-CT images of lung cancer. We compared our method to baseline techniques for multi-modality image analysis (pre-fused inputs, multi-branch techniques, multi-channel techniques) and demonstrated that our approach had a significantly higher accuracy ($p < 0.05$) than the baselines.
        △ Less
",analysi multi modal positron emiss tomographi comput tomographi pet ct imag requir combin sensit pet detect abnorm region anatom local ct howev current method pet ct imag analysi either process modal separ fuse inform modal base knowledg imag analysi task method gener consid spatial vari visual characterist encod differ inform across differ modal differ prioriti differ locat exampl high abnorm pet uptak lung meaning tumor detect physiolog pet uptak heart aim improv fusion complementari inform multi modal pet ct new supervis convolut neural network cnn learn fuse complementari inform multi modal medic imag analysi cnn first encod modal specif featur use deriv spatial vari fusion map quantifi rel import modal featur across differ spatial locat fusion map multipli modal specif featur map obtain represent complementari multi modal inform differ locat use imag analysi e g region detect evalu cnn region detect problem use dataset pet ct imag lung cancer compar method baselin techniqu multi modal imag analysi pre fuse input multi branch techniqu multi channel techniqu demonstr approach significantli higher accuraci p baselin less
443,1810.02490,"
        Dense femtocells and the integration of these femtocells with the macrocell are the ultimate goal of the femtocellular network deployment. Integrated macrocell/femtocell networks surely able to provide high data rate for the indoor users as well as able to offload huge traffic from the macrocellular networks to femtocellular networks. Efficient handling of handover calls is the key for the successful macrocell/femtocell integration. An appropriate traffic model for the integrated macrocell/femtocell networks is also needed for the performance analysis measurement. In this paper we presented a call admission control process and a traffic model for the integrated macrocell/femtocell networks. The numerical and simulation results show the important of the integrated macrocell/femtocell network and the performance improvement of the proposed schemes.
        △ Less
",dens femtocel integr femtocel macrocel ultim goal femtocellular network deploy integr macrocel femtocel network sure abl provid high data rate indoor user well abl offload huge traffic macrocellular network femtocellular network effici handl handov call key success macrocel femtocel integr appropri traffic model integr macrocel femtocel network also need perform analysi measur paper present call admiss control process traffic model integr macrocel femtocel network numer simul result show import integr macrocel femtocel network perform improv propos scheme less
444,1810.02489,"
        Recently, video broadcast/multicast over wireless networks has created a significant interest in the field of wireless communication. However, the wireless resources have limitations to broadcast/multicast many video sessions at the same time with the best quality. Hence, during the video transmission through wireless networks, it is very important to make the best utilization of the limited bandwidth. When the system bandwidth is not sufficient to allocate the demanded bandwidth for all of the active broadcasting/multicasting video sessions, instead of allocating equal bandwidth to each of them, our proposed scheme allocates bandwidth per video session based on popularity of the video program. Using the mathematical and simulation analyses, we show that the proposed scheme maximizes average user satisfaction level. The simulation results also indicate that a large number of subscribers can receive a significantly improved quality of video. To improve the video quality for large number of subscribers, the only tradeoff is that a very few subscribers receive slightly degraded video quality.
        △ Less
",recent video broadcast multicast wireless network creat signific interest field wireless commun howev wireless resourc limit broadcast multicast mani video session time best qualiti henc video transmiss wireless network import make best util limit bandwidth system bandwidth suffici alloc demand bandwidth activ broadcast multicast video session instead alloc equal bandwidth propos scheme alloc bandwidth per video session base popular video program use mathemat simul analys show propos scheme maxim averag user satisfact level simul result also indic larg number subscrib receiv significantli improv qualiti video improv video qualiti larg number subscrib tradeoff subscrib receiv slightli degrad video qualiti less
445,1810.02488,"
        The femto-access-point (FAP), a low power small cellular base station provides better signal quality for the indoor users. The mobile users in the vehicular environment suffer for the low quality signal from the outside wireless networks. The deployment of femtocells in the vehicular environment can solve the low level signal-to-noise plus interference problem. In this paper, we propose new application of the femtocell technology. The femtocells are deployed in the vehicular environment. Short distance between the user and the FAP provides better signal quality. The inside FAPs are connected to the core network through the outside macrocellular networks or the satellite networks. One stronger transceiver is installed at outside the vehicle. This transceiver is connected to FAPs using wired connection and to macrocellular or the satellite access networks through wireless link. The capacity and the outage probability are analyzed. The simulation results show that the proposed mobile femtocell deployment significantly enhances the service quality of mobile users in the vehicular environment.
        △ Less
",femto access point fap low power small cellular base station provid better signal qualiti indoor user mobil user vehicular environ suffer low qualiti signal outsid wireless network deploy femtocel vehicular environ solv low level signal nois plu interfer problem paper propos new applic femtocel technolog femtocel deploy vehicular environ short distanc user fap provid better signal qualiti insid fap connect core network outsid macrocellular network satellit network one stronger transceiv instal outsid vehicl transceiv connect fap use wire connect macrocellular satellit access network wireless link capac outag probabl analyz simul result show propos mobil femtocel deploy significantli enhanc servic qualiti mobil user vehicular environ less
446,1810.02486,"
        The next generation network aims to efficiently deploy low cost and low power cellular base station in the subscriber's home environment. For the femtocell deployment, frequency allocation among femtocells and macrocell is big concern to mitigate the interference, and to ensure the best use of the expensive spectrum. There are many sources of interference in integrated femtocell/macrocell networks. Lagging in proper management of interference reduces the system capacity, increases the outage probability, and finally users feel bad quality of experience (QoE). The cost effective interference management technique depends on the size of femtocells deployment. In this paper, firstly we present deployable various possible femtocell network scenarios. We propose the dynamic frequency re-use scheme to mitigate interference for femtocell deployment. For highly dense femtocells, we propose the functionalities of self organizing network (SON) based femtocell network architecture. The outage probability of a femtocell user is analyzed in details. The performances of the proposed schemes for various femtocell deployments are performed using numerical analysis.
        △ Less
",next gener network aim effici deploy low cost low power cellular base station subscrib home environ femtocel deploy frequenc alloc among femtocel macrocel big concern mitig interfer ensur best use expens spectrum mani sourc interfer integr femtocel macrocel network lag proper manag interfer reduc system capac increas outag probabl final user feel bad qualiti experi qoe cost effect interfer manag techniqu depend size femtocel deploy paper firstli present deploy variou possibl femtocel network scenario propos dynam frequenc use scheme mitig interfer femtocel deploy highli dens femtocel propos function self organ network son base femtocel network architectur outag probabl femtocel user analyz detail perform propos scheme variou femtocel deploy perform use numer analysi less
447,1810.02481,"
        In wireless communication systems, Quality of Service (QoS) is one of the most important issues from both the users and operators point of view. All the parameters related to QoS are not same important for all users and applications. The satisfaction level of different users also does not depend on same QoS parameters. In this paper, we discuss the QoS parameters and then propose a priority order of QoS parameters based on protocol layers and service applications. We present the relation among the QoS parameters those influence the performance of other QoS parameters and, finally, we demonstrate the numerical analysis results for our proposed soft-QoS scheme to reduce the dropped call rate which is the most important QoS parameter for all types of services
        △ Less
",wireless commun system qualiti servic qo one import issu user oper point view paramet relat qo import user applic satisfact level differ user also depend qo paramet paper discuss qo paramet propos prioriti order qo paramet base protocol layer servic applic present relat among qo paramet influenc perform qo paramet final demonstr numer analysi result propos soft qo scheme reduc drop call rate import qo paramet type servic less
448,1810.02474,"
        The TV set feedback feature standardized in the next generation TV system, ATSC 3.0, would enable opportunistic access of active TV channels in future Cognitive Radio Networks. This new dynamic spectrum access approach is named as black-space access, as it is complementary of current TV white space, which stands for inactive TV channels. TV black-space access can significantly increase the available spectrum of Cognitive Radio Networks in populated urban markets, where spectrum shortage is most severe while TV whitespace is very limited. However, to enable TV black-space access, secondary user has to evacuate a TV channel in a timely manner when TV user comes in. Such strict real-time constraint is an unique challenge of spectrum management infrastructure of Cognitive Radio Networks. In this paper, the real-time performance of spectrum management with regard to the degree of centralization of infrastructure is modeled and tested. Based on collected empirical network latency and database response time, we analyze the average evacuation time under four structures of spectrum management infrastructure: fully distribution, city-wide centralization, national-wide centralization, and semi-national centralization. The results show that national wide centralization may not meet the real-time requirement, while semi-national centralization that use multiple co-located independent spectrum manager can achieve real-time performance while keep most of the operational advantage of fully centralized structure.
        △ Less
",tv set feedback featur standard next gener tv system atsc would enabl opportunist access activ tv channel futur cognit radio network new dynam spectrum access approach name black space access complementari current tv white space stand inact tv channel tv black space access significantli increas avail spectrum cognit radio network popul urban market spectrum shortag sever tv whitespac limit howev enabl tv black space access secondari user evacu tv channel time manner tv user come strict real time constraint uniqu challeng spectrum manag infrastructur cognit radio network paper real time perform spectrum manag regard degre central infrastructur model test base collect empir network latenc databas respons time analyz averag evacu time four structur spectrum manag infrastructur fulli distribut citi wide central nation wide central semi nation central result show nation wide central may meet real time requir semi nation central use multipl co locat independ spectrum manag achiev real time perform keep oper advantag fulli central structur less
449,1810.02472,"
        We study an urgent semantics of asynchronous timed session types, where input actions happen as soon as possible. We show that with this semantics we can recover to the timed setting an appealing property of untimed session types: namely, deadlock-freedom is preserved when passing from synchronous to asynchronous communication.
        △ Less
",studi urgent semant asynchron time session type input action happen soon possibl show semant recov time set appeal properti untim session type name deadlock freedom preserv pass synchron asynchron commun less
450,1810.02471,"
        We consider recognizable trace rewriting systems with level-regular contexts (RTL). A trace language is level-regular if the set of Foata normal forms of its elements is regular. We prove that the rewriting graph of a RTL is word-automatic. Thus its first-order theory is decidable. Then, we prove that the concurrent unfolding of a finite concurrent automaton with the reachability relation is a RTL graph. It follows that the first-order theory with the reachability predicate (FO[Reach] theory) of such an unfolding is decidable. It is known that this property holds also for the ground term rewriting graphs. We provide examples of finite concurrent automata of which the concurrent unfoldings fail to be ground term rewriting graphs. The infinite grid tree (for each vertex of an infinite grid, there is an edge from this vertex to the origin of a copy of the infinite grid) is such an unfolding. We prove that the infinite grid tree is not a ground term rewriting graph. We have thus obtained a new class of graphs for with a decidable FO[Reach] theory.
        △ Less
",consid recogniz trace rewrit system level regular context rtl trace languag level regular set foata normal form element regular prove rewrit graph rtl word automat thu first order theori decid prove concurr unfold finit concurr automaton reachabl relat rtl graph follow first order theori reachabl predic fo reach theori unfold decid known properti hold also ground term rewrit graph provid exampl finit concurr automata concurr unfold fail ground term rewrit graph infinit grid tree vertex infinit grid edg vertex origin copi infinit grid unfold prove infinit grid tree ground term rewrit graph thu obtain new class graph decid fo reach theori less
451,1810.02470,"
        We propose active object languages as a development tool for formal system models of distributed systems. Additionally to a formalization based on a term rewriting system, we use established Software Engineering concepts,  including software product lines and object orientation that come with extensive tool support. We illustrate our modeling approach by prototyping a weak memory model. The resulting executable model is modular and has clear interfaces between communicating participants  through object-oriented modeling. Relaxations of the basic memory model are expressed as self-contained variants of a software product line. As a modeling language we use the formal active object language ABS which comes with an extensive tool set. This permits rapid formalization of core ideas, early validity checks in terms of formal invariant proofs, and debugging support by executing test runs.  Hence, our approach supports the prototyping of formal system models with early feedback.
        △ Less
",propos activ object languag develop tool formal system model distribut system addit formal base term rewrit system use establish softwar engin concept includ softwar product line object orient come extens tool support illustr model approach prototyp weak memori model result execut model modular clear interfac commun particip object orient model relax basic memori model express self contain variant softwar product line model languag use formal activ object languag ab come extens tool set permit rapid formal core idea earli valid check term formal invari proof debug support execut test run henc approach support prototyp formal system model earli feedback less
452,1810.02469,"
        Pomsets are a model of concurrent computations introduced by Pratt. They can provide a syntax-oblivious description of semantics of coordination models based on asynchronous message-passing, such as Message Sequence Charts (MSCs). In this paper, we study conditions that ensure a specification expressed as a set of pomsets can be faithfully realised via communicating automata. Our main contributions are (i) the definition of a realisability condition accounting for termination soundness, (ii) conditions for global specifications with ""multi-threaded"" participants, and (iii) the definition of realisability conditions that can be decided directly over pomsets. A positive by-product of our approach is the efficiency gain in the verification of the realisability conditions obtained when restricting to specific classes of choreographies characterisable in term of behavioural types.

        △ Less
",pomset model concurr comput introduc pratt provid syntax oblivi descript semant coordin model base asynchron messag pass messag sequenc chart msc paper studi condit ensur specif express set pomset faith realis via commun automata main contribut definit realis condit account termin sound ii condit global specif multi thread particip iii definit realis condit decid directli pomset posit product approach effici gain verif realis condit obtain restrict specif class choreographi characteris term behaviour type less
453,1810.02468,"
        Global-type formalisms enable to describe the overall behaviour of distributed systems and at the same time to enforce  safety properties for communications between system components. Our goal is that of amending a weakness of such formalisms: the difficulty in describing open systems, i.e. systems which can be connected and interact with other open systems. We parametrically extend, with the notion of interface role and interface connection, the syntax of global-type formalisms. Semantically, global types with interface roles denote open systems of communicating finite state machines connected by means of gateways obtained from compatible interfaces. We show that safety properties are preserved when open systems are connected that way.
        △ Less
",global type formal enabl describ overal behaviour distribut system time enforc safeti properti commun system compon goal amend weak formal difficulti describ open system e system connect interact open system parametr extend notion interfac role interfac connect syntax global type formal semant global type interfac role denot open system commun finit state machin connect mean gateway obtain compat interfac show safeti properti preserv open system connect way less
454,1810.02466,"
        As Bitcoin's popularity has grown over the decade since its creation, it has become an increasingly attractive target for adversaries of all kinds. One of the most powerful potential adversaries is the country of China, which has expressed adversarial positions regarding the cryptocurrency and demonstrated powerful capabilities to influence it. In this paper, we explore how China threatens the security, stability, and viability of Bitcoin through its dominant position in the Bitcoin ecosystem, political and economic control over domestic activity, and control over its domestic Internet infrastructure. We explore the relationship between China and Bitcoin, document China's motivation to undermine Bitcoin, and present a case study to demonstrate the strong influence that China has over Bitcoin. Finally, we systematize the class of attacks that China can deploy against Bitcoin to better understand the threat China poses. We conclude that China has mature capabilities and strong motives for performing a variety of attacks against Bitcoin.
        △ Less
",bitcoin popular grown decad sinc creation becom increasingli attract target adversari kind one power potenti adversari countri china express adversari posit regard cryptocurr demonstr power capabl influenc paper explor china threaten secur stabil viabil bitcoin domin posit bitcoin ecosystem polit econom control domest activ control domest internet infrastructur explor relationship china bitcoin document china motiv undermin bitcoin present case studi demonstr strong influenc china bitcoin final systemat class attack china deploy bitcoin better understand threat china pose conclud china matur capabl strong motiv perform varieti attack bitcoin less
455,1810.02460,"
        Seamless global parametrization of surfaces is a key operation in geometry processing, e.g. for high-quality quad mesh generation. A common approach is to prescribe the parametric domain structure, in particular the locations of parametrization singularities (cones), and solve a non-convex optimization problem minimizing a distortion measure, with local injectivity imposed through either constraints or barrier terms. In both cases, an initial valid parametrization is essential to serve as feasible starting point for obtaining an optimized solution. While convexified versions of the constraints eliminate this initialization requirement, they narrow the range of solutions, causing some problem instances that actually do have a solution to become infeasible. We demonstrate that for arbitrary given sets of topologically admissible parametric cones with prescribed curvature, a global seamless parametrization always exists (with the exception of one well-known case). Importantly, our proof is constructive and directly leads to a general algorithm for computing such parametrizations. Most distinctively, this algorithm is bootstrapped with a convex optimization problem (solving for a conformal map), in tandem with a simple linear equation system (determining a seamless modification of this map). This initial map can then serve as valid starting point and be optimized with respect to application specific distortion measures using existing injectivity preserving methods.
        △ Less
",seamless global parametr surfac key oper geometri process e g high qualiti quad mesh gener common approach prescrib parametr domain structur particular locat parametr singular cone solv non convex optim problem minim distort measur local inject impos either constraint barrier term case initi valid parametr essenti serv feasibl start point obtain optim solut convexifi version constraint elimin initi requir narrow rang solut caus problem instanc actual solut becom infeas demonstr arbitrari given set topolog admiss parametr cone prescrib curvatur global seamless parametr alway exist except one well known case importantli proof construct directli lead gener algorithm comput parametr distinct algorithm bootstrap convex optim problem solv conform map tandem simpl linear equat system determin seamless modif map initi map serv valid start point optim respect applic specif distort measur use exist inject preserv method less
456,1810.02455,"
        Small unmanned aircraft can help firefighters combat wildfires by providing real-time surveillance of the growing fires. However, guiding the aircraft autonomously given only wildfire images is a challenging problem. This work models noisy images obtained from on-board cameras and proposes two approaches to filtering the wildfire images. The first approach uses a simple Kalman filter to reduce noise and update a belief map in observed areas. The second approach uses a particle filter to predict wildfire growth and uses observations to estimate uncertainties relating to wildfire expansion. The belief maps are used to train a deep reinforcement learning controller, which learns a policy to navigate the aircraft to survey the wildfire while avoiding flight directly over the fire. Simulation results show that the proposed controllers precisely guide the aircraft and accurately estimate wildfire growth, and a study of observation noise demonstrates the robustness of the particle filter approach.
        △ Less
",small unman aircraft help firefight combat wildfir provid real time surveil grow fire howev guid aircraft autonom given wildfir imag challeng problem work model noisi imag obtain board camera propos two approach filter wildfir imag first approach use simpl kalman filter reduc nois updat belief map observ area second approach use particl filter predict wildfir growth use observ estim uncertainti relat wildfir expans belief map use train deep reinforc learn control learn polici navig aircraft survey wildfir avoid flight directli fire simul result show propos control precis guid aircraft accur estim wildfir growth studi observ nois demonstr robust particl filter approach less
457,1810.02453,"
        Consider linear regression where the examples are generated by an unknown distribution on $R^d\times R$. Without any assumptions on the noise, the linear least squares solution for any i.i.d. sample will typically be biased w.r.t. the least squares optimum over the entire distribution. However, we show that if an i.i.d. sample of any size k is augmented by a certain small additional sample, then the solution of the combined sample becomes unbiased. We show this when the additional sample consists of d points drawn jointly according to the input distribution that is rescaled by the squared volume spanned by the points. Furthermore, we propose algorithms to sample from this volume-rescaled distribution when the data distribution is only known through an i.i.d sample.
        △ Less
",consid linear regress exampl gener unknown distribut r time r without assumpt nois linear least squar solut sampl typic bias w r least squar optimum entir distribut howev show sampl size k augment certain small addit sampl solut combin sampl becom unbias show addit sampl consist point drawn jointli accord input distribut rescal squar volum span point furthermor propos algorithm sampl volum rescal distribut data distribut known sampl less
458,1810.02452,"
        The $k$-leaf power graph $G$ of a tree $T$ is a graph whose vertices are the leaves of $T$ and whose edges connect pairs of leaves at unweighted distance at most $k$ in $T$. Recognition of the $k$-leaf power graphs for $k \geq 6$ is still an open problem. In this paper, we provide an algorithm for this problem for sparse leaf power graphs. Our result shows that the problem of recognizing these graphs is fixed-parameter tractable when parameterized both by $k$ and by the degeneracy of the given graph. To prove this, we describe how to embed the leaf root of a leaf power graph into a product of the graph with a cycle graph. We bound the treewidth of the resulting product in terms of $k$ and the degeneracy of $G$. As a result, we can use methods based on monadic second-order logic (MSO$_2$) to recognize the existence of a leaf power as a subgraph of the product graph.
        △ Less
",k leaf power graph g tree graph whose vertic leav whose edg connect pair leav unweight distanc k recognit k leaf power graph k geq still open problem paper provid algorithm problem spars leaf power graph result show problem recogn graph fix paramet tractabl parameter k degeneraci given graph prove describ emb leaf root leaf power graph product graph cycl graph bound treewidth result product term k degeneraci g result use method base monad second order logic mso recogn exist leaf power subgraph product graph less
459,1810.02451,"
        This paper studies the Pliable Index CODing problem (PICOD), which models content-type distribution networks. In the PICOD$(t)$ problem there are $m$ messages, $n$ users and each user has a distinct message side information set, as in the classical Index Coding problem (IC). Differently from IC, where each user has a pre-specified set of messages to decode, in the PICOD$(t)$ a user is ""pliable"" and is satisfied if it can decode any $t$ messages that are not in its side information set. The goal is to find a code with the shortest length that satisfies all the users. This flexibility in determining the desired message sets makes the PICOD$(t)$ behave quite differently compared to the IC, and its analysis challenging.
  This paper mainly focuses on the \emph{complete--$S$} PICOD$(t)$ with $m$ messages, where the set $S\subset[m]$ contains the sizes of the side information sets, and the number of users is $n=\sum_{s\in S}\binom{m}{s}$, with no two users having the same side information set. Capacity results are shown for: (i) the \emph{consecutive} complete--$S$ PICOD$(t)$, where $S=[s_{\min}:s_{\max}]$ for some $0 \leq s_{\min} \leq s_{\max} \leq m-t$, and (ii) the \emph{complement-consecutive} complete--$S$ PICOD$(t)$, where $S=[0:m-t]\backslash[s_{\min}:s_{\max}]$, for some $0 < s_{\min} \leq s_{\max} < m-t$. The novel converse proof is inspired by combinatorial design techniques and the key insight is to consider all messages that a user can eventually decode successfully, even those in excess of the $t$ required ones. This allows one to circumvent the need to consider all possible desired message set assignments at the users in order to find the one that leads to the shortest code length.
  In addition, tight converse results are also shown for those PICOD$(1)$ with circular-arc network topology hypergraph.
        △ Less
",paper studi pliabl index code problem picod model content type distribut network picod problem messag n user user distinct messag side inform set classic index code problem ic differ ic user pre specifi set messag decod picod user pliabl satisfi decod messag side inform set goal find code shortest length satisfi user flexibl determin desir messag set make picod behav quit differ compar ic analysi challeng paper mainli focus emph complet picod messag set subset contain size side inform set number user n sum binom two user side inform set capac result shown emph consecut complet picod min max leq min leq max leq ii emph complement consecut complet picod backslash min max min leq max novel convers proof inspir combinatori design techniqu key insight consid messag user eventu decod success even excess requir one allow one circumv need consid possibl desir messag set assign user order find one lead shortest code length addit tight convers result also shown picod circular arc network topolog hypergraph less
460,1810.02450,"
        Conditions for the detectability of topology variations in dynamical networks are developed in a recent article in the IEEE Transactions on Control of Network Systems [1]. Here, an example is presented which illustrates an error in the network-theoretic conditions for detectability developed in [1].
        △ Less
",condit detect topolog variat dynam network develop recent articl ieee transact control network system exampl present illustr error network theoret condit detect develop less
461,1810.02445,"
        Point sets in 2D with multiple classes are a common type of data. A canonical visualization design for them are scatterplots, which do not scale to large collections of points. For these larger data sets, binned aggregation (or binning) is often used to summarize the data, with many possible design alternatives for creating effective visual representations of these summaries. There are a wide range of designs to show summaries of 2D multi-class point data, each capable of supporting different analysis tasks. In this paper, we explore the space of visual designs for such data, and provide design guidelines for different analysis scenarios. To support these guidelines, we compile a set of abstract tasks and ground them in concrete examples using multiple sample datasets. We then assess designs, and survey a range of design decisions, considering their appropriateness to the tasks. In addition, we provide a web-based implementation to experiment with design choices, supporting the validation of designs based on task needs.
        △ Less
",point set multipl class common type data canon visual design scatterplot scale larg collect point larger data set bin aggreg bin often use summar data mani possibl design altern creat effect visual represent summari wide rang design show summari multi class point data capabl support differ analysi task paper explor space visual design data provid design guidelin differ analysi scenario support guidelin compil set abstract task ground concret exampl use multipl sampl dataset assess design survey rang design decis consid appropri task addit provid web base implement experi design choic support valid design base task need less
462,1810.02443,"
        With the rapid growth of fashion-focused social networks and online shopping, intelligent fashion recommendation is now in great need. We design algorithms which automatically suggest users outfits (e.g. a shirt, together with a skirt and a pair of high-heel shoes), that fit their personal fashion preferences. Recommending sets, each of which is composed of multiple interacted items, is relatively new to recommender systems, which usually recommend individual items to users. We explore the use of deep networks for this challenging task. Our system, dubbed FashionNet, consists of two components, a feature network for feature extraction and a matching network for compatibility computation. The former is achieved through a deep convolutional network. And for the latter, we adopt a multi-layer fully-connected network structure. We design and compare three alternative architectures for FashionNet. To achieve personalized recommendation, we develop a two-stage training strategy, which uses the fine-tuning technique to transfer a general compatibility model to a model that embeds personal preference. Experiments on a large scale data set collected from a popular fashion-focused social network validate the effectiveness of the proposed networks.
        △ Less
",rapid growth fashion focus social network onlin shop intellig fashion recommend great need design algorithm automat suggest user outfit e g shirt togeth skirt pair high heel shoe fit person fashion prefer recommend set compos multipl interact item rel new recommend system usual recommend individu item user explor use deep network challeng task system dub fashionnet consist two compon featur network featur extract match network compat comput former achiev deep convolut network latter adopt multi layer fulli connect network structur design compar three altern architectur fashionnet achiev person recommend develop two stage train strategi use fine tune techniqu transfer gener compat model model emb person prefer experi larg scale data set collect popular fashion focus social network valid effect propos network less
463,1810.02442,"
        Many machine learning problems involve iteratively and alternately optimizing different task objectives with respect to different sets of parameters. Appropriately scheduling the optimization of a task objective or a set of parameters is usually crucial to the quality of convergence. In this paper, we present AutoLoss, a meta-learning framework that automatically learns and determines the optimization schedule. AutoLoss provides a generic way to represent and learn the discrete optimization schedule from metadata, allows for a dynamic and data-driven schedule in ML problems that involve alternating updates of different parameters or from different loss objectives. We apply AutoLoss on four ML tasks: d-ary quadratic regression, classification using a multi-layer perceptron (MLP), image generation using GANs, and multi-task neural machine translation (NMT). We show that the AutoLoss controller is able to capture the distribution of better optimization schedules that result in higher quality of convergence on all four tasks. The trained AutoLoss controller is generalizable -- it can guide and improve the learning of a new task model with different specifications, or on different datasets.
        △ Less
",mani machin learn problem involv iter altern optim differ task object respect differ set paramet appropri schedul optim task object set paramet usual crucial qualiti converg paper present autoloss meta learn framework automat learn determin optim schedul autoloss provid gener way repres learn discret optim schedul metadata allow dynam data driven schedul ml problem involv altern updat differ paramet differ loss object appli autoloss four ml task ari quadrat regress classif use multi layer perceptron mlp imag gener use gan multi task neural machin translat nmt show autoloss control abl captur distribut better optim schedul result higher qualiti converg four task train autoloss control generaliz guid improv learn new task model differ specif differ dataset less
464,1810.02440,"
        We study the topology of the space of learning tasks, which is critical to understanding transfer learning whereby a model such as a deep neural network is pre-trained on a task, and then used on a different one after some fine-tuning. First we show that using the Kolmogorov structure function we can define a distance between tasks, which is independent on any particular model used and, empirically, correlates with the semantic similarity between tasks. Then, using a path integral approximation, we show that this plays a central role in the learning dynamics of Deep Networks, and in particular in the reachability of one task from another. We show that the probability of paths connecting two tasks, is asymmetric and has a static component that depends on the geometry of the loss function, in particular on the curvature, and a dynamic component that is model dependent and relates to the ease of traversing such paths. Surprisingly, the static component corresponds to the distance derived from the Kolmogorov Structure Function. With the dynamic component, this gives strict lower bounds on the complexity necessary to learn a task starting from the solution to another. Our analysis also explains more complex phenomena where semantically similar tasks may be unreachable from one another, a phenomenon called Information Plasticity and observed in diverse learning systems such as animals and deep neural networks.
        △ Less
",studi topolog space learn task critic understand transfer learn wherebi model deep neural network pre train task use differ one fine tune first show use kolmogorov structur function defin distanc task independ particular model use empir correl semant similar task use path integr approxim show play central role learn dynam deep network particular reachabl one task anoth show probabl path connect two task asymmetr static compon depend geometri loss function particular curvatur dynam compon model depend relat eas travers path surprisingli static compon correspond distanc deriv kolmogorov structur function dynam compon give strict lower bound complex necessari learn task start solut anoth analysi also explain complex phenomena semant similar task may unreach one anoth phenomenon call inform plastic observ divers learn system anim deep neural network less
465,1810.02438,"
        Updating a probability distribution in the light of new evidence is a very basic operation in Bayesian probability theory. It is also known as state revision or simply as conditioning. This paper recalls how locally updating a joint state can equivalently be described via inference using the channel extracted from the state (via disintegration).
  This paper also investigates the quantum analogues of conditioning, and in particular the analogues of this equivalence between updating a joint state and inference. The main finding is that in order to obtain a similar equivalence, we have to distinguish two forms of quantum conditioning, which we call lower and upper conditioning. They are known from the literature, but the common framework in which we describe them and the equivalence result are new.
        △ Less
",updat probabl distribut light new evid basic oper bayesian probabl theori also known state revis simpli condit paper recal local updat joint state equival describ via infer use channel extract state via disintegr paper also investig quantum analogu condit particular analogu equival updat joint state infer main find order obtain similar equival distinguish two form quantum condit call lower upper condit known literatur common framework describ equival result new less
466,1810.02434,"
        Abstraction is a powerful idea widely used in science, to model, reason and explain the behavior of systems in a more tractable search space, by omitting irrelevant details. While notions of abstraction have matured for deterministic systems, the case for abstracting probabilistic models is not yet fully understood. In this paper, we develop a foundational framework for abstraction in probabilistic relational models from first principles. These models borrow syntactic devices from first-order logic and are very expressive, thus naturally allowing for relational and hierarchical constructs with stochastic primitives. We motivate a definition of consistency between a high-level model and its low-level counterpart, but also treat the case when the high-level model is missing critical information present in the low-level model. We prove properties of abstractions, both at the level of the parameter as well as the structure of the models.
        △ Less
",abstract power idea wide use scienc model reason explain behavior system tractabl search space omit irrelev detail notion abstract matur determinist system case abstract probabilist model yet fulli understood paper develop foundat framework abstract probabilist relat model first principl model borrow syntact devic first order logic express thu natur allow relat hierarch construct stochast primit motiv definit consist high level model low level counterpart also treat case high level model miss critic inform present low level model prove properti abstract level paramet well structur model less
467,1810.02426,"
        Salient object detection is a problem that has been considered in detail and many solutions proposed. In this paper, we argue that work to date has addressed a problem that is relatively ill-posed. Specifically, there is not universal agreement about what constitutes a salient object when multiple observers are queried. This implies that some objects are more likely to be judged salient than others, and implies a relative rank exists on salient objects. Initially, we present a novel deep learning solution based on a hierarchical representation of relative saliency and stage-wise refinement. Furthermore, we present data, analysis and benchmark baseline results towards addressing the problem of salient object ranking. Methods for deriving suitable ranked salient object instances are presented, along with metrics suitable to measuring algorithm performance. In addition, we show how a derived dataset can be successively refined to provide cleaned results that correlate well with pristine ground truth. Finally, we provide a comparison among prevailing algorithms that address salient object ranking or detection to establish initial baselines.
        △ Less
",salient object detect problem consid detail mani solut propos paper argu work date address problem rel ill pose specif univers agreement constitut salient object multipl observ queri impli object like judg salient other impli rel rank exist salient object initi present novel deep learn solut base hierarch represent rel salienc stage wise refin furthermor present data analysi benchmark baselin result toward address problem salient object rank method deriv suitabl rank salient object instanc present along metric suitabl measur algorithm perform addit show deriv dataset success refin provid clean result correl well pristin ground truth final provid comparison among prevail algorithm address salient object rank detect establish initi baselin less
468,1810.02424,"
        Adversarial training has been successfully applied to build robust models at a certain cost. While the robustness of a model increases, the standard classification accuracy declines. This phenomenon is suggested to be an inherent trade-off between standard accuracy and robustness. We propose a model that employs feature prioritization by a nonlinear attention module and $L_2$ regularization as implicit denoising to improve the adversarial robustness and the standard accuracy relative to adversarial training. Focusing sharply on the regions of interest, the attention maps encourage the model to rely heavily on features extracted from the most relevant areas while suppressing the unrelated background. Penalized by a regularizer, the model extracts similar features for the natural and adversarial images, effectively ignoring the added perturbation. In addition to qualitative evaluation, we also propose a novel experimental strategy that quantitatively demonstrates that our model is almost ideally aligned with salient data characteristics. Additional experimental results illustrate the power of our model relative to the state of the art methods.
        △ Less
",adversari train success appli build robust model certain cost robust model increas standard classif accuraci declin phenomenon suggest inher trade standard accuraci robust propos model employ featur priorit nonlinear attent modul l regular implicit denois improv adversari robust standard accuraci rel adversari train focus sharpli region interest attent map encourag model reli heavili featur extract relev area suppress unrel background penal regular model extract similar featur natur adversari imag effect ignor ad perturb addit qualit evalu also propos novel experiment strategi quantit demonstr model almost ideal align salient data characterist addit experiment result illustr power model rel state art method less
469,1810.02423,"
        Cooperation information sharing is important to theories of human learning and has potential implications for machine learning. Prior work derived conditions for achieving optimal Cooperative Inference given strong, relatively restrictive assumptions. We relax these assumptions by demonstrating convergence for any discrete joint distribution, robustness through equivalence classes and stability under perturbation, and effectiveness by deriving bounds from structural properties of the original joint distribution. We provide geometric interpretations, connections to and implications for optimal transport, and connections to importance sampling, and conclude by outlining open questions and challenges to realizing the promise of Cooperative Inference.
        △ Less
",cooper inform share import theori human learn potenti implic machin learn prior work deriv condit achiev optim cooper infer given strong rel restrict assumpt relax assumpt demonstr converg discret joint distribut robust equival class stabil perturb effect deriv bound structur properti origin joint distribut provid geometr interpret connect implic optim transport connect import sampl conclud outlin open question challeng realiz promis cooper infer less
470,1810.02422,"
        Simulation-to-real transfer is an important strategy for making reinforcement learning practical with real robots. Successful sim-to-real transfer systems have difficulty producing policies which generalize across tasks, despite training for thousands of hours equivalent real robot time. To address this shortcoming, we present a novel approach to efficiently learning new robotic skills directly on a real robot, based on model-predictive control (MPC) and an algorithm for learning task representations. In short, we show how to reuse the simulation from the pre-training step of sim-to-real methods as a tool for foresight, allowing the sim-to-real policy adapt to unseen tasks. Rather than end-to-end learning policies for single tasks and attempting to transfer them, we first use simulation to simultaneously learn (1) a continuous parameterization (i.e. a skill embedding or latent) of task-appropriate primitive skills, and (2) a single policy for these skills which is conditioned on this representation. We then directly transfer our multi-skill policy to a real robot, and actuate the robot by choosing sequences of skill latents which actuate the policy, with each latent corresponding to a pre-learned primitive skill controller. We complete unseen tasks by choosing new sequences of skill latents to control the robot using MPC, where our MPC model is composed of the pre-trained skill policy executed in the simulation environment, run in parallel with the real robot. We discuss the background and principles of our method, detail its practical implementation, and evaluate its performance by using our method to train a real Sawyer Robot to achieve motion tasks such as drawing and block pushing.
        △ Less
",simul real transfer import strategi make reinforc learn practic real robot success sim real transfer system difficulti produc polici gener across task despit train thousand hour equival real robot time address shortcom present novel approach effici learn new robot skill directli real robot base model predict control mpc algorithm learn task represent short show reus simul pre train step sim real method tool foresight allow sim real polici adapt unseen task rather end end learn polici singl task attempt transfer first use simul simultan learn continu parameter e skill embed latent task appropri primit skill singl polici skill condit represent directli transfer multi skill polici real robot actuat robot choos sequenc skill latent actuat polici latent correspond pre learn primit skill control complet unseen task choos new sequenc skill latent control robot use mpc mpc model compos pre train skill polici execut simul environ run parallel real robot discuss background principl method detail practic implement evalu perform use method train real sawyer robot achiev motion task draw block push less
471,1810.02419,"
        The extension of image generation to video generation turns out to be a very difficult task, since the temporal dimension of videos introduces an extra challenge during the generation process. Besides, due to the limitation of memory and training stability, the generation becomes increasingly challenging with the increase of the resolution/duration of videos. In this work, we exploit the idea of progressive growing of Generative Adversarial Networks (GANs) for higher resolution video generation. In particular, we begin to produce video samples of low-resolution and short-duration, and then progressively increase both resolution and duration alone (or jointly) by adding new spatiotemporal convolutional layers to the current networks. Starting from the learning on a very raw-level spatial appearance and temporal movement of the video distribution, the proposed progressive method learns spatiotemporal information incrementally to generate higher resolution videos. Furthermore, we introduce a sliced version of Wasserstein GAN (SWGAN) loss to improve the distribution learning on the video data of high-dimension and mixed-spatiotemporal distribution. SWGAN loss replaces the distance between joint distributions by that of one-dimensional marginal distributions, making the loss easier to compute. We evaluate the proposed model on our collected face video dataset of 10,900 videos to generate photorealistic face videos of 256x256x32 resolution. In addition, our model also reaches a record inception score of 14.57 in unsupervised action recognition dataset UCF-101.
        △ Less
",extens imag gener video gener turn difficult task sinc tempor dimens video introduc extra challeng gener process besid due limit memori train stabil gener becom increasingli challeng increas resolut durat video work exploit idea progress grow gener adversari network gan higher resolut video gener particular begin produc video sampl low resolut short durat progress increas resolut durat alon jointli ad new spatiotempor convolut layer current network start learn raw level spatial appear tempor movement video distribut propos progress method learn spatiotempor inform increment gener higher resolut video furthermor introduc slice version wasserstein gan swgan loss improv distribut learn video data high dimens mix spatiotempor distribut swgan loss replac distanc joint distribut one dimension margin distribut make loss easier comput evalu propos model collect face video dataset video gener photorealist face video x x resolut addit model also reach record incept score unsupervis action recognit dataset ucf less
472,1810.02411,"
        In this work, we construct energy-efficient variable-to-fixed length (V2F), fixed-to-variable length (F2V), and variable-to-variable length (V2V) prefix-free codes, which are optimal (or near-optimal) in the sense that no (or few) other codes with the size can achieve a smaller energy per code letter for the same entropy rate. Under stringent constraints of 4096 entries or below per codebook, the constructed codes yield an energy per code letter within a few tenths of a dB of the unconstrained theoretic lower bound, across a wide range of entropy rates with a very fine granularity. We also propose a framing method that allows variable-length codes to be transmitted using a fixed-length frame. The penalty caused by framing is studied using simulations and analysis, showing that the energy per code letter is kept within 0.2 dB of the unconstrained theoretic limit for some tested codes with a large frame length. When framed prefix-free codes are used to implement probabilistic constellation shaping (PCS) for communications in the additive white Gaussian noise channel, simulations show that 1.1 dB and 0.65 dB of shaping gains are achieved relative to uniform 8- and 16-quadrature amplitude modulation (QAM), respectively.
        △ Less
",work construct energi effici variabl fix length v f fix variabl length f v variabl variabl length v v prefix free code optim near optim sens code size achiev smaller energi per code letter entropi rate stringent constraint entri per codebook construct code yield energi per code letter within tenth db unconstrain theoret lower bound across wide rang entropi rate fine granular also propos frame method allow variabl length code transmit use fix length frame penalti caus frame studi use simul analysi show energi per code letter kept within db unconstrain theoret limit test code larg frame length frame prefix free code use implement probabilist constel shape pc commun addit white gaussian nois channel simul show db db shape gain achiev rel uniform quadratur amplitud modul qam respect less
473,1810.02401,"
        We address the problem of suppressing facial expressions in videos because expressions can hinder the retrieval of important information in applications such as face recognition. To achieve this, we present an optical strain suppression method that removes any facial expression without requiring training for a specific expression. For each frame in a video, an optical strain map that provides the strain magnitude value at each pixel is generated; this strain map is then utilized to neutralize the expression by replacing pixels of high strain values with pixels from a reference face frame. Experimental results of testing the method on various expressions namely happiness, sadness, and anger for two publicly available data sets (i.e., BU-4DFE and AM-FED) show the ability of our method in suppressing facial expressions.
        △ Less
",address problem suppress facial express video express hinder retriev import inform applic face recognit achiev present optic strain suppress method remov facial express without requir train specif express frame video optic strain map provid strain magnitud valu pixel gener strain map util neutral express replac pixel high strain valu pixel refer face frame experiment result test method variou express name happi sad anger two publicli avail data set e bu dfe fed show abil method suppress facial express less
474,1810.02400,"
        In recent years, machine learning techniques are widely used in numerous applications, such as weather forecast, financial data analysis, spam filtering, and medical prediction. In the meantime, massive data generated from multiple sources further improve the performance of machine learning tools. However, data sharing from multiple sources brings privacy issues for those sources since sensitive information may be leaked in this process. In this paper, we propose a framework enabling multiple parties to collaboratively and accurately train a learning model over distributed datasets while guaranteeing the privacy of data sources. Specifically, we consider logistic regression model for data training and propose two approaches for perturbing the objective function to preserve ε-differential privacy. The proposed solutions are tested on real datasets, including Bank Marketing and Credit Card Default prediction. Experimental results demonstrate that the proposed multiparty learning framework is highly efficient and accurate.
        △ Less
",recent year machin learn techniqu wide use numer applic weather forecast financi data analysi spam filter medic predict meantim massiv data gener multipl sourc improv perform machin learn tool howev data share multipl sourc bring privaci issu sourc sinc sensit inform may leak process paper propos framework enabl multipl parti collabor accur train learn model distribut dataset guarante privaci data sourc specif consid logist regress model data train propos two approach perturb object function preserv differenti privaci propos solut test real dataset includ bank market credit card default predict experiment result demonstr propos multiparti learn framework highli effici accur less
475,1810.02396,"
        Motivated by cryptographic applications such as predicate encryption, we consider the problem of representing an arbitrary predicate as the inner product predicate on two vectors. Concretely, fix a Boolean function $P$ and some modulus $q$. We are interested in encoding $x$ to $\vec x$ and $y$ to $\vec y$ so that $$P(x,y) = 1 \Longleftrightarrow \langle\vec x,\vec y\rangle= 0 \bmod q,$$ where the vectors should be as short as possible. This problem can also be viewed as a generalization of matching vector families, which corresponds to the equality predicate. Matching vector families have been used in the constructions of Ramsey graphs, private information retrieval (PIR) protocols, and more recently, secret sharing.
  Our main result is a simple lower bound that allows us to show that known encodings for many predicates considered in the cryptographic literature such as greater than and threshold are essentially optimal for prime modulus $q$. Using this approach, we also prove lower bounds on encodings for composite $q$, and then show tight upper bounds for such predicates as greater than, index and disjointness.
        △ Less
",motiv cryptograph applic predic encrypt consid problem repres arbitrari predic inner product predic two vector concret fix boolean function p modulu q interest encod x vec x vec p x longleftrightarrow langl vec x vec rangl bmod q vector short possibl problem also view gener match vector famili correspond equal predic match vector famili use construct ramsey graph privat inform retriev pir protocol recent secret share main result simpl lower bound allow us show known encod mani predic consid cryptograph literatur greater threshold essenti optim prime modulu q use approach also prove lower bound encod composit q show tight upper bound predic greater index disjoint less
476,1810.02393,"
        We investigate the relation between the block sensitivity $\text{bs}(f)$ and fractional block sensitivity $\text{fbs}(f)$ complexity measures of Boolean functions. While it is known that $\text{fbs}(f) = O(\text{bs}(f)^2)$, the best known separation achieves $\text{fbs}(f) = \left(\frac{1}{3\sqrt2} +o(1)\right) \text{bs(f)}^{3/2}$. We improve the constant factor and show a family of functions that give $\text{fbs}(f) = \left(\frac{1}{\sqrt6}-o(1)\right) \text{bs}(f)^{3/2}.$
        △ Less
",investig relat block sensit text bs f fraction block sensit text fb f complex measur boolean function known text fb f text bs f best known separ achiev text fb f left frac sqrt right text bs f improv constant factor show famili function give text fb f left frac sqrt right text bs f less
477,1810.02389,"
        The notion of bounded expansion captures uniform sparsity of graph classes and renders various algorithmic problems that are hard in general tractable. In particular, the model-checking problem for first-order logic is fixed-parameter tractable over such graph classes. With the aim of generalizing such results to dense graphs, we introduce classes of graphs with structurally bounded expansion, defined as first-order interpretations of classes of bounded expansion. As a first step towards their algorithmic treatment, we provide their characterization analogous to the characterization of classes of bounded expansion via low treedepth decompositions, replacing treedepth by its dense analogue called shrubdepth.
        △ Less
",notion bound expans captur uniform sparsiti graph class render variou algorithm problem hard gener tractabl particular model check problem first order logic fix paramet tractabl graph class aim gener result dens graph introduc class graph structur bound expans defin first order interpret class bound expans first step toward algorithm treatment provid character analog character class bound expans via low treedepth decomposit replac treedepth dens analogu call shrubdepth less
478,1810.02383,"
        In this study, we propose a generic complementary sequence (CS) encoder that limits the peak-to-average-power ratio (PAPR) of an orthogonal frequency division multiplexing (OFDM) symbol for a non-contiguous frequency domain resource allocation. To this end, we introduce a framework which describes a recursion evolved with two linear operators at each step as an encoding operation. The framework algebraically determines how the operators applied at each recursion step are distributed to the coefficients of a polynomial via binary construction sequences. By applying the introduced framework to a recursive Golay complementary pair (GCP) construction relying on Budi$š$in's methods, we show the impact of the initial sequences, phase rotations, signs, real scalars, and the shifting factors applied at each step on the elements of the sequences in a GCP explicitly. Hence, we provide further insights into GCPs. As a result, we obtain an encoder which generates non-contiguous CSs, i.e., CSs with zero symbols, by deriving the algebraic normal forms (ANFs) for the shifting factors. Thus, three important aspects for communications, i.e., frequency diversity, coding gain, and low PAPR, are achieved simultaneously for OFDM symbols. As another result, we extend the standard sequences by separating the encoders that control the amplitude and the phase of the elements of a CSs. We also demonstrate that the proposed encoder generates CSs with quadrature amplitude modulation (QAM) alphabet.
        △ Less
",studi propos gener complementari sequenc cs encod limit peak averag power ratio papr orthogon frequenc divis multiplex ofdm symbol non contigu frequenc domain resourc alloc end introduc framework describ recurs evolv two linear oper step encod oper framework algebra determin oper appli recurs step distribut coeffici polynomi via binari construct sequenc appli introduc framework recurs golay complementari pair gcp construct reli budi method show impact initi sequenc phase rotat sign real scalar shift factor appli step element sequenc gcp explicitli henc provid insight gcp result obtain encod gener non contigu css e css zero symbol deriv algebra normal form anf shift factor thu three import aspect commun e frequenc divers code gain low papr achiev simultan ofdm symbol anoth result extend standard sequenc separ encod control amplitud phase element css also demonstr propos encod gener css quadratur amplitud modul qam alphabet less
479,1810.02364,"
        Automatic classification of sound commands is becoming increasingly important, especially for mobile and embedded devices. Many of these devices contain both cameras and microphones, and companies that develop them would like to use the same technology for both of these classification tasks. One way of achieving this is to represent sound commands as images, and use convolutional neural networks when classifying images as well as sounds. In this paper we consider several approaches to the problem of sound classification that we applied in TensorFlow Speech Recognition Challenge organized by Google Brain team on the Kaggle platform. Here we show different representation of sounds (Wave frames, Spectrograms, Mel-Spectrograms, MFCCs) and apply several 1D and 2D convolutional neural networks in order to get the best performance. Our experiments show that we found appropriate sound representation and corresponding convolutional neural networks. As a result we achieved good classification accuracy that allowed us to finish the challenge on 8-th place among 1315 teams.
        △ Less
",automat classif sound command becom increasingli import especi mobil embed devic mani devic contain camera microphon compani develop would like use technolog classif task one way achiev repres sound command imag use convolut neural network classifi imag well sound paper consid sever approach problem sound classif appli tensorflow speech recognit challeng organ googl brain team kaggl platform show differ represent sound wave frame spectrogram mel spectrogram mfcc appli sever convolut neural network order get best perform experi show found appropri sound represent correspond convolut neural network result achiev good classif accuraci allow us finish challeng th place among team less
480,1810.02363,"
        Manually authoring transition animations for a complete locomotion system can be a tedious and time-consuming task, especially for large games that allow complex and constrained locomotion movements, where the number of transitions grows exponentially with the number of states. In this paper, we present a novel approach, based on deep recurrent neural networks, to automatically generate such transitions given a \textit{past context} of a few frames and a target character state to reach. We present the Recurrent Transition Network (RTN), based on a modified version of the Long-Short-Term-Memory (LSTM) network, designed specifically for transition generation and trained without any gait, phase, contact or action labels. We further propose a simple yet principled way to initialize the hidden states of the LSTM layer for a given sequence which improves the performance and generalization to new motions. We both quantitatively and qualitatively evaluate our system and show that making the network terrain-aware by adding a local terrain representation to the input yields better performance for rough-terrain navigation on long transitions. Our system produces realistic and fluid transitions that rival the quality of Motion Capture-based ground-truth motions, even before applying any inverse-kinematics postprocess. Direct benefits of our approach could be to accelerate the creation of transition variations for large coverage, or even to entirely replace transition nodes in an animation graph. We further explore applications of this model in a animation super-resolution setting where we temporally decompress animations saved at 1 frame per second and show that the network is able to reconstruct motions that are hard to distinguish from un-compressed locomotion sequences.
        △ Less
",manual author transit anim complet locomot system tediou time consum task especi larg game allow complex constrain locomot movement number transit grow exponenti number state paper present novel approach base deep recurr neural network automat gener transit given textit past context frame target charact state reach present recurr transit network rtn base modifi version long short term memori lstm network design specif transit gener train without gait phase contact action label propos simpl yet principl way initi hidden state lstm layer given sequenc improv perform gener new motion quantit qualit evalu system show make network terrain awar ad local terrain represent input yield better perform rough terrain navig long transit system produc realist fluid transit rival qualiti motion captur base ground truth motion even appli invers kinemat postprocess direct benefit approach could acceler creation transit variat larg coverag even entir replac transit node anim graph explor applic model anim super resolut set tempor decompress anim save frame per second show network abl reconstruct motion hard distinguish un compress locomot sequenc less
481,1810.02358,"
        We study how to leverage off-the-shelf visual and linguistic data to cope with out-of-vocabulary answers in visual question answering. Existing large-scale visual data with annotations such as image class labels, bounding boxes and region descriptions are good sources for learning rich and diverse visual concepts. However, it is not straightforward how the visual concepts should be captured and transferred to visual question answering models due to missing link between question dependent answering models and visual data without question or task specification. We tackle this problem in two steps: 1) learning a task conditional visual classifier based on unsupervised task discovery and 2) transferring and adapting the task conditional visual classifier to visual question answering models. Specifically, we employ linguistic knowledge sources such as structured lexical database (e.g. Wordnet) and visual descriptions for unsupervised task discovery, and adapt a learned task conditional visual classifier to answering unit in a visual question answering model. We empirically show that the proposed algorithm generalizes to unseen answers successfully using the knowledge transferred from the visual data.
        △ Less
",studi leverag shelf visual linguist data cope vocabulari answer visual question answer exist larg scale visual data annot imag class label bound box region descript good sourc learn rich divers visual concept howev straightforward visual concept captur transfer visual question answer model due miss link question depend answer model visual data without question task specif tackl problem two step learn task condit visual classifi base unsupervis task discoveri transfer adapt task condit visual classifi visual question answer model specif employ linguist knowledg sourc structur lexic databas e g wordnet visual descript unsupervis task discoveri adapt learn task condit visual classifi answer unit visual question answer model empir show propos algorithm gener unseen answer success use knowledg transfer visual data less
482,1810.02355,"
        In this work, we present a novel strategy for correcting imperfections in occupancy grid maps called map decay. The objective of map decay is to correct invalid occupancy probabilities of map cells that are unobservable by sensors. The strategy was inspired by an analogy between the memory architecture believed to exist in the human brain and the maps maintained by an autonomous vehicle. It consists in merging sensory information obtained during runtime (online) with a priori data from a high-precision map constructed offline. In map decay, cells observed by sensors are updated using traditional occupancy grid mapping techniques and unobserved cells are adjusted so that their occupancy probabilities tend to the values found in the offline map. This strategy is grounded in the idea that the most precise information available about an unobservable cell is the value found in the high-precision offline map. Map decay was successfully tested and is still in use in the IARA autonomous vehicle from Universidade Federal do Espírito Santo.
        △ Less
",work present novel strategi correct imperfect occup grid map call map decay object map decay correct invalid occup probabl map cell unobserv sensor strategi inspir analog memori architectur believ exist human brain map maintain autonom vehicl consist merg sensori inform obtain runtim onlin priori data high precis map construct offlin map decay cell observ sensor updat use tradit occup grid map techniqu unobserv cell adjust occup probabl tend valu found offlin map strategi ground idea precis inform avail unobserv cell valu found high precis offlin map map decay success test still use iara autonom vehicl universidad feder esp rito santo less
483,1810.02350,"
        Drivers for globalization are significant where today's organizations look for cheaper and faster ways to develop software as well as ways to satisfy quality and investment requirements imposed by customers, shareholders, and governments. Given these needs, Global Software Development (GSD) has become a ""normal"" way of doing business. Working in GSD often require teams of different cultures to work together. A poor understanding of cultural differences can create barriers to trust or missed opportunities. The literature on culture in GSD is either outdated or disparate, requiring practitioners to read many papers to get an overview of how to manage multi-cultural teams. In this study, we aim to highlight how to increase cultural awareness within teams, avoid potential conflict and harness differences for improved team spirit. To answer our research question, ""How should cultural differences be managed, identified and communicated to a GSD team?"", we conducted a systematic literature review of the GSD literature. A synthesis of solutions found in nineteen studies provided 12 distinct practices that organizations can implement, to include, ""provide a cultural knowledge base"", ""understand and make team members aware of cultural differences"" and ""plan responses to mitigate occurrences of cultural misunderstandings"". These implementable cultural practices go some way to providing solutions to managing multi-cultural development teams, and thus to support one of the problem dimensions in GSD and embrace cultural differences.
        △ Less
",driver global signific today organ look cheaper faster way develop softwar well way satisfi qualiti invest requir impos custom sharehold govern given need global softwar develop gsd becom normal way busi work gsd often requir team differ cultur work togeth poor understand cultur differ creat barrier trust miss opportun literatur cultur gsd either outdat dispar requir practition read mani paper get overview manag multi cultur team studi aim highlight increas cultur awar within team avoid potenti conflict har differ improv team spirit answer research question cultur differ manag identifi commun gsd team conduct systemat literatur review gsd literatur synthesi solut found nineteen studi provid distinct practic organ implement includ provid cultur knowledg base understand make team member awar cultur differ plan respons mitig occurr cultur misunderstand implement cultur practic go way provid solut manag multi cultur develop team thu support one problem dimens gsd embrac cultur differ less
484,1810.02348,"
        In this paper we provide nearly linear time algorithms for several problems closely associated with the classic Perron-Frobenius theorem, including computing Perron vectors, i.e. entrywise non-negative eigenvectors of non-negative matrices, and solving linear systems in asymmetric M-matrices, a generalization of Laplacian systems. The running times of our algorithms depend nearly linearly on the input size and polylogarithmically on the desired accuracy and problem condition number.
  Leveraging these results we also provide improved running times for a broader range of problems including computing random walk-based graph kernels, computing Katz centrality, and more. The running times of our algorithms improve upon previously known results which either depended polynomially on the condition number of the problem, required quadratic time, or only applied to special cases.
  We obtain these results by providing new iterative methods for reducing these problems to solving linear systems in Row-Column Diagonally Dominant (RCDD) matrices. Our methods are related to the classic shift-and-invert preconditioning technique for eigenvector computation and constitute the first alternative to the result in Cohen et al. (2016) for reducing stationary distribution computation and solving directed Laplacian systems to solving RCDD systems.
        △ Less
",paper provid nearli linear time algorithm sever problem close associ classic perron frobeniu theorem includ comput perron vector e entrywis non neg eigenvector non neg matric solv linear system asymmetr matric gener laplacian system run time algorithm depend nearli linearli input size polylogarithm desir accuraci problem condit number leverag result also provid improv run time broader rang problem includ comput random walk base graph kernel comput katz central run time algorithm improv upon previous known result either depend polynomi condit number problem requir quadrat time appli special case obtain result provid new iter method reduc problem solv linear system row column diagon domin rcdd matric method relat classic shift invert precondit techniqu eigenvector comput constitut first altern result cohen et al reduc stationari distribut comput solv direct laplacian system solv rcdd system less
485,1810.02344,"
        Motivated by the detection of prohibited objects in carry-on luggage as a part of avionic security screening, we develop a CNN-based object detection approach for multi-view X-ray image data. Our contributions are two-fold. First, we introduce a novel multi-view pooling layer to perform a 3D aggregation of 2D CNN-features extracted from each view. To that end, our pooling layer exploits the known geometry of the imaging system to ensure geometric consistency of the feature aggregation. Second, we introduce an end-to-end trainable multi-view detection pipeline based on Faster R-CNN, which derives the region proposals and performs the final classification in 3D using these aggregated multi-view features. Our approach shows significant accuracy gains compared to single-view detection while even being more efficient than performing single-view detection in each view.
        △ Less
",motiv detect prohibit object carri luggag part avion secur screen develop cnn base object detect approach multi view x ray imag data contribut two fold first introduc novel multi view pool layer perform aggreg cnn featur extract view end pool layer exploit known geometri imag system ensur geometr consist featur aggreg second introduc end end trainabl multi view detect pipelin base faster r cnn deriv region propos perform final classif use aggreg multi view featur approach show signific accuraci gain compar singl view detect even effici perform singl view detect view less
486,1810.02340,"
        Pruning large neural networks while maintaining the performance is often highly desirable due to the reduced space and time complexity. In existing methods, pruning is incorporated within an iterative optimization procedure with either heuristically designed pruning schedules or additional hyperparameters, undermining their utility. In this work, we present a new approach that prunes a given network once at initialization. Specifically, we introduce a saliency criterion based on connection sensitivity that identifies structurally important connections in the network for the given task even before training. This eliminates the need for both pretraining as well as the complex pruning schedule while making it robust to architecture variations. After pruning, the sparse network is trained in the standard way. Our method obtains extremely sparse networks with virtually the same accuracy as the reference network on image classification tasks and is broadly applicable to various architectures including convolutional, residual and recurrent networks. Unlike existing methods, our approach enables us to demonstrate that the retained connections are indeed relevant to the given task.
        △ Less
",prune larg neural network maintain perform often highli desir due reduc space time complex exist method prune incorpor within iter optim procedur either heurist design prune schedul addit hyperparamet undermin util work present new approach prune given network initi specif introduc salienc criterion base connect sensit identifi structur import connect network given task even train elimin need pretrain well complex prune schedul make robust architectur variat prune spars network train standard way method obtain extrem spars network virtual accuraci refer network imag classif task broadli applic variou architectur includ convolut residu recurr network unlik exist method approach enabl us demonstr retain connect inde relev given task less
487,1810.02338,"
        We marry two powerful ideas: deep representation learning for visual recognition and language understanding, and symbolic program execution for reasoning. Our neural-symbolic visual question answering (NS-VQA) system first recovers a structural scene representation from the image and a program trace from the question. It then executes the program on the scene representation to obtain an answer. Incorporating symbolic structure as prior knowledge offers three unique advantages. First, executing programs on a symbolic space is more robust to long program traces; our model can solve complex reasoning tasks better, achieving an accuracy of 99.8% on the CLEVR dataset. Second, the model is more data- and memory-efficient: it performs well after learning on a small number of training data; it can also encode an image into a compact representation, requiring less storage than existing methods for offline question answering. Third, symbolic program execution offers full transparency to the reasoning process; we are thus able to interpret and diagnose each execution step.
        △ Less
",marri two power idea deep represent learn visual recognit languag understand symbol program execut reason neural symbol visual question answer ns vqa system first recov structur scene represent imag program trace question execut program scene represent obtain answer incorpor symbol structur prior knowledg offer three uniqu advantag first execut program symbol space robust long program trace model solv complex reason task better achiev accuraci clevr dataset second model data memori effici perform well learn small number train data also encod imag compact represent requir less storag exist method offlin question answer third symbol program execut offer full transpar reason process thu abl interpret diagnos execut step less
488,1810.02334,"
        A central goal of unsupervised learning is to acquire representations from unlabeled data or experience that can be used for more effective learning of downstream tasks from modest amounts of labeled data. Many prior unsupervised learning works aim to do so by developing proxy objectives based on reconstruction, disentanglement, prediction, and other metrics. Instead, we develop an unsupervised learning method that explicitly optimizes for the ability to learn a variety of tasks from small amounts of data. To do so, we construct tasks from unlabeled data in an automatic way and run meta-learning over the constructed tasks. Surprisingly, we find that, when integrated with meta-learning, relatively simple mechanisms for task design, such as clustering unsupervised representations, lead to good performance on a variety of downstream tasks. Our experiments across four image datasets indicate that our unsupervised meta-learning approach acquires a learning algorithm without any labeled data that is applicable to a wide range of downstream classification tasks, improving upon the representation learned by four prior unsupervised learning methods.
        △ Less
",central goal unsupervis learn acquir represent unlabel data experi use effect learn downstream task modest amount label data mani prior unsupervis learn work aim develop proxi object base reconstruct disentangl predict metric instead develop unsupervis learn method explicitli optim abil learn varieti task small amount data construct task unlabel data automat way run meta learn construct task surprisingli find integr meta learn rel simpl mechan task design cluster unsupervis represent lead good perform varieti downstream task experi across four imag dataset indic unsupervis meta learn approach acquir learn algorithm without label data applic wide rang downstream classif task improv upon represent learn four prior unsupervis learn method less
489,1810.02328,"
        Memorization is worst-case generalization. Based on MacKay's information theoretic model of supervised machine learning, this article discusses how to practically estimate the maximum size of a neural network given a training data set. First, we present four easily applicable rules to analytically determine the capacity of neural network architectures. This allows the comparison of the efficiency of different network architectures independently of a task. Second, we introduce and experimentally validate a heuristic method to estimate the neural network capacity requirement for a given dataset and labeling. This allows an estimate of the required size of a neural network for a given problem. We conclude the article with a discussion on the consequences of sizing the network wrongly, which includes both increased computation effort for training as well as reduced generalization capability.
        △ Less
",memor worst case gener base mackay inform theoret model supervis machin learn articl discuss practic estim maximum size neural network given train data set first present four easili applic rule analyt determin capac neural network architectur allow comparison effici differ network architectur independ task second introduc experiment valid heurist method estim neural network capac requir given dataset label allow estim requir size neural network given problem conclud articl discuss consequ size network wrongli includ increas comput effort train well reduc gener capabl less
490,1810.02320,"
        The extraction of geological lineaments from digital satellite data is a fundamental application in remote sensing. The location of geological lineaments such as faults and dykes are of interest for a range of applications, particularly because of their association with hydrothermal mineralization. Although a wide range of applications have utilized computer vision techniques, a standard workflow for application of these techniques to mineral exploration is lacking. We present a framework for extracting geological lineaments using computer vision techniques which is a combination of edge detection and line extraction algorithms for extracting geological lineaments using optical remote sensing data. It features ancillary computer vision techniques for reducing data dimensionality, removing noise and enhancing the expression of lineaments. We test the proposed framework on Landsat 8 data of a mineral-rich portion of the Gascoyne Province in Western Australia using different dimension reduction techniques and convolutional filters. To validate the results, the extracted lineaments are compared to our manual photointerpretation and geologically mapped structures by the Geological Survey of Western Australia (GSWA). The results show that the best correlation between our extracted geological lineaments and the GSWA geological lineament map is achieved by applying a minimum noise fraction transformation and a Laplacian filter. Application of a directional filter instead shows a stronger correlation with the output of our manual photointerpretation and known sites of hydrothermal mineralization. Hence, our framework using either filter can be used for mineral prospectivity mapping in other regions where faults are exposed and observable in optical remote sensing data.
        △ Less
",extract geolog lineament digit satellit data fundament applic remot sens locat geolog lineament fault dyke interest rang applic particularli associ hydrotherm miner although wide rang applic util comput vision techniqu standard workflow applic techniqu miner explor lack present framework extract geolog lineament use comput vision techniqu combin edg detect line extract algorithm extract geolog lineament use optic remot sens data featur ancillari comput vision techniqu reduc data dimension remov nois enhanc express lineament test propos framework landsat data miner rich portion gascoyn provinc western australia use differ dimens reduct techniqu convolut filter valid result extract lineament compar manual photointerpret geolog map structur geolog survey western australia gswa result show best correl extract geolog lineament gswa geolog lineament map achiev appli minimum nois fraction transform laplacian filter applic direct filter instead show stronger correl output manual photointerpret known site hydrotherm miner henc framework use either filter use miner prospect map region fault expos observ optic remot sens data less
491,1810.02318,"
        Browsing privacy solutions face an uphill battle to deployment. Many operate counter to the economic objectives of popular online services (e.g., by completely blocking ads) and do not provide enough incentive for users who may be subject to performance degradation for deploying them. In this study, we take a step towards realizing a system for online privacy that is mutually beneficial to users and online advertisers: an information market. This system not only maintains economic viability for online services, but also provides users with financial compensation to encourage them to participate. We prototype and evaluate an information market that provides privacy and revenue to users while preserving and sometimes improving their Web performance. We evaluate feasibility of the market via a one month field study with 63 users and find that users are indeed willing to sell their browsing information. We also use Web traces of millions of users to drive a simulation study to evaluate the system at scale. We find that the system can indeed be profitable to both users and online advertisers.
        △ Less
",brows privaci solut face uphil battl deploy mani oper counter econom object popular onlin servic e g complet block ad provid enough incent user may subject perform degrad deploy studi take step toward realiz system onlin privaci mutual benefici user onlin advertis inform market system maintain econom viabil onlin servic also provid user financi compens encourag particip prototyp evalu inform market provid privaci revenu user preserv sometim improv web perform evalu feasibl market via one month field studi user find user inde will sell brows inform also use web trace million user drive simul studi evalu system scale find system inde profit user onlin advertis less
492,1810.02315,"
        Electricity distribution networks (DNs) in many regions are increasingly subjected to disruptions caused by tropical storms. Distributed Energy Resources (DERs) can act as temporary supply sources to sustain ""microgrids"" resulting from disruptions. In this paper, we investigate the problem of suitable DER allocation to facilitate more efficient repair operations and faster recovery. First, we estimate the failure probabilities of DN components (lines) using a stochastic model of line failures which parametrically depends on the location-specific storm wind field. Next, we formulate a two-stage stochastic mixed integer program, which models the distribution utility's decision to allocate DERs in the DN (pre-storm stage); and accounts for multi-period decisions on optimal dispatch and line repair scheduling (post-storm stage). A key feature of this formulation is that it jointly optimizes electricity dispatch within the individual microgrids and the line repair schedules to minimize the sum of the cost of DER allocation and cost due to lost load. To illustrate our approach, we use the sample average approximation method to solve our problem for a small-size DN under different storm intensities and DER/crew constraints.
        △ Less
",electr distribut network dn mani region increasingli subject disrupt caus tropic storm distribut energi resourc der act temporari suppli sourc sustain microgrid result disrupt paper investig problem suitabl der alloc facilit effici repair oper faster recoveri first estim failur probabl dn compon line use stochast model line failur parametr depend locat specif storm wind field next formul two stage stochast mix integ program model distribut util decis alloc der dn pre storm stage account multi period decis optim dispatch line repair schedul post storm stage key featur formul jointli optim electr dispatch within individu microgrid line repair schedul minim sum cost der alloc cost due lost load illustr approach use sampl averag approxim method solv problem small size dn differ storm intens der crew constraint less
493,1810.02309,"
        The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a rich class of LDR matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component. This class generalizes several previous constructions while preserving compression and efficient computation. We prove bounds on the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes exceed the accuracy of existing compression approaches, and on some tasks even outperform general unstructured layers while using more than 20X fewer parameters.
        △ Less
",low displac rank ldr framework structur matric repres matrix two displac oper low rank residu exist use ldr matric deep learn appli fix displac oper encod form shift invari akin convolut introduc rich class ldr matric gener displac oper explicitli learn oper low rank compon class gener sever previou construct preserv compress effici comput prove bound vc dimens multi layer neural network structur weight matric show empir compact parameter reduc sampl complex learn replac weight layer fulli connect convolut recurr neural network imag classif languag model task new class exceed accuraci exist compress approach task even outperform gener unstructur layer use x fewer paramet less
494,1810.02304,"
        The kth-power of a given graph G=(V,E) is obtained from G by adding an edge between every two distinct vertices at a distance at most k in G. We call G a k-Steiner power if it is an induced subgraph of the kth-power of some tree. Our main contribution is a polynomial-time recognition algorithm of 4-Steiner powers, thereby extending the decade-year-old results of (Lin, Kearney and Jiang, ISAAC'00) for k=1,2 and (Chang and Ko, WG'07) for k=3. A graph G is termed k-leaf power if there is some tree T such that: all vertices in V(G) are leaf-nodes of T, and G is an induced subgraph of the kth-power of T. As a byproduct of our main result, we give the first known polynomial-time recognition algorithm for 6-leaf powers.
        △ Less
",kth power given graph g v e obtain g ad edg everi two distinct vertic distanc k g call g k steiner power induc subgraph kth power tree main contribut polynomi time recognit algorithm steiner power therebi extend decad year old result lin kearney jiang isaac k chang ko wg k graph g term k leaf power tree vertic v g leaf node g induc subgraph kth power byproduct main result give first known polynomi time recognit algorithm leaf power less
495,1810.02303,"
        We propose a novel approach for performing convolution of signals on curved surfaces and show its utility in a variety of geometric deep learning applications. Key to our construction is the notion of directional functions defined on the surface, which extend the classic real-valued signals and which can be naturally convolved with with real-valued template functions. As a result, rather than trying to fix a canonical orientation or only keeping the maximal response across all alignments of a 2D template at every point of the surface, as done in previous works, we show how information across all rotations can be kept across different layers of the neural network. Our construction, which we call multi-directional geodesic convolution, or directional convolution for short, allows, in particular, to propagate and relate directional information across layers and thus different regions on the shape. We first define directional convolution in the continuous setting, prove its key properties and then show how it can be implemented in practice, for shapes represented as triangle meshes. We evaluate directional convolution in a wide variety of learning scenarios ranging from classification of signals on surfaces, to shape segmentation and shape matching, where we show a significant improvement over several baselines.
        △ Less
",propos novel approach perform convolut signal curv surfac show util varieti geometr deep learn applic key construct notion direct function defin surfac extend classic real valu signal natur convolv real valu templat function result rather tri fix canon orient keep maxim respons across align templat everi point surfac done previou work show inform across rotat kept across differ layer neural network construct call multi direct geodes convolut direct convolut short allow particular propag relat direct inform across layer thu differ region shape first defin direct convolut continu set prove key properti show implement practic shape repres triangl mesh evalu direct convolut wide varieti learn scenario rang classif signal surfac shape segment shape match show signific improv sever baselin less
496,1810.02286,"
        SiMRX is a MRX simulation toolbox written in MATLAB for simulation of realistic 2D and 3D Magnetorelaxometry (MRX) setups, including coils, sensors and activation patterns. MRX is a new modality that uses magnetic nanoparticles (MNP) as contrast agent and shows promising results in medical applications, e.g. cancer treatment. Its basic principles were outlined in [Baumgarten et al., 2008], further elaborated in [Liebl et al., 2014], transferred into a rigorous mathematical model and analyzed in [Föcke et al., 2018].
  SiMRX is available at https://gitlab.com/foecke/SiMRX/.
        △ Less
",simrx mrx simul toolbox written matlab simul realist magnetorelaxometri mrx setup includ coil sensor activ pattern mrx new modal use magnet nanoparticl mnp contrast agent show promis result medic applic e g cancer treatment basic principl outlin baumgarten et al elabor liebl et al transfer rigor mathemat model analyz f cke et al simrx avail http gitlab com foeck simrx less
497,1810.02283,"
        Single image dehazing is a challenging ill-posed restoration problem. Various prior-based and learning-based methods have been proposed. Most of them follow a classic atmospheric scattering model which is an elegant simplified physical model based on the assumption of single-scattering and homogeneous atmospheric medium. The formulation of haze in realistic environment is more complicated. In this paper, we propose to take its essential mechanism as ""black box"", and focus on learning an input-adaptive trainable end-to-end dehazing model. An U-Net like encoder-decoder deep network via progressive feature fusions has been proposed to directly learn highly nonlinear transformation function from observed hazy image to haze-free ground-truth. The proposed network is evaluated on two public image dehazing benchmarks. The experiments demonstrate that it can achieve superior performance when compared with popular state-of-the-art methods. With efficient GPU memory usage, it can satisfactorily recover ultra high definition hazed image up to 4K resolution, which is unaffordable by many deep learning based dehazing algorithms.
        △ Less
",singl imag dehaz challeng ill pose restor problem variou prior base learn base method propos follow classic atmospher scatter model eleg simplifi physic model base assumpt singl scatter homogen atmospher medium formul haze realist environ complic paper propos take essenti mechan black box focu learn input adapt trainabl end end dehaz model u net like encod decod deep network via progress featur fusion propos directli learn highli nonlinear transform function observ hazi imag haze free ground truth propos network evalu two public imag dehaz benchmark experi demonstr achiev superior perform compar popular state art method effici gpu memori usag satisfactorili recov ultra high definit haze imag k resolut unafford mani deep learn base dehaz algorithm less
498,1810.02281,"
        We analyze speed of convergence to global optimum for gradient descent training a deep linear neural network (parameterized as $x\mapsto W_N \cdots W_1x$) by minimizing the $\ell_2$ loss over whitened data. Convergence at a linear rate is guaranteed when the following hold: (i) dimensions of hidden layers are at least the minimum of the input and output dimensions; (ii) weight matrices at initialization are approximately balanced; and (iii) the initial loss is smaller than the loss of any rank-deficient solution. The assumptions on initialization (conditions (ii) and (iii)) are necessary, in the sense that violating any one of them may lead to convergence failure. Moreover, in the important case of output dimension 1, i.e. scalar regression, they are met, and thus convergence to global optimum holds, with constant probability under a random initialization scheme. Our results significantly extend previous analyses, e.g., of deep linear residual networks (Bartlett et al., 2018).
        △ Less
",analyz speed converg global optimum gradient descent train deep linear neural network parameter x mapsto w n cdot w x minim ell loss whiten data converg linear rate guarante follow hold dimens hidden layer least minimum input output dimens ii weight matric initi approxim balanc iii initi loss smaller loss rank defici solut assumpt initi condit ii iii necessari sens violat one may lead converg failur moreov import case output dimens e scalar regress met thu converg global optimum hold constant probabl random initi scheme result significantli extend previou analys e g deep linear residu network bartlett et al less
499,1810.02277,"
        Cardiovascular disease (CVD) is a leading cause of death in the lung cancer screening population. Chest CT scans made in lung cancer screening are suitable for identification of participants at risk of CVD. Existing methods analyzing CT images from lung cancer screening for prediction of CVD events or mortality use engineered features extracted from the images combined with patient information. In this work we propose a method that automatically predicts 5-year cardiovascular mortality directly from chest CT scans without the need for hand-crafting image features. A set of 1,583 participants of the National Lung Screening Trial was included (1,188 survivors, 395 non-survivors). Low-dose chest CT images acquired at baseline were analyzed and the follow-up time was 5 years. To limit the analysis to the heart region, the heart was first localized by our previously developed algorithm for organ localization exploiting convolutional neural networks. Thereafter, a convolutional autoencoder was used to encode the identified heart region. Finally, based on the extracted encodings subjects were classified into survivors or non-survivors using a support vector machine classifier. The performance of the method was assessed in eight cross-validation experiments with 1,433 images used for training, 50 for validation and 100 for testing. The method achieved a performance with an area under the ROC curve of 0.72. The results demonstrate that prediction of cardiovascular mortality directly from low-dose screening chest CT scans, without hand-crafted features, is feasible, allowing identification of subjects at risk of fatal CVD events.
        △ Less
",cardiovascular diseas cvd lead caus death lung cancer screen popul chest ct scan made lung cancer screen suitabl identif particip risk cvd exist method analyz ct imag lung cancer screen predict cvd event mortal use engin featur extract imag combin patient inform work propos method automat predict year cardiovascular mortal directli chest ct scan without need hand craft imag featur set particip nation lung screen trial includ survivor non survivor low dose chest ct imag acquir baselin analyz follow time year limit analysi heart region heart first local previous develop algorithm organ local exploit convolut neural network thereaft convolut autoencod use encod identifi heart region final base extract encod subject classifi survivor non survivor use support vector machin classifi perform method assess eight cross valid experi imag use train valid test method achiev perform area roc curv result demonstr predict cardiovascular mortal directli low dose screen chest ct scan without hand craft featur feasibl allow identif subject risk fatal cvd event less
500,1810.02276,"
        Grant-free non-orthogonal multiple access (NOMA) has been regarded as a key-enabler technology for ultra-reliable and low-latency communications (URLLC). In this paper, we analyse the performance of NOMA with short packet communications for URLLC. In this regard, the overall packet loss probability consists of transmission error probability and queueing-delay violation probability. Queueing-delay has been modelled using the effective bandwidth. Due to short transmission time, the infinite block-length has been replaced with finite blocklength of the channel codes which rules out the application of Shannon's formula. The achievable effective bandwidth of the system is derived, and then, the transmission error probability has been analysed. The derivations are validated through extensive simulations, which shows the variations of the signal-to-noise ratio (SNR) requirements of the system for various transmission-error probability, QoS exponent, and the transmission packet size.
        △ Less
",grant free non orthogon multipl access noma regard key enabl technolog ultra reliabl low latenc commun urllc paper analys perform noma short packet commun urllc regard overal packet loss probabl consist transmiss error probabl queue delay violat probabl queue delay model use effect bandwidth due short transmiss time infinit block length replac finit blocklength channel code rule applic shannon formula achiev effect bandwidth system deriv transmiss error probabl analys deriv valid extens simul show variat signal nois ratio snr requir system variou transmiss error probabl qo expon transmiss packet size less
501,1810.02274,"
        Rewards are sparse in the real world and most today's reinforcement learning algorithms struggle with such sparsity. One solution to this problem is to allow the agent to create rewards for itself - thus making rewards dense and more suitable for learning. In particular, inspired by curious behaviour in animals, observing something novel could be rewarded with a bonus. Such bonus is summed up with the real task reward - making it possible for RL algorithms to learn from the combined reward. We propose a new curiosity method which uses episodic memory to form the novelty bonus. To determine the bonus, the current observation is compared with the observations in memory. Crucially, the comparison is done based on how many environment steps it takes to reach the current observation from those in memory - which incorporates rich information about environment dynamics. This allows us to overcome the known ""couch-potato"" issues of prior work - when the agent finds a way to instantly gratify itself by exploiting actions which lead to unpredictable consequences. We test our approach in visually rich 3D environments in ViZDoom and DMLab. In ViZDoom, our agent learns to successfully navigate to a distant goal at least 2 times faster than the state-of-the-art curiosity method ICM. In DMLab, our agent generalizes well to new procedurally generated levels of the game - reaching the goal at least 2 times more frequently than ICM on test mazes with very sparse reward.
        △ Less
",reward spars real world today reinforc learn algorithm struggl sparsiti one solut problem allow agent creat reward thu make reward dens suitabl learn particular inspir curiou behaviour anim observ someth novel could reward bonu bonu sum real task reward make possibl rl algorithm learn combin reward propos new curios method use episod memori form novelti bonu determin bonu current observ compar observ memori crucial comparison done base mani environ step take reach current observ memori incorpor rich inform environ dynam allow us overcom known couch potato issu prior work agent find way instantli gratifi exploit action lead unpredict consequ test approach visual rich environ vizdoom dmlab vizdoom agent learn success navig distant goal least time faster state art curios method icm dmlab agent gener well new procedur gener level game reach goal least time frequent icm test maze spars reward less
502,1810.02272,"
        Over the past few years Caffe, from Berkeley AI Research, has gained a strong following in the deep learning community with over 15K forks on the github.com/BLVC/Caffe site. With its well organized, very modular C++ design it is easy to work with and very fast. However, in the world of Windows development, C# has helped accelerate development with many of the enhancements that it offers over C++, such as garbage collection, a very rich .NET programming framework and easy database access via Entity Frameworks. So how can a C# developer use the advances of C# to take full advantage of the benefits offered by the Berkeley Caffe deep learning system? The answer is the fully open source, 'MyCaffe' for Windows .NET programmers. MyCaffe is an open source, complete C# language re-write of Berkeley's Caffe. This article describes the general architecture of MyCaffe including the newly added MyCaffeTrainerRL for Reinforcement Learning. In addition, this article discusses how MyCaffe closely follows the C++ Caffe, while talking efficiently to the low level NVIDIA CUDA hardware to offer a high performance, highly programmable deep learning system for Windows .NET programmers.
        △ Less
",past year caff berkeley ai research gain strong follow deep learn commun k fork github com blvc caff site well organ modular c design easi work fast howev world window develop c help acceler develop mani enhanc offer c garbag collect rich net program framework easi databas access via entiti framework c develop use advanc c take full advantag benefit offer berkeley caff deep learn system answer fulli open sourc mycaff window net programm mycaff open sourc complet c languag write berkeley caff articl describ gener architectur mycaff includ newli ad mycaffetrainerrl reinforc learn addit articl discuss mycaff close follow c caff talk effici low level nvidia cuda hardwar offer high perform highli programm deep learn system window net programm less
503,1810.02270,"
        The Binary Search Tree (BST) is average in computer science which supports a compact data structure in memory and oneself even conducts a row of quick algorithms, by which people often apply it in dynamical circumstance. Besides these edges, it is also with weakness on its own structure specially with poor performance at worst case. In this paper, we will develop this data structure into a synthesis to show a series of novel features residing in. Of that, there are new methods invented for raising the performance and efficiency nevertheless some existing ones in logarithm or linear time.
        △ Less
",binari search tree bst averag comput scienc support compact data structur memori oneself even conduct row quick algorithm peopl often appli dynam circumst besid edg also weak structur special poor perform worst case paper develop data structur synthesi show seri novel featur resid new method invent rais perform effici nevertheless exist one logarithm linear time less
504,1810.02268,"
        The translation of pronouns presents a special challenge to machine translation to this day, since it often requires context outside the current sentence. Recent work on models that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as BLEU. However, metrics that quantify the overall translation quality are ill-equipped to measure gains from additional context. We argue that a different kind of evaluation is needed to assess how well models translate inter-sentential phenomena such as pronouns. This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns. Furthermore, we perform experiments with several context-aware models. We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of accuracy on our contrastive test set. Our experiments also show the effectiveness of parameter tying for multi-encoder architectures.
        △ Less
",translat pronoun present special challeng machin translat day sinc often requir context outsid current sentenc recent work model access inform across sentenc boundari seen moder improv term automat evalu metric bleu howev metric quantifi overal translat qualiti ill equip measur gain addit context argu differ kind evalu need assess well model translat inter sententi phenomena pronoun paper therefor present test suit contrast translat focus specif translat pronoun furthermor perform experi sever context awar model show gain bleu moder system outperform baselin larg margin term accuraci contrast test set experi also show effect paramet tie multi encod architectur less
505,1810.02266,"
        Learning from data streams is an increasingly important topic in data mining, machine learning, and artificial intelligence in general. A major focus in the data stream literature is on designing methods that can deal with concept drift, a challenge where the generating distribution changes over time. A general assumption in most of this literature is that instances are independently distributed in the stream. In this work we show that, in the context of concept drift, this assumption is contradictory, and that the presence of concept drift necessarily implies temporal dependence; and thus some form of time series. This has important implications on model design and deployment. We explore and highlight the these implications, and show that Hoeffding-tree based ensembles, which are very popular for learning in streams, are not naturally suited to learning \emph{within} drift; and can perform in this scenario only at significant computational cost of destructive adaptation. On the other hand, we develop and parameterize gradient-descent methods and demonstrate how they can perform \emph{continuous} adaptation with no explicit drift-detection mechanism, offering major advantages in terms of accuracy and efficiency. As a consequence of our theoretical discussion and empirical observations, we outline a number of recommendations for deploying methods in concept-drifting streams.
        △ Less
",learn data stream increasingli import topic data mine machin learn artifici intellig gener major focu data stream literatur design method deal concept drift challeng gener distribut chang time gener assumpt literatur instanc independ distribut stream work show context concept drift assumpt contradictori presenc concept drift necessarili impli tempor depend thu form time seri import implic model design deploy explor highlight implic show hoeffd tree base ensembl popular learn stream natur suit learn emph within drift perform scenario signific comput cost destruct adapt hand develop parameter gradient descent method demonstr perform emph continu adapt explicit drift detect mechan offer major advantag term accuraci effici consequ theoret discuss empir observ outlin number recommend deploy method concept drift stream less
506,1810.02254,"
        Logic program transformation by the unfold/fold method ad- vocates the writing of correct logic programs via the application of some rules to a naive program. This work focuses on how to overcome subgoal- introduction difficulties in synthesizing efficient sorting algorithms from an naive sorting algorithm, through logic program transformation and abductive reasoning.
        △ Less
",logic program transform unfold fold method ad vocat write correct logic program via applic rule naiv program work focus overcom subgoal introduct difficulti synthes effici sort algorithm naiv sort algorithm logic program transform abduct reason less
507,1810.02251,"
        This paper introduces DATA Agent, a system which creates murder mystery adventures from open data. In the game, the player takes on the role of a detective tasked with finding the culprit of a murder. All characters, places, and items in DATA Agent games are generated using open data as source content. The paper discusses the general game design and user interface of DATA Agent, and provides details on the generative algorithms which transform linked data into different game objects. Findings from a user study with 30 participants playing through two games of DATA Agent show that the game is easy and fun to play, and that the mysteries it generates are straightforward to solve.
        △ Less
",paper introduc data agent system creat murder mysteri adventur open data game player take role detect task find culprit murder charact place item data agent game gener use open data sourc content paper discuss gener game design user interfac data agent provid detail gener algorithm transform link data differ game object find user studi particip play two game data agent show game easi fun play mysteri gener straightforward solv less
508,1810.02245,"
        We present a simple and accurate span-based model for semantic role labeling (SRL). Our model directly takes into account all possible argument spans and scores them for each label. At decoding time, we greedily select higher scoring labeled spans. One advantage of our model is to allow us to design and use span-level features, that are difficult to use in token-based BIO tagging approaches. Experimental results demonstrate that our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively.
        △ Less
",present simpl accur span base model semant role label srl model directli take account possibl argument span score label decod time greedili select higher score label span one advantag model allow us design use span level featur difficult use token base bio tag approach experiment result demonstr ensembl model achiev state art result f f conll dataset respect less
509,1810.02244,"
        In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically---showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the $1$-dimensional Weisfeiler-Leman graph isomorphism heuristic ($1$-WL). We show that GNNs have the same expressiveness as the $1$-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called $k$-dimensional GNNs ($k$-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.
        △ Less
",recent year graph neural network gnn emerg power neural architectur learn vector represent node graph supervis end end fashion gnn evalu empir show promis result follow work investig gnn theoret point view relat dimension weisfeil leman graph isomorph heurist wl show gnn express wl term distinguish non isomorph sub graph henc algorithm also shortcom base propos gener gnn call k dimension gnn k gnn take higher order graph structur multipl scale account higher order structur play essenti role character social network molecul graph experiment evalu confirm theoret find well confirm higher order inform use task graph classif regress less
510,1810.02243,"
        In this study, a shell-and-tube heat exchanger (STHX) design based on seven continuous independent design variables is proposed. Delayed Rejection Adaptive Metropolis hasting (DRAM) was utilized as a powerful tool in the Markov chain Monte Carlo (MCMC) sampling method. This Reverse Sampling (RS) method was used to find the probability distribution of design variables of the shell and tube heat exchanger. Thanks to this probability distribution, an uncertainty analysis was also performed to find the quality of these variables. In addition, a decision-making strategy based on confidence intervals of design variables and on the Total Annual Cost (TAC) provides the final selection of design variables. Results indicated high accuracies for the estimation of design variables which leads to marginally improved performance compared to commonly used optimization methods. In order to verify the capability of the proposed method, a case of study is also presented, it shows that a significant cost reduction is feasible with respect to multi-objective and single-objective optimization methods. Furthermore, the selected variables have good quality (in terms of probability distribution) and a lower TAC was also achieved. Results show that the costs of the proposed design are lower than those obtained from optimization method reported in previous studies. The algorithm was also used to determine the impact of using probability values for the design variables rather than single values to obtain the best heat transfer area and pumping power. In particular, a reduction of the TAC up to 3.5% was achieved in the case considered.
        △ Less
",studi shell tube heat exchang sthx design base seven continu independ design variabl propos delay reject adapt metropoli hast dram util power tool markov chain mont carlo mcmc sampl method revers sampl rs method use find probabl distribut design variabl shell tube heat exchang thank probabl distribut uncertainti analysi also perform find qualiti variabl addit decis make strategi base confid interv design variabl total annual cost tac provid final select design variabl result indic high accuraci estim design variabl lead margin improv perform compar commonli use optim method order verifi capabl propos method case studi also present show signific cost reduct feasibl respect multi object singl object optim method furthermor select variabl good qualiti term probabl distribut lower tac also achiev result show cost propos design lower obtain optim method report previou studi algorithm also use determin impact use probabl valu design variabl rather singl valu obtain best heat transfer area pump power particular reduct tac achiev case consid less
511,1810.02241,"
        This papers studies the expressive and computational power of discrete Ordinary Differential Equations (ODEs). It presents a new framework using discrete ODEs as a central tool for computation and provides several implicit characterizations of complexity and computability classes.
  The proposed framework presents an original point of view on complexity and computability classes. It also unifies in an elegant settings various constructions that have been proposed for characterizing these classes. This includes Cobham's and, Bellantoni and Cook's definition of polynomial time and later extensions on the approach, as well as recent characterizations of computability and complexity by classes of ordinary differential equations. It also helps understanding the relationships between analog computations and classical discrete models of computation theory.
  At a more technical point of view, this paper points out the fundamental role of linear (discrete) ordinary differential equations and classical ODE tools such as changes of variables to capture computability and complexity measures, or as a tool for programming various algorithms.
        △ Less
",paper studi express comput power discret ordinari differenti equat ode present new framework use discret ode central tool comput provid sever implicit character complex comput class propos framework present origin point view complex comput class also unifi eleg set variou construct propos character class includ cobham bellantoni cook definit polynomi time later extens approach well recent character comput complex class ordinari differenti equat also help understand relationship analog comput classic discret model comput theori technic point view paper point fundament role linear discret ordinari differenti equat classic ode tool chang variabl captur comput complex measur tool program variou algorithm less
512,1810.02239,"
        Corrado Böhm once observed that if $Y$ is any fixed point combinator (fpc), then $Y(λyx.x(yx))$ is again fpc.  He thus discovered the first ""fpc generating scheme"" -- a generic way to build new fpcs from old.  Continuing this idea, define an \emph{fpc generator} to be any sequence of terms $G_1,\dots,G_n$ such that $$Y \text{ is fpc } \Longrightarrow  YG_1\cdots G_n \text{ is fpc}$$ In this contribution, we take first steps in studying the structure of (weak) fpc generators. We isolate several classes of such generators, and examine elementary properties like injectivity and constancy. We provide sufficient conditions for existence of fixed points of a given generator $(G_1,..,G_n)$: an fpc $Y$ such that $Y = YG_1\cdots G_n$. We conjecture that weak constancy is a necessary condition for existence of such (higher-order) fixed points.  This generalizes Statman's conjecture on the non-existence of ``double fpcs'': fixed points of the generator $(G) = (λyx.x(yx))$ discovered by Böhm.
        △ Less
",corrado b hm observ fix point combin fpc yx x yx fpc thu discov first fpc gener scheme gener way build new fpc old continu idea defin emph fpc gener sequenc term g dot g n text fpc longrightarrow yg cdot g n text fpc contribut take first step studi structur weak fpc gener isol sever class gener examin elementari properti like inject constanc provid suffici condit exist fix point given gener g g n fpc yg cdot g n conjectur weak constanc necessari condit exist higher order fix point gener statman conjectur non exist doubl fpc fix point gener g yx x yx discov b hm less
513,1810.02231,"
        We consider unions of interior disjoint discs in the plane such that the graph whose vertices are disc centers and edges connect centers of mutually tangent discs is triangulated, called compact packings. There is only one compact packing by discs all of the same size, called the hexagonal compact packing. It has been proven that there are exactly $9$ values of $r$ which allow a compact packing with discs of radius $1$ and $r$. It has also been proven that at most $11462$ pairs $(r,s)$ allow a compact packing with discs of radius $1$, $r$ and $s$. This paper shows that there are exactly $164$ such pairs $(r,s)$.
        △ Less
",consid union interior disjoint disc plane graph whose vertic disc center edg connect center mutual tangent disc triangul call compact pack one compact pack disc size call hexagon compact pack proven exactli valu r allow compact pack disc radiu r also proven pair r allow compact pack disc radiu r paper show exactli pair r less
514,1810.02229,"
        This paper reports on a set of experiments with different word embeddings to initialize a state-of-the-art Bi-LSTM-CRF network for event detection and classification in Italian, following the EVENTI evaluation exercise. The net- work obtains a new state-of-the-art result by improving the F1 score for detection of 1.3 points, and of 6.5 points for classification, by using a single step approach. The results also provide further evidence that embeddings have a major impact on the performance of such architectures.
        △ Less
",paper report set experi differ word embed initi state art bi lstm crf network event detect classif italian follow eventi evalu exercis net work obtain new state art result improv f score detect point point classif use singl step approach result also provid evid embed major impact perform architectur less
515,1810.02227,"
        Algorithms that rely on a pseudorandom number generator often lose their performance guarantees when adversaries can predict the behavior of the generator. To protect non-cryptographic applications against such attacks, we propose 'strong' pseudorandom generators characterized by two properties: computationally indistinguishable from random and backtracking-resistant. Some existing cryptographically secure generators also meet these criteria, but they are too slow to be accepted for general-purpose use. We introduce a new open-sourced generator called 'Randen' and show that it is 'strong' in addition to outperforming Mersenne Twister, PCG, ChaCha8, ISAAC and Philox in real-world benchmarks. This is made possible by hardware acceleration. Randen is an instantiation of Reverie, a recently published robust sponge-like random generator, with a new permutation built from an improved generalized Feistel structure with 16 branches. We provide new bounds on active s-boxes for up to 24 rounds of this construction, made possible by a memory-efficient search algorithm. Replacing existing generators with Randen can protect randomized algorithms such as reservoir sampling from attack. The permutation may also be useful for wide-block ciphers and hashing functions.
        △ Less
",algorithm reli pseudorandom number gener often lose perform guarante adversari predict behavior gener protect non cryptograph applic attack propos strong pseudorandom gener character two properti comput indistinguish random backtrack resist exist cryptograph secur gener also meet criteria slow accept gener purpos use introduc new open sourc gener call randen show strong addit outperform mersenn twister pcg chacha isaac philox real world benchmark made possibl hardwar acceler randen instanti reveri recent publish robust spong like random gener new permut built improv gener feistel structur branch provid new bound activ box round construct made possibl memori effici search algorithm replac exist gener randen protect random algorithm reservoir sampl attack permut may also use wide block cipher hash function less
516,1810.02225,"
        In this paper, we firstly introduce a method to efficiently implement large-scale high-dimensional convolution with realistic memristor-based circuit components. An experiment verified simulator is adapted for accurate prediction of analog crossbar behavior. An improved conversion algorithm is developed to convert convolution kernels to memristor-based circuits, which minimizes the error with consideration of the data and kernel patterns in CNNs. With circuit simulation for all convolution layers in ResNet-20, we found that 8-bit ADC/DAC is necessary to preserve software level classification accuracy.
        △ Less
",paper firstli introduc method effici implement larg scale high dimension convolut realist memristor base circuit compon experi verifi simul adapt accur predict analog crossbar behavior improv convers algorithm develop convert convolut kernel memristor base circuit minim error consider data kernel pattern cnn circuit simul convolut layer resnet found bit adc dac necessari preserv softwar level classif accuraci less
517,1810.02223,"
        Electroencephalography (EEG) signals are known to manifest differential patterns when individuals visually concentrate on different objects (e.g., a car). In this work, we present an end-to-end digital fabrication system , Brain2Object, to print the 3D object that an individual is observing by solely decoding visually-evoked EEG brain signal streams. We propose a unified training framework which combines multi-class Common Spatial Pattern and deep Convolutional Neural Networks to support the backend computation. Specially, a Dynamical Graph Representation of EEG signals is learned for accurately capturing the structured spatial correlations of EEG channels in an adaptive manner. A user friendly interface is developed as the system front end. Brain2Object presents a streamlined end-to-end workflow which can serve as a template for deeper integration of BCI technologies to assist with our routine activities. The proposed system is evaluated extensively using offline experiments and through an online demonstrator. For the former, we use a rich widely used public dataset and a limited but locally collected dataset. The experiment results show that our approach consistently outperforms a wide range of baseline and state-of-the-art approaches. The proof-of-concept corroborates the practicality of our approach and illustrates the ease with which such a system could be deployed.
        △ Less
",electroencephalographi eeg signal known manifest differenti pattern individu visual concentr differ object e g car work present end end digit fabric system brain object print object individu observ sole decod visual evok eeg brain signal stream propos unifi train framework combin multi class common spatial pattern deep convolut neural network support backend comput special dynam graph represent eeg signal learn accur captur structur spatial correl eeg channel adapt manner user friendli interfac develop system front end brain object present streamlin end end workflow serv templat deeper integr bci technolog assist routin activ propos system evalu extens use offlin experi onlin demonstr former use rich wide use public dataset limit local collect dataset experi result show approach consist outperform wide rang baselin state art approach proof concept corrobor practic approach illustr eas system could deploy less
518,1810.02201,"
        In the clinical routine, short axis (SA) cine cardiac MR (CMR) image stacks are acquired during multiple subsequent breath-holds. If the patient cannot consistently hold the breath at the same position, the acquired image stack will be affected by inter-slice respiratory motion and will not correctly represent the cardiac volume, introducing potential errors in the following analyses and visualisations. We propose an approach to automatically correct inter-slice respiratory motion in SA CMR image stacks. Our approach makes use of probabilistic segmentation maps (PSMs) of the left ventricular (LV) cavity generated with decision forests. PSMs are generated for each slice of the SA stack and rigidly registered in-plane to a target PSM. If long axis (LA) images are available, PSMs are generated for them and combined to create the target PSM; if not, the target PSM is produced from the same stack using a 3D model trained from motion-free stacks. The proposed approach was tested on a dataset of SA stacks acquired from 24 healthy subjects (for which anatomical 3D cardiac images were also available as reference) and compared to two techniques which use LA intensity images and LA segmentations as targets, respectively. The results show the accuracy and robustness of the proposed approach in motion compensation.
        △ Less
",clinic routin short axi sa cine cardiac mr cmr imag stack acquir multipl subsequ breath hold patient cannot consist hold breath posit acquir imag stack affect inter slice respiratori motion correctli repres cardiac volum introduc potenti error follow analys visualis propos approach automat correct inter slice respiratori motion sa cmr imag stack approach make use probabilist segment map psm left ventricular lv caviti gener decis forest psm gener slice sa stack rigidli regist plane target psm long axi la imag avail psm gener combin creat target psm target psm produc stack use model train motion free stack propos approach test dataset sa stack acquir healthi subject anatom cardiac imag also avail refer compar two techniqu use la intens imag la segment target respect result show accuraci robust propos approach motion compens less
519,1810.02196,"
        Many optimization problems admit a number of local optima, among which there is the global optimum. For these problems, various heuristic optimization methods have been proposed. Comparing the results of these solvers requires the definition of suitable metrics. In the electrical energy systems literature, simple metrics such as best value obtained, the mean value, the median or the standard deviation of the solutions are still used. However, the comparisons carried out with these metrics are rather weak, and on these bases a somehow uncontrolled proliferation of heuristic solvers is taking place. This paper addresses the overall issue of understanding the reasons of this proliferation, showing that the assessment of the best solver can be cast into a perpetual motion scheme. Moreover, this paper shows how the use of more refined metrics defined to compare the optimization result, associated with the definition of appropriate benchmarks, may make the comparisons among the solvers more robust. The proposed metrics are based on the concept of first-order stochastic dominance and are defined for the cases in which: : (i) the globally optimal solution can be found (for testing purposes); and (ii) the number of possible solutions is so large that practically it cannot be guaranteed that the global optimum has been found. Illustrative examples are provided for a typical problem in the electrical energy systems area - distribution network reconfiguration. The conceptual results obtained are generally valid to compare the results of other optimization problems.
        △ Less
",mani optim problem admit number local optima among global optimum problem variou heurist optim method propos compar result solver requir definit suitabl metric electr energi system literatur simpl metric best valu obtain mean valu median standard deviat solut still use howev comparison carri metric rather weak base somehow uncontrol prolifer heurist solver take place paper address overal issu understand reason prolifer show assess best solver cast perpetu motion scheme moreov paper show use refin metric defin compar optim result associ definit appropri benchmark may make comparison among solver robust propos metric base concept first order stochast domin defin case global optim solut found test purpos ii number possibl solut larg practic cannot guarante global optimum found illustr exampl provid typic problem electr energi system area distribut network reconfigur conceptu result obtain gener valid compar result optim problem less
520,1810.02194,"
        In this short note we study a class of multi-player, turn-based games with deterministic state transitions and reachability / safety objectives (this class contains as special cases ""classic"" two-player reachability and safety games as well as multi-player ""stay--in-a-set"" and ""reach-a-set"" games). Quantitative and qualitative versions of the objectives are presented and for both cases we prove the existence of a deterministic and memoryless Nash equilibrium; the proof is short and simple, using only Fink's classic result about the existence of Nash equilibria for multi-player discounted stochastic games.
        △ Less
",short note studi class multi player turn base game determinist state transit reachabl safeti object class contain special case classic two player reachabl safeti game well multi player stay set reach set game quantit qualit version object present case prove exist determinist memoryless nash equilibrium proof short simpl use fink classic result exist nash equilibria multi player discount stochast game less
521,1810.02186,"
        This paper introduces a new family of consensus protocols, namely \emph{Lachesis-class} denoted by $\mathcal{L}$, for distributed networks with guaranteed Byzantine fault tolerance. Each Lachesis protocol $L$ in $\mathcal{L}$ has complete asynchrony, is leaderless, has no round robin, no proof-of-work, and has eventual consensus.
  The core concept of our technology is the \emph{OPERA chain}, generated by the Lachesis protocol. In the most general form, each node in Lachesis has a set of $k$ neighbours of most preference. When receiving transactions a node creates and shares an event block with all neighbours. Each event block is signed by the hashes of the creating node and its $k$ peers. The OPERA chain of the event blocks is a Directed Acyclic Graph (DAG); it guarantees practical Byzantine fault tolerance (pBFT). Our framework is then presented using Lamport timestamps and concurrent common knowledge.
  Further, we present an example of Lachesis consensus protocol $L_0$ of our framework. Our $L_0$ protocol can reach consensus upon 2/3 of all participants' agreement to an event block without any additional communication overhead. $L_0$ protocol relies on a cost function to identify $k$ peers and to generate the DAG-based OPERA chain. By creating a binary flag table that stores connection information and share information between blocks, Lachesis achieves consensus in fewer steps than pBFT protocol for consensus.
        △ Less
",paper introduc new famili consensu protocol name emph lachesi class denot mathcal l distribut network guarante byzantin fault toler lachesi protocol l mathcal l complet asynchroni leaderless round robin proof work eventu consensu core concept technolog emph opera chain gener lachesi protocol gener form node lachesi set k neighbour prefer receiv transact node creat share event block neighbour event block sign hash creat node k peer opera chain event block direct acycl graph dag guarante practic byzantin fault toler pbft framework present use lamport timestamp concurr common knowledg present exampl lachesi consensu protocol l framework l protocol reach consensu upon particip agreement event block without addit commun overhead l protocol reli cost function identifi k peer gener dag base opera chain creat binari flag tabl store connect inform share inform block lachesi achiev consensu fewer step pbft protocol consensu less
522,1810.02182,"
        We define the notion of a $k$-maximal submonoid. A submonoid $M$ is $k$-maximal if there does not exist another submonoid generated by at most $k$ words containing $M$. We prove that the intersection of two $2$-maximal submonoids is either the empty word or a submonoid generated by one primitive word. As a consequence, for every submonoid $M$ generated by two words that do not commute, there exists a unique $2$-maximal submonoid containing $M$. We aim to show that this algebraic framework can be used to introduce a novel approach in combinatorics on words. We call primitive pairs those pairs of nonempty words that generate a $2$-maximal submonoid. Primitive pairs therefore represent an algebraic generalization of the classical notion of a primitive word. As an immediate consequence of our results, we have that for every pair of nonempty words $\{x,y\}$ such that $xy\neq yx$ there exists a unique primitive pair $\{u,v\}$ such that $x$ and $y$ can be written as concatenations of copies of $u$ and $v$. We call the pair $\{u,v\}$ the binary root of the pair $\{x,y\}$, in analogy with the classical notion of root of a single word. For a single word $w$, we say that $\{x,y\}$ is a binary root of $w$ if $w$ can be written as a concatenation of copies of $x$ and $y$ and $\{x,y\}$ is a primitive pair. We prove that every word $w$ has at most one binary root $\{x,y\}$ such that $|x|+|y|<\sqrt{|w|}$. That is, the binary root of a word is unique provided the length of the word is sufficiently large with respect to the size of the root. Our results can also be compared to previous approaches that investigate pseudo-repetitions. Finally, we discuss the case of infinite words, where the notion of a binary root represents a new refinement in the classical dichotomy periodic/aperiodic.
        △ Less
",defin notion k maxim submonoid submonoid k maxim exist anoth submonoid gener k word contain prove intersect two maxim submonoid either empti word submonoid gener one primit word consequ everi submonoid gener two word commut exist uniqu maxim submonoid contain aim show algebra framework use introduc novel approach combinator word call primit pair pair nonempti word gener maxim submonoid primit pair therefor repres algebra gener classic notion primit word immedi consequ result everi pair nonempti word x xy neq yx exist uniqu primit pair u v x written concaten copi u v call pair u v binari root pair x analog classic notion root singl word singl word w say x binari root w w written concaten copi x x primit pair prove everi word w one binari root x x sqrt w binari root word uniqu provid length word suffici larg respect size root result also compar previou approach investig pseudo repetit final discuss case infinit word notion binari root repres new refin classic dichotomi period aperiod less
523,1810.02180,"
        We consider a model of robust learning in an adversarial environment. The learner gets uncorrupted training data with access to possible corruptions that may be used by the adversary during testing. Their aim is to build a robust classifier that would be tested on future adversarially corrupted data. We use a zero-sum game between the learner and the adversary as our game theoretic framework. The adversary is limited to $k$ possible corruptions for each input. Our model is closely related to the adversarial examples model of one of Schmidt et al. (2018) and Madry et al. (2017).
  We refer to binary and multi-class classification settings, and regression setting. Our main results are generalization bounds for all settings. For the binary classification setting, we improve a generalization bound previously found in Feige et al. (2015). We generalize to the case of weighted average of hypotheses from $H$ that is not limited to be finite. The sample complexity has been improved from $Ø(\frac{1}{ε^4}\log(\frac{|H|}δ))$ to $Ø(\frac{1}{ε^2}(k\log(k)VC(H)+\log\frac{1}δ))$. The core of all is proofs based on bounds of the empirical Rademacher complexity.
  For the binary classification, we use a known regret minimization algorithm of Feige et al. that uses an ERM oracle as a blackbox and we expand on the multi-class and regression settings. The algorithm provides us near optimal policies for the players on a given training sample. The learner starts with a fixed hypothesis class $H$ and chooses a convex combination of hypotheses from $H$. The learner's loss is measured on adversarial corrupted inputs.
  Along the way, we obtain results on fat-shattering dimension and Rademacher complexity of $k$-fold maxima over function classes; these may be of independent interest.
        △ Less
",consid model robust learn adversari environ learner get uncorrupt train data access possibl corrupt may use adversari test aim build robust classifi would test futur adversari corrupt data use zero sum game learner adversari game theoret framework adversari limit k possibl corrupt input model close relat adversari exampl model one schmidt et al madri et al refer binari multi class classif set regress set main result gener bound set binari classif set improv gener bound previous found feig et al gener case weight averag hypothes h limit finit sampl complex improv frac log frac h frac k log k vc h log frac core proof base bound empir rademach complex binari classif use known regret minim algorithm feig et al use erm oracl blackbox expand multi class regress set algorithm provid us near optim polici player given train sampl learner start fix hypothesi class h choos convex combin hypothes h learner loss measur adversari corrupt input along way obtain result fat shatter dimens rademach complex k fold maxima function class may independ interest less
524,1810.02176,"
        Maximising the detection of intrusions is a fundamental and often critical aim of perimeter surveillance. Commonly, this requires a decision-maker to optimally allocate multiple searchers to segments of the perimeter. We consider a scenario where the decision-maker may sequentially update the searchers' allocation, learning from the observed data to improve decisions over time. In this work we propose a formal model and solution methods for this sequential perimeter surveillance problem. Our model is a combinatorial multi-armed bandit (CMAB) with Poisson rewards and a novel filtered feedback mechanism - arising from the failure to detect certain intrusions. Our solution method is an upper confidence bound approach and we derive upper and lower bounds on its expected performance. We prove that the gap between these bounds is of constant order, and demonstrate empirically that our approach is more reliable in simulated problems than competing algorithms.
        △ Less
",maximis detect intrus fundament often critic aim perimet surveil commonli requir decis maker optim alloc multipl searcher segment perimet consid scenario decis maker may sequenti updat searcher alloc learn observ data improv decis time work propos formal model solut method sequenti perimet surveil problem model combinatori multi arm bandit cmab poisson reward novel filter feedback mechan aris failur detect certain intrus solut method upper confid bound approach deriv upper lower bound expect perform prove gap bound constant order demonstr empir approach reliabl simul problem compet algorithm less
525,1810.02174,"
        Since the creation of Bitcoin in 2009 we have seen a great push towards public and private blockchains. In order to avoid fragmentation, a global network connecting all these blockchains is envisioned. Just like the Internet facilitates communication and the transfer of information, we propose a system, similar in size and reach for payments and transactions: A cryptographically-secure off-chain multi-asset instant transaction network (COMIT) can connect and exchange any asset on any blockchain to any other blockchain using a cross-chain routing protocol (CRP). COMIT is a super blockchain network that allows for instant transactions which are enforced using off-chain smart contracts. It leverages Payment Channels and Hashed Timelock Contracts (HTLC) across chains to solve the problem of double spending attacks without requiring a settlement onto the underlying blockchains. COMIT's connectivity is provided by Liquidity Providers (LP), who operate on one or more blockchains, acting as payment hubs and nodes on a single chain and market makers in a decentralized network for cross-chain asset conversions. This paper lays out how COMIT works, the benefits for Users, Liquidity Providers and Businesses; and how this does not only accelerate the adoption of blockchain technology, but furthermore allows for an integration with the traditional banking system.
        △ Less
",sinc creation bitcoin seen great push toward public privat blockchain order avoid fragment global network connect blockchain envis like internet facilit commun transfer inform propos system similar size reach payment transact cryptograph secur chain multi asset instant transact network comit connect exchang asset blockchain blockchain use cross chain rout protocol crp comit super blockchain network allow instant transact enforc use chain smart contract leverag payment channel hash timelock contract htlc across chain solv problem doubl spend attack without requir settlement onto underli blockchain comit connect provid liquid provid lp oper one blockchain act payment hub node singl chain market maker decentr network cross chain asset convers paper lay comit work benefit user liquid provid busi acceler adopt blockchain technolog furthermor allow integr tradit bank system less
526,1810.02171,"
        In this paper, we propose a comprehensive research over triple hop all-optical relaying free-space optical (FSO) systems in the presence of all main noise sources including background, thermal and amplified spontaneous emission (ASE) noise and by considering the effect of the optical degree-of- freedom (DoF). Using full CSI relaying, we derive the exact expressions for the noise variance at the destination. Then, in order to simplify the analytical expressions of full CSI relaying, we also propose and investigate the validity of different approximations over noise variance at the destination. Finally, we evaluate the the performance of considered triple-hop all-optical relaying FSO system in term of ergodic capacity.
        △ Less
",paper propos comprehens research tripl hop optic relay free space optic fso system presenc main nois sourc includ background thermal amplifi spontan emiss ase nois consid effect optic degre freedom dof use full csi relay deriv exact express nois varianc destin order simplifi analyt express full csi relay also propos investig valid differ approxim nois varianc destin final evalu perform consid tripl hop optic relay fso system term ergod capac less
527,1810.02167,"
        Due to the relatively high cost of deploying optical fibers, free space optical (FSO) links have been developed as a cost-effective alternative technology for next generation cellular networks. However, in order to have a practical role in the physical layer of future communication systems, data rate of FSO links must be improved. To this aim, in this paper we employ a multiple-input multiple-output (MIMO) multiplexing scheme with two transceivers to increase the data rate of the considered FSO system. Unlike MIMO diversity case, the performance of MIMO multiplexing is significantly affected by interference be- tween parallel channels. To solve this problem, we propose a novel space-time scheme which significantly reduces the interference between parallel channels. We analyze the performance of the proposed scheme in terms of BER and outage probability.
        △ Less
",due rel high cost deploy optic fiber free space optic fso link develop cost effect altern technolog next gener cellular network howev order practic role physic layer futur commun system data rate fso link must improv aim paper employ multipl input multipl output mimo multiplex scheme two transceiv increas data rate consid fso system unlik mimo divers case perform mimo multiplex significantli affect interfer tween parallel channel solv problem propos novel space time scheme significantli reduc interfer parallel channel analyz perform propos scheme term ber outag probabl less
528,1810.02163,"
        Recently, Branco da Silva and Silva described an efficient encoding and decoding algorithm for Construction D$^\prime$ lattices. Using their algorithm, we propose a Construction D$^\prime$ lattice based on binary quasi-cyclic low-density parity-check (QC-LPDC) codes and single parity-check product codes. The underlying codes designed by the balanced-distances rule contribute in a balanced manner to the squared minimum distance of the constructed lattice, which results in a high lattice coding gain. The proposed lattice based on IEEE 802.16e QC-LDPC codes is shown to provide competitive error-rate performance on the power-unconstrained additive white Gaussian noise channel.
        △ Less
",recent branco da silva silva describ effici encod decod algorithm construct prime lattic use algorithm propos construct prime lattic base binari quasi cyclic low densiti pariti check qc lpdc code singl pariti check product code underli code design balanc distanc rule contribut balanc manner squar minimum distanc construct lattic result high lattic code gain propos lattic base ieee e qc ldpc code shown provid competit error rate perform power unconstrain addit white gaussian nois channel less
529,1810.02156,"
        Negation scope has been annotated in several English and Chinese corpora, and highly accurate models for this task in these languages have been learned from these annotations. Unfortunately, annotations are not available in other languages. Could a model that detects negation scope be applied to a language that it hasn't been trained on? We develop neural models that learn from cross-lingual word embeddings or universal dependencies in English, and test them on Chinese, showing that they work surprisingly well. We find that modelling syntax is helpful even in monolingual settings and that cross-lingual word embeddings help relatively little, and we analyse cases that are still difficult for this task.
        △ Less
",negat scope annot sever english chines corpora highli accur model task languag learn annot unfortun annot avail languag could model detect negat scope appli languag train develop neural model learn cross lingual word embed univers depend english test chines show work surprisingli well find model syntax help even monolingu set cross lingual word embed help rel littl analys case still difficult task less
530,1810.02142,"
        Short-circuit evaluation denotes the semantics of propositional connectives in which the second argument is evaluated only if the first argument does not suffice to determine the value of the expression. Short-circuit evaluation is widely used in programming, with sequential conjunction and disjunction as primitive connectives.
  We study the question which logical laws axiomatize short-circuit evaluation under the following assumptions: compound statements are evaluated from left to right, each atom (propositional variable) evaluates to either true or false, and atomic evaluations can cause a side effect. The answer to this question depends on the kind of atomic side effects that can occur and leads to different ""short-circuit logics"". The basic case is FSCL (free short-circuit logic), which characterizes the setting in which each atomic evaluation can cause a side effect. We recall some main results and then relate FSCL to MSCL (memorizing short-circuit logic), where in the evaluation of a compound statement, the first evaluation result of each atom is memorized. MSCL can be seen as a sequential variant of propositional logic: atomic evaluations cannot cause a side effect and the sequential connectives are not commutative. Then we relate MSCL to SSCL (static short-circuit logic), the variant of propositional logic that prescribes short-circuit evaluation with commutative sequential connectives.
  We present evaluation trees as an intuitive semantics for short-circuit evaluation, and simple equational axiomatizations for the short-circuit logics mentioned that use negation and the sequential connectives only.
        △ Less
",short circuit evalu denot semant proposit connect second argument evalu first argument suffic determin valu express short circuit evalu wide use program sequenti conjunct disjunct primit connect studi question logic law axiomat short circuit evalu follow assumpt compound statement evalu left right atom proposit variabl evalu either true fals atom evalu caus side effect answer question depend kind atom side effect occur lead differ short circuit logic basic case fscl free short circuit logic character set atom evalu caus side effect recal main result relat fscl mscl memor short circuit logic evalu compound statement first evalu result atom memor mscl seen sequenti variant proposit logic atom evalu cannot caus side effect sequenti connect commut relat mscl sscl static short circuit logic variant proposit logic prescrib short circuit evalu commut sequenti connect present evalu tree intuit semant short circuit evalu simpl equat axiomat short circuit logic mention use negat sequenti connect less
531,1810.02137,"
        OT (Operational Transformation) was invented for supporting real-time co-editors in the late 1980s and has evolved to become a core technique used in today's working co-editors and adopted in major industrial products. CRDT (Commutative Replicated Data Type) for co-editors was first proposed around 2006, under the name of WOOT (WithOut Operational Transformation). Follow-up CRDT variations are commonly labeled as ""post-OT"" techniques capable of making concurrent operations natively commutative, and have made broad claims of superiority over OT solutions, in terms of correctness, time and space complexity, simplicity, etc. Over one decade later, however, CRDT solutions are rarely found in working co-editors, while OT solutions remain the choice for building the vast majority of co-editors. Contradictions between this reality and CRDT's purported advantages have been the source of much debate and confusion in co-editing research and developer communities. What is CRDT really to co-editing? What are the real differences between OT and CRDT for co-editors? What are the key factors that may have affected the adoption of and choice between OT and CRDT for co-editors in the real world? In this paper, we report our discoveries, in relation to these questions and beyond, from a comprehensive review and comparison study on representative OT and CRDT solutions and working co-editors based on them. Moreover, this work reveals facts and presents evidences that refute CRDT claimed advantages over OT. We hope the results reported in this paper will help clear up common myths, misconceptions, and confusions surrounding alternative co-editing techniques, and accelerate progress in co-editing technology for real world applications.
        △ Less
",ot oper transform invent support real time co editor late evolv becom core techniqu use today work co editor adopt major industri product crdt commut replic data type co editor first propos around name woot without oper transform follow crdt variat commonli label post ot techniqu capabl make concurr oper nativ commut made broad claim superior ot solut term correct time space complex simplic etc one decad later howev crdt solut rare found work co editor ot solut remain choic build vast major co editor contradict realiti crdt purport advantag sourc much debat confus co edit research develop commun crdt realli co edit real differ ot crdt co editor key factor may affect adopt choic ot crdt co editor real world paper report discoveri relat question beyond comprehens review comparison studi repres ot crdt solut work co editor base moreov work reveal fact present evid refut crdt claim advantag ot hope result report paper help clear common myth misconcept confus surround altern co edit techniqu acceler progress co edit technolog real world applic less
532,1810.02132,"
        Existential rules, long known as tuple-generating dependencies in database theory, have been intensively studied in the last decade as a powerful formalism to represent ontological knowledge in the context of ontology-based query answering. A knowledge base is then composed of an instance that contains incomplete data and a set of existential rules, and answers to queries are logically entailed from the knowledge base. This brought again to light the fundamental chase tool, and its different variants that have been proposed in the literature. It is well-known that the problem of determining, given a chase variant and a set of existential rules, whether the chase will halt on any instance, is undecidable. Hence, a crucial issue is whether it becomes decidable for known subclasses of existential rules. In this work, we consider linear existential rules, a simple yet important subclass of existential rules that generalizes inclusion dependencies. We show the decidability of the all instance chase termination problem on linear rules for three main chase variants, namely semi-oblivious, restricted and core chase. To obtain these results, we introduce a novel approach based on so-called derivation trees and a single notion of forbidden pattern. Besides the theoretical interest of a unified approach and new proofs, we provide the first positive decidability results concerning the termination of the restricted chase, proving that chase termination on linear existential rules is decidable for both versions of the problem: Does every fair chase sequence terminate? Does some fair chase sequence terminate?
        △ Less
",existenti rule long known tupl gener depend databas theori intens studi last decad power formal repres ontolog knowledg context ontolog base queri answer knowledg base compos instanc contain incomplet data set existenti rule answer queri logic entail knowledg base brought light fundament chase tool differ variant propos literatur well known problem determin given chase variant set existenti rule whether chase halt instanc undecid henc crucial issu whether becom decid known subclass existenti rule work consid linear existenti rule simpl yet import subclass existenti rule gener inclus depend show decid instanc chase termin problem linear rule three main chase variant name semi oblivi restrict core chase obtain result introduc novel approach base call deriv tree singl notion forbidden pattern besid theoret interest unifi approach new proof provid first posit decid result concern termin restrict chase prove chase termin linear existenti rule decid version problem everi fair chase sequenc termin fair chase sequenc termin less
533,1810.02129,"
        In this work, we have studied the collaboration and citation network between Indian Institutes from publications in American Physical Society(APS) journals between 1970-2013. We investigate the role of geographic proximity on the network structure and find that it is the characteristics of the Institution, rather than the geographic distance, that plays a dominant role in collaboration networks. We find that Institutions with better federal funding dominate the network topology and play a crucial role in overall research output. We find that the citation flow across different categories of institutions is strongly linked to the collaborations between them. We have estimated the knowledge flow in and out of Institutions and identified the top knowledge source and sinks.
        △ Less
",work studi collabor citat network indian institut public american physic societi ap journal investig role geograph proxim network structur find characterist institut rather geograph distanc play domin role collabor network find institut better feder fund domin network topolog play crucial role overal research output find citat flow across differ categori institut strongli link collabor estim knowledg flow institut identifi top knowledg sourc sink less
534,1810.02126,"
        Many real-world visual recognition use-cases can not directly benefit from state-of-the-art CNN-based approaches because of the lack of many annotated data. The usual approach to deal with this is to transfer a representation pre-learned on a large annotated source-task onto a target-task of interest. This raises the question of how well the original representation is ""universal"", that is to say directly adapted to many different target-tasks. To improve such universality, the state-of-the-art consists in training networks on a diversified source problem, that is modified either by adding generic or specific categories to the initial set of categories. In this vein, we proposed a method that exploits finer-classes than the most specific ones existing, for which no annotation is available. We rely on unsupervised learning and a bottom-up split and merge strategy. We show that our method learns more universal representations than state-of-the-art, leading to significantly better results on 10 target-tasks from multiple domains, using several network architectures, either alone or combined with networks learned at a coarser semantic level.
        △ Less
",mani real world visual recognit use case directli benefit state art cnn base approach lack mani annot data usual approach deal transfer represent pre learn larg annot sourc task onto target task interest rais question well origin represent univers say directli adapt mani differ target task improv univers state art consist train network diversifi sourc problem modifi either ad gener specif categori initi set categori vein propos method exploit finer class specif one exist annot avail reli unsupervis learn bottom split merg strategi show method learn univers represent state art lead significantli better result target task multipl domain use sever network architectur either alon combin network learn coarser semant level less
535,1810.02114,"
        Structural information is important in natural language understanding. Although some current neural net-based models have a limited ability to take local syntactic information, they fail to use high-level and large-scale structures of documents. This information is valuable for text understanding since it contains the author's strategy to express information, in building an effective representation and forming appropriate output modes. We propose a neural net-based model, Zooming Network, capable of representing and leveraging text structure of long document and developing its own analyzing rhythm to extract critical information. Generally, ZN consists of an encoding neural net that can build a hierarchical representation of a document, and an interpreting neural model that can read the information at multi-levels and issuing labeling actions through a policy-net. Our model is trained with a hybrid paradigm of supervised learning (distinguishing right and wrong decision) and reinforcement learning (determining the goodness among multiple right paths). We applied the proposed model to long text sequence labeling tasks, with performance exceeding baseline model (biLSTM-crf) by 10 F1-measure.
        △ Less
",structur inform import natur languag understand although current neural net base model limit abil take local syntact inform fail use high level larg scale structur document inform valuabl text understand sinc contain author strategi express inform build effect represent form appropri output mode propos neural net base model zoom network capabl repres leverag text structur long document develop analyz rhythm extract critic inform gener zn consist encod neural net build hierarch represent document interpret neural model read inform multi level issu label action polici net model train hybrid paradigm supervis learn distinguish right wrong decis reinforc learn determin good among multipl right path appli propos model long text sequenc label task perform exceed baselin model bilstm crf f measur less
536,1810.02113,"
        Accurate segmentation of anatomical structures in chest radiographs is essential for many computer-aided diagnosis tasks. In this paper we investigate the latest fully-convolutional architectures for the task of multi-class segmentation of the lungs field, heart and clavicles in a chest radiograph. In addition, we explore the influence of using different loss functions in the training process of a neural network for semantic segmentation. We evaluate all models on a common benchmark of 247 X-ray images from the JSRT database and ground-truth segmentation masks from the SCR dataset. Our best performing architecture, is a modified U-Net that benefits from pre-trained encoder weights. This model outperformed the current state-of-the-art methods tested on the same benchmark, with Jaccard overlap scores of 96.1% for lung fields, 90.6% for heart and 85.5% for clavicles.
        △ Less
",accur segment anatom structur chest radiograph essenti mani comput aid diagnosi task paper investig latest fulli convolut architectur task multi class segment lung field heart clavicl chest radiograph addit explor influenc use differ loss function train process neural network semant segment evalu model common benchmark x ray imag jsrt databas ground truth segment mask scr dataset best perform architectur modifi u net benefit pre train encod weight model outperform current state art method test benchmark jaccard overlap score lung field heart clavicl less
537,1810.02112,"
        Estimating the dependency of variables is a fundamental task in data analysis. Identifying the relevant attributes in databases leads to better data understanding and also improves the performance of learning algorithms, both in terms of runtime and quality. In data streams, dependency monitoring provides key insights into the underlying process, but is challenging. In this paper, we propose Monte Carlo Dependency Estimation (MCDE), a theoretical framework to estimate multivariate dependency in static and dynamic data. MCDE quantifies dependency as the average discrepancy between marginal and conditional distributions via Monte Carlo simulations. Based on this framework, we present Mann-Whitney P (MWP), a novel dependency estimator. We show that MWP satisfies a number of desirable properties and can accommodate any kind of numerical data. We demonstrate the superiority of our estimator by comparing it to the state-of-the-art multivariate dependency measures.
        △ Less
",estim depend variabl fundament task data analysi identifi relev attribut databas lead better data understand also improv perform learn algorithm term runtim qualiti data stream depend monitor provid key insight underli process challeng paper propos mont carlo depend estim mcde theoret framework estim multivari depend static dynam data mcde quantifi depend averag discrep margin condit distribut via mont carlo simul base framework present mann whitney p mwp novel depend estim show mwp satisfi number desir properti accommod kind numer data demonstr superior estim compar state art multivari depend measur less
538,1810.02106,"
        Consider the following problem: given a graph and two maximal independent sets (MIS), is there a valid sequence of independent sets starting from the first one and ending in the second, in which a single node is inserted to or removed from the set at each step? While this would be trivial without any restrictions by simply removing all the nodes and then inserting the required ones, this problem, called the MIS reconfiguration problem, has been studied in the centralized setting with the caveat that intermediate sets in the sequence (schedule) must be at least of a certain size. In this paper, we investigate a distributed MIS reconfiguration problem, in which nodes can be inserted or removed from the sets concurrently. Each node of the graph is aware of its membership in the initial and final independent sets, and the nodes communicate with their neighbors in order to produce a reconfiguration schedule. The schedule is restricted by forbidding two neighbors to change their membership status at the same step. Here, we do not impose a lower bound on the size of the intermediate independent sets, as this would be hard to coordinate in a non-centralized fashion. However, we do want the independent sets to be non-trivial. We show that obtaining an actual MIS (and even a 3-dominating set) in each intermediate step is impossible. However, we provide efficient solutions when the intermediate sets are only required to be independent and 4-dominating. We prove that a constant length schedule can be found in $O(MIS+R32)$ rounds, where $MIS$ is the complexity of finding an MIS on a worst-case graph and $R32$ is the complexity of finding a $(3,2)$-ruling set. For bounded degree graphs, this is $O(\log^*n)$ rounds and we show that it is necessary. On the other extreme, we show that with a constant number of rounds we can find a linear length schedule.
        △ Less
",consid follow problem given graph two maxim independ set mi valid sequenc independ set start first one end second singl node insert remov set step would trivial without restrict simpli remov node insert requir one problem call mi reconfigur problem studi central set caveat intermedi set sequenc schedul must least certain size paper investig distribut mi reconfigur problem node insert remov set concurr node graph awar membership initi final independ set node commun neighbor order produc reconfigur schedul schedul restrict forbid two neighbor chang membership statu step impos lower bound size intermedi independ set would hard coordin non central fashion howev want independ set non trivial show obtain actual mi even domin set intermedi step imposs howev provid effici solut intermedi set requir independ domin prove constant length schedul found mi r round mi complex find mi worst case graph r complex find rule set bound degre graph log n round show necessari extrem show constant number round find linear length schedul less
539,1810.02100,"
        Dependency parsing is one of the important natural language processing tasks that assigns syntactic trees to texts. Due to the wider availability of dependency corpora and improved parsing and machine learning techniques, parsing accuracies of supervised learning-based systems have been significantly improved. However, due to the nature of supervised learning, those parsing systems highly rely on the manually annotated training corpora. They work reasonably good on the in-domain data but the performance drops significantly when tested on out-of-domain texts. To bridge the performance gap between in-domain and out-of-domain, this thesis investigates three semi-supervised techniques for out-of-domain dependency parsing, namely co-training, self-training and dependency language models. Our approaches use easily obtainable unlabelled data to improve out-of-domain parsing accuracies without the need of expensive corpora annotation. The evaluations on several English domains and multi-lingual data show quite good improvements on parsing accuracy. Overall this work conducted a survey of semi-supervised methods for out-of-domain dependency parsing, where I extended and compared a number of important semi-supervised methods in a unified framework. The comparison between those techniques shows that self-training works equally well as co-training on out-of-domain parsing, while dependency language models can improve both in- and out-of-domain accuracies.
        △ Less
",depend pars one import natur languag process task assign syntact tree text due wider avail depend corpora improv pars machin learn techniqu pars accuraci supervis learn base system significantli improv howev due natur supervis learn pars system highli reli manual annot train corpora work reason good domain data perform drop significantli test domain text bridg perform gap domain domain thesi investig three semi supervis techniqu domain depend pars name co train self train depend languag model approach use easili obtain unlabel data improv domain pars accuraci without need expens corpora annot evalu sever english domain multi lingual data show quit good improv pars accuraci overal work conduct survey semi supervis method domain depend pars extend compar number import semi supervis method unifi framework comparison techniqu show self train work equal well co train domain pars depend languag model improv domain accuraci less
540,1810.02099,"
        In this paper we introduce a new family of string processing problems. We are given two or more strings and we are asked to compute a factor common to all strings that preserves a specific property and has maximal length. Here we consider three fundamental string properties: square-free factors, periodic factors, and palindromic factors under three different settings, one per property. In the first setting, we are given a string $x$ and we are asked to construct a data structure over $x$ answering the following type of on-line queries: given string $y$, find a longest square-free factor common to $x$ and $y$. In the second setting, we are given $k$ strings and an integer $1 < k'\leq k$ and we are asked to find a longest periodic factor common to at least $k'$ strings. In the third setting, we are given two strings and we are asked to find a longest palindromic factor common to the two strings. We present linear-time solutions for all settings. We anticipate that our paradigm can be extended to other string properties or settings.
        △ Less
",paper introduc new famili string process problem given two string ask comput factor common string preserv specif properti maxim length consid three fundament string properti squar free factor period factor palindrom factor three differ set one per properti first set given string x ask construct data structur x answer follow type line queri given string find longest squar free factor common x second set given k string integ k leq k ask find longest period factor common least k string third set given two string ask find longest palindrom factor common two string present linear time solut set anticip paradigm extend string properti set less
541,1810.02091,"
        This report describes the results of the eSCF Awareness Workshop on Handling Uncertainty for Data Quality Management - Challenges from Transport and Supply Chain Management that was held on June 5, 2018 in Heeze, The Netherlands. The goal of this workshop was to create and enhance awareness into data quality management issues that are encountered in practice, for business organizations that aim to integrate a data-analytical mind set into their operations.
        △ Less
",report describ result escf awar workshop handl uncertainti data qualiti manag challeng transport suppli chain manag held june heez netherland goal workshop creat enhanc awar data qualiti manag issu encount practic busi organ aim integr data analyt mind set oper less
542,1810.02090,"
        Cybercriminals use Return Oriented Programming techniques to attack systems and IoT devices. While defenses have been developed, not all of them are applicable to constrained devices. We present Shakedown, which is a compile-time randomizing build tool which creates several versions of the binary, each with a distinct memory layout. An attack developed against one device will not work on another device which has a different memory layout. We tested Shakedown on an industrial IoT device and shown that its normal functionality remained intact while an exploit was blocked.
        △ Less
",cybercrimin use return orient program techniqu attack system iot devic defens develop applic constrain devic present shakedown compil time random build tool creat sever version binari distinct memori layout attack develop one devic work anoth devic differ memori layout test shakedown industri iot devic shown normal function remain intact exploit block less
543,1810.02080,"
        Graphs are general and powerful data representations which can model complex real-world phenomena, ranging from chemical compounds to social networks; however, effective feature extraction from graphs is not a trivial task, and much work has been done in the field of machine learning and data mining. The recent advances in graph neural networks have made automatic and flexible feature extraction from graphs possible and have improved the predictive performance significantly. In this paper, we go further with this line of research and address a more general problem of learning with a graph of graphs (GoG) consisting of an external graph and internal graphs, where each node in the external graph has an internal graph structure. We propose a dual convolutional neural network that extracts node representations by combining the external and internal graph structures in an end-to-end manner. Experiments on link prediction tasks using several chemical network datasets demonstrate the effectiveness of the proposed method.
        △ Less
",graph gener power data represent model complex real world phenomena rang chemic compound social network howev effect featur extract graph trivial task much work done field machin learn data mine recent advanc graph neural network made automat flexibl featur extract graph possibl improv predict perform significantli paper go line research address gener problem learn graph graph gog consist extern graph intern graph node extern graph intern graph structur propos dual convolut neural network extract node represent combin extern intern graph structur end end manner experi link predict task use sever chemic network dataset demonstr effect propos method less
544,1810.02076,"
        Inertial information processing plays a pivotal role in ego-motion awareness for mobile agents, as inertial measurements are entirely egocentric and not environment dependent. However, they are affected greatly by changes in sensor placement/orientation or motion dynamics, and it is infeasible to collect labelled data from every domain. To overcome the challenges of domain adaptation on long sensory sequences, we propose a novel framework that extracts domain-invariant features of raw sequences from arbitrary domains, and transforms to new domains without any paired data. Through the experiments, we demonstrate that it is able to efficiently and effectively convert the raw sequence from a new unlabelled target domain into an accurate inertial trajectory, benefiting from the physical motion knowledge transferred from the labelled source domain. We also conduct real-world experiments to show our framework can reconstruct physically meaningful trajectories from raw IMU measurements obtained with a standard mobile phone in various attachments.
        △ Less
",inerti inform process play pivot role ego motion awar mobil agent inerti measur entir egocentr environ depend howev affect greatli chang sensor placement orient motion dynam infeas collect label data everi domain overcom challeng domain adapt long sensori sequenc propos novel framework extract domain invari featur raw sequenc arbitrari domain transform new domain without pair data experi demonstr abl effici effect convert raw sequenc new unlabel target domain accur inerti trajectori benefit physic motion knowledg transfer label sourc domain also conduct real world experi show framework reconstruct physic meaning trajectori raw imu measur obtain standard mobil phone variou attach less
545,1810.02074,"
        Deep learning based object detectors require thousands of diversified bounding box and class annotated examples. Though image object detectors have shown rapid progress in recent years with the release of multiple large-scale static image datasets, object detection on videos still remains an open problem due to scarcity of annotated video frames. Having a robust video object detector is an essential component for video understanding and curating large-scale automated annotations in videos. Domain difference between images and videos makes the transferability of image object detectors to videos sub-optimal. The most common solution is to use weakly supervised annotations where a video frame has to be tagged for presence/absence of object categories. This still takes up manual effort. In this paper we take a step forward by adapting the concept of unsupervised adversarial image-to-image translation to perturb static high quality images to be visually indistinguishable from a set of video frames. We assume the presence of a fully annotated static image dataset and an unannotated video dataset. Object detector is trained on adversarially transformed image dataset using the annotations of the original dataset. Experiments on Youtube-Objects and Youtube-Objects-Subset datasets with two contemporary baseline object detectors reveal that such unsupervised pixel level domain adaptation boosts the generalization performance on video frames compared to direct application of original image object detector. Also, we achieve competitive performance compared to recent baselines of weakly supervised methods. This paper can be seen as an application of image translation for cross domain object detection.
        △ Less
",deep learn base object detector requir thousand diversifi bound box class annot exampl though imag object detector shown rapid progress recent year releas multipl larg scale static imag dataset object detect video still remain open problem due scarciti annot video frame robust video object detector essenti compon video understand curat larg scale autom annot video domain differ imag video make transfer imag object detector video sub optim common solut use weakli supervis annot video frame tag presenc absenc object categori still take manual effort paper take step forward adapt concept unsupervis adversari imag imag translat perturb static high qualiti imag visual indistinguish set video frame assum presenc fulli annot static imag dataset unannot video dataset object detector train adversari transform imag dataset use annot origin dataset experi youtub object youtub object subset dataset two contemporari baselin object detector reveal unsupervis pixel level domain adapt boost gener perform video frame compar direct applic origin imag object detector also achiev competit perform compar recent baselin weakli supervis method paper seen applic imag translat cross domain object detect less
546,1810.02069,"
        We present heuristics for solving the maximin problem induced by the generative adversarial privacy setting for linear and convolutional neural network (CNN) adversaries. In the linear adversary setting, we present a greedy algorithm for approximating the optimal solution for the privatizer, which performs better as the number of instances increases. We also provide an analysis of the algorithm to show that it not only removes the features most correlated with the private label first, but also preserves the prediction accuracy of public labels that are sufficiently independent of the features that are relevant to the private label. In the CNN adversary setting, we present a method of hiding selected information from the adversary while preserving the others through alternately optimizing the goals of the privatizer and the adversary using neural network backpropagation. We experimentally show that our method succeeds on a fixed adversary.
        △ Less
",present heurist solv maximin problem induc gener adversari privaci set linear convolut neural network cnn adversari linear adversari set present greedi algorithm approxim optim solut privat perform better number instanc increas also provid analysi algorithm show remov featur correl privat label first also preserv predict accuraci public label suffici independ featur relev privat label cnn adversari set present method hide select inform adversari preserv other altern optim goal privat adversari use neural network backpropag experiment show method succe fix adversari less
547,1810.02068,"
        Binarized Neural Network (BNN) removes bitwidth redundancy in classical CNN by using a single bit (-1/+1) for network parameters and intermediate representations, which has greatly reduced the off-chip data transfer and storage overhead. However, a large amount of computation redundancy still exists in BNN inference. By analyzing local properties of images and the learned BNN kernel weights, we observe an average of $\sim$78% input similarity and $\sim$59% weight similarity among weight kernels, measured by our proposed metric in common network architectures. Thus there does exist redundancy that can be exploited to further reduce the amount of on-chip computations.
  Motivated by the observation, in this paper, we proposed two types of fast and energy-efficient architectures for BNN inference. We also provide analysis and insights to pick the better strategy of these two for different datasets and network models. By reusing the results from previous computation, much cycles for data buffer access and computations can be skipped. By experiments, we demonstrate that 80% of the computation and 40% of the buffer access can be skipped by exploiting BNN similarity. Thus, our design can achieve 17% reduction in total power consumption, 54% reduction in on-chip power consumption and 2.4$\times$ maximum speedup, compared to the baseline without applying our reuse technique. Our design also shows 1.9$\times$ more area-efficiency compared to state-of-the-art BNN inference design. We believe our deployment of BNN on FPGA leads to a promising future of running deep learning models on mobile devices.
        △ Less
",binar neural network bnn remov bitwidth redund classic cnn use singl bit network paramet intermedi represent greatli reduc chip data transfer storag overhead howev larg amount comput redund still exist bnn infer analyz local properti imag learn bnn kernel weight observ averag sim input similar sim weight similar among weight kernel measur propos metric common network architectur thu exist redund exploit reduc amount chip comput motiv observ paper propos two type fast energi effici architectur bnn infer also provid analysi insight pick better strategi two differ dataset network model reus result previou comput much cycl data buffer access comput skip experi demonstr comput buffer access skip exploit bnn similar thu design achiev reduct total power consumpt reduct chip power consumpt time maximum speedup compar baselin without appli reus techniqu design also show time area effici compar state art bnn infer design believ deploy bnn fpga lead promis futur run deep learn model mobil devic less
548,1810.02066,"
        In many cases, assessing the quality of goods is hard. For example, when purchasing a car, it is hard to measure how pollutant the car is since there are infinitely many driving conditions to be tested. Typically, these situations are considered under the umbrella of information asymmetry and as Akelrof showed may lead to a market of lemons. However, we argue that in many of these situations, the problem is not the missing information but the computational challenge of obtaining it. In a nut-shell, if verifying the value of goods requires a large amount of computation or even infinite amounts of computation, the buyer is forced to use a finite test that samples, in some sense, the quality of the goods. However, if the seller knows the test, then the seller can over-fit the test and create goods that pass the quality test despite not having the desired quality. We show different solutions to this situation including a novel approach that uses secure computation to hide the test from the seller to prevent over-fitting.
        △ Less
",mani case assess qualiti good hard exampl purchas car hard measur pollut car sinc infinit mani drive condit test typic situat consid umbrella inform asymmetri akelrof show may lead market lemon howev argu mani situat problem miss inform comput challeng obtain nut shell verifi valu good requir larg amount comput even infinit amount comput buyer forc use finit test sampl sens qualiti good howev seller know test seller fit test creat good pass qualiti test despit desir qualiti show differ solut situat includ novel approach use secur comput hide test seller prevent fit less
549,1810.02062,"
        Encryption-then-Compression (EtC) systems have been proposed to securely transmit images through an untrusted channel provider. In this study, EtC systems were applied to social media like Twitter that carry out image manipulations. The block scrambling-based encryption schemes used in EtC systems were evaluated in terms of their robustness against image manipulation on social media. The aim was to investigate how five social networking service (SNS) providers, Facebook, Twitter, Google+, Tumblr and Flickr, manipulate images and to determine whether the encrypted images uploaded to SNS providers can avoid being distorted by such manipulations. In an experiment, encrypted and non-encrypted JPEG images were uploaded to various SNS providers. The results show that EtC systems are applicable to the five SNS providers.
        △ Less
",encrypt compress etc system propos secur transmit imag untrust channel provid studi etc system appli social media like twitter carri imag manipul block scrambl base encrypt scheme use etc system evalu term robust imag manipul social media aim investig five social network servic sn provid facebook twitter googl tumblr flickr manipul imag determin whether encrypt imag upload sn provid avoid distort manipul experi encrypt non encrypt jpeg imag upload variou sn provid result show etc system applic five sn provid less
550,1810.02061,"
        Data-intensive applications exhibit increasing reliance on Database Management Systems (DBMSs, for short). With the growing cyber-security threats to government and commercial infrastructures, the need to develop high resilient cyber systems is becoming increasingly important. Cyber-attacks on DBMSs include intrusion attacks that may result in severe degradation in performance. Several efforts have been directed towards designing an integrated management system to detect, respond, and recover from malicious attacks. In this paper, we propose a data Partitioning-based Intrusion Management System (PIMS, for short) that can endure intense malicious intrusion attacks on DBMS. The novelty in PIMS is the ability to contain the damage into data partitions, termed Intrusion Boundaries (IBs, for short). The IB Demarcation Problem (IBDP, for short) is formulated as a mixed integer nonlinear programming. We prove that IBDP is NP-hard. Accordingly, two heuristic solutions for IBDP are introduced. The proposed architecture for PIMS includes novel IB-centric response and recovery mechanisms, which executes compensating transactions. PIMS is prototyped within PostgreSQL, an open-source DBMS. Finally, empirical and experimental performance evaluation of PIMS are conducted to demonstrate that intelligent partitioning of data tuples improves the overall availability of the DBMS under intrusion attacks.
        △ Less
",data intens applic exhibit increas relianc databas manag system dbmss short grow cyber secur threat govern commerci infrastructur need develop high resili cyber system becom increasingli import cyber attack dbmss includ intrus attack may result sever degrad perform sever effort direct toward design integr manag system detect respond recov malici attack paper propos data partit base intrus manag system pim short endur intens malici intrus attack dbm novelti pim abil contain damag data partit term intrus boundari ib short ib demarc problem ibdp short formul mix integ nonlinear program prove ibdp np hard accordingli two heurist solut ibdp introduc propos architectur pim includ novel ib centric respons recoveri mechan execut compens transact pim prototyp within postgresql open sourc dbm final empir experiment perform evalu pim conduct demonstr intellig partit data tupl improv overal avail dbm intrus attack less
551,1810.02054,"
        One of the mystery in the success of neural networks is randomly initialized first order methods like gradient descent can achieve zero training loss even though the objective function is non-convex and non-smooth. This paper demystifies this surprising phenomenon for two-layer fully connected ReLU activated neural networks. For an $m$ hidden node shallow neural network with ReLU activation and $n$ training data, we show as long as $m$ is large enough and the data is non-degenerate, randomly initialized gradient descent converges a globally optimal solution with a linear convergence rate for the quadratic loss function.
  Our analysis is based on the following observation: over-parameterization and random initialization jointly restrict every weight vector to be close to its initialization for all iterations, which allows us to exploit a strong convexity-like property to show that gradient descent converges at a global linear rate to the global optimum. We believe these insights are also useful in analyzing deep models and other first order methods.
        △ Less
",one mysteri success neural network randomli initi first order method like gradient descent achiev zero train loss even though object function non convex non smooth paper demystifi surpris phenomenon two layer fulli connect relu activ neural network hidden node shallow neural network relu activ n train data show long larg enough data non degener randomli initi gradient descent converg global optim solut linear converg rate quadrat loss function analysi base follow observ parameter random initi jointli restrict everi weight vector close initi iter allow us exploit strong convex like properti show gradient descent converg global linear rate global optimum believ insight also use analyz deep model first order method less
552,1810.02053,"
        This volume contains the proceedings of ICE'18, the 11th Interaction and Concurrency Experience, which was held in Madrid, Spain on the 20th and 21st of June 2018 as a satellite event of DisCoTec'18. 
  The ICE workshop series features a distinguishing review and selection procedure, allowing PC members to interact anonymously with authors. As in the past ten editions, this interaction considerably improved the accuracy of the feedback from the reviewers and the quality of accepted papers, and offered the basis for lively discussion during the workshop. For the second time, the 2018 edition of ICE included double blind reviewing of original research papers, in order to increase fairness and avoid bias in reviewing.
  Each paper was reviewed by three PC members, and altogether six papers were accepted for publication (the workshop also featured four oral presentations which are not part of this volume). We were proud to host three invited talks, by Elvira Albert, Silvia Crafa, and Alexey Gotsman. The abstracts of these talks are included in this volume together with the regular papers. Final versions of the contributions, taking into account the discussion at the workshop, are included. 
        △ Less
",volum contain proceed ice th interact concurr experi held madrid spain th st june satellit event discotec ice workshop seri featur distinguish review select procedur allow pc member interact anonym author past ten edit interact consider improv accuraci feedback review qualiti accept paper offer basi live discuss workshop second time edit ice includ doubl blind review origin research paper order increas fair avoid bia review paper review three pc member altogeth six paper accept public workshop also featur four oral present part volum proud host three invit talk elvira albert silvia crafa alexey gotsman abstract talk includ volum togeth regular paper final version contribut take account discuss workshop includ less
553,1810.02047,"
        The formalism of multiple context-free grammars (MCFG) is a non-trivial generalization of context-free grammars (CFG), where basic constituents on which rules operate are discontinuous tuples of words rather than single words. Just as context-free ones, multiple context-free grammars have polynomial parsing algorithms, but their expressive power is strictly stronger.
  It is well known that CFG generate the same class of languages as type logical grammars based on Lambek calculus, which is, basically, a variant of noncommutative linear logic.
  We construct a system of type logical grammars based on ordinary commutative linear logic and show that these grammars are in the same relationship with MCFG as Lambek grammars with CFG.
  It turns out that tuples of words on which MCFG operate can be organized into a symmetric monoidal category, very similar to the category of topological cobordisms; we call it the category of word cobordisms. In particular, this category is compact closed and, thus, a model of linear logic. Using interpretation of linear logic proofs as word cobordisms allows us to define type logical grammars by adding extra axioms (a lexicon) and interpreting them as cobordisms as well. Such grammars turn out to be equivalent to MCFG.
        △ Less
",formal multipl context free grammar mcfg non trivial gener context free grammar cfg basic constitu rule oper discontinu tupl word rather singl word context free one multipl context free grammar polynomi pars algorithm express power strictli stronger well known cfg gener class languag type logic grammar base lambek calculu basic variant noncommut linear logic construct system type logic grammar base ordinari commut linear logic show grammar relationship mcfg lambek grammar cfg turn tupl word mcfg oper organ symmetr monoid categori similar categori topolog cobord call categori word cobord particular categori compact close thu model linear logic use interpret linear logic proof word cobord allow us defin type logic grammar ad extra axiom lexicon interpret cobord well grammar turn equival mcfg less
554,1810.02042,"
        In this paper, we present a novel method for learning to synthesize 3D mesh animation sequences with long short-term memory (LSTM) blocks and mesh-based convolutional neural networks (CNNs). Synthesizing realistic 3D mesh animation sequences is a challenging and important task in computer animation. To achieve this, researchers have long been focusing on shape analysis to develop new interpolation and extrapolation techniques. However, such techniques have limited learning capabilities and therefore can produce unrealistic animation. Deep architectures that operate directly on mesh sequences remain unexplored, due to the following major barriers: meshes with irregular triangles, sequences containing rich temporal information and flexible deformations. To address these, we utilize convolutional neural networks defined on triangular meshes along with a shape deformation representation to extract useful features, followed by LSTM cells that iteratively process the features. To allow completion of a missing mesh sequence from given endpoints, we propose a new weight-shared bidirectional structure. The bidirectional generation loss also helps mitigate error accumulation over iterations. Benefiting from all these technical advances, our approach outperforms existing methods in sequence prediction and completion both qualitatively and quantitatively. Moreover, this network can also generate follow-up frames conditioned on initial shapes and improve the accuracy as more bootstrap models are provided, which other works in the geometry processing domain cannot achieve.
        △ Less
",paper present novel method learn synthes mesh anim sequenc long short term memori lstm block mesh base convolut neural network cnn synthes realist mesh anim sequenc challeng import task comput anim achiev research long focus shape analysi develop new interpol extrapol techniqu howev techniqu limit learn capabl therefor produc unrealist anim deep architectur oper directli mesh sequenc remain unexplor due follow major barrier mesh irregular triangl sequenc contain rich tempor inform flexibl deform address util convolut neural network defin triangular mesh along shape deform represent extract use featur follow lstm cell iter process featur allow complet miss mesh sequenc given endpoint propos new weight share bidirect structur bidirect gener loss also help mitig error accumul iter benefit technic advanc approach outperform exist method sequenc predict complet qualit quantit moreov network also gener follow frame condit initi shape improv accuraci bootstrap model provid work geometri process domain cannot achiev less
555,1810.02032,"
        This paper establishes risk convergence and asymptotic weight matrix alignment --- a form of implicit regularization --- of gradient flow and gradient descent when applied to deep linear networks on linearly separable data. In more detail, for gradient flow applied to strictly decreasing loss functions (with similar results for gradient descent with particular decreasing step sizes): (i) the risk converges to 0; (ii) the normalized i-th weight matrix asymptotically equals its rank-1 approximation $u_iv_i^{\top}$; (iii) these rank-1 matrices are aligned across layers, meaning $|v_{i+1}^{\top}u_i|\to1$. In the case of the logistic loss (binary cross entropy), more can be said: the linear function induced by the network --- the product of its weight matrices --- converges to the same direction as the maximum margin solution. This last property was identified in prior work, but only under assumptions on gradient descent which here are implied by the alignment phenomenon.
        △ Less
",paper establish risk converg asymptot weight matrix align form implicit regular gradient flow gradient descent appli deep linear network linearli separ data detail gradient flow appli strictli decreas loss function similar result gradient descent particular decreas step size risk converg ii normal th weight matrix asymptot equal rank approxim u iv top iii rank matric align across layer mean v top u case logist loss binari cross entropi said linear function induc network product weight matric converg direct maximum margin solut last properti identifi prior work assumpt gradient descent impli align phenomenon less
556,1810.02023,"
        Modern malware typically makes use of a domain generation algorithm (DGA) to avoid command and control domains or IPs being seized or sinkholed. This means that an infected system may attempt to access many domains in an attempt to contact the command and control server. Therefore, the automatic detection of DGA domains is an important task, both for the sake of blocking malicious domains and identifying compromised hosts. However, many DGAs use English wordlists to generate plausibly clean-looking domain names; this makes automatic detection difficult. In this work, we devise a notion of difficulty for DGA families called the smashword score; this measures how much a DGA family looks like English words. We find that this measure accurately reflects how much a DGA family's domains look like they are made from natural English words. We then describe our new modeling approach, which is a combination of a novel recurrent neural network architecture with domain registration side information. Our experiments show the model is capable of effectively identifying domains generated by difficult DGA families. Our experiments also show that our model outperforms existing approaches, and is able to reliably detect difficult DGA families such as matsnu, suppobox, rovnix, and others. The model's performance compared to the state of the art is best for DGA families that resemble English words. We believe that this model could either be used in a standalone DGA domain detector---such as an endpoint security application---or alternately the model could be used as a part of a larger malware detection system.
        △ Less
",modern malwar typic make use domain gener algorithm dga avoid command control domain ip seiz sinkhol mean infect system may attempt access mani domain attempt contact command control server therefor automat detect dga domain import task sake block malici domain identifi compromis host howev mani dga use english wordlist gener plausibl clean look domain name make automat detect difficult work devis notion difficulti dga famili call smashword score measur much dga famili look like english word find measur accur reflect much dga famili domain look like made natur english word describ new model approach combin novel recurr neural network architectur domain registr side inform experi show model capabl effect identifi domain gener difficult dga famili experi also show model outperform exist approach abl reliabl detect difficult dga famili matsnu suppobox rovnix other model perform compar state art best dga famili resembl english word believ model could either use standalon dga domain detector endpoint secur applic altern model could use part larger malwar detect system less
557,1810.02020,"
        Deep learning-based methods have reached state of the art performances, relying on large quantity of available data and computational power. Such methods still remain highly inappropriate when facing a major open machine learning problem, which consists of learning incrementally new classes and examples over time. Combining the outstanding performances of Deep Neural Networks (DNNs) with the flexibility of incremental learning techniques is a promising venue of research. In this contribution, we introduce Transfer Incremental Learning using Data Augmentation (TILDA). TILDA is based on pre-trained DNNs as feature extractor, robust selection of feature vectors in subspaces using a nearest-class-mean based technique, majority votes and data augmentation at both the training and the prediction stages. Experiments on challenging vision datasets demonstrate the ability of the proposed method for low complexity incremental learning, while achieving significantly better accuracy than existing incremental counterparts.
        △ Less
",deep learn base method reach state art perform reli larg quantiti avail data comput power method still remain highli inappropri face major open machin learn problem consist learn increment new class exampl time combin outstand perform deep neural network dnn flexibl increment learn techniqu promis venu research contribut introduc transfer increment learn use data augment tilda tilda base pre train dnn featur extractor robust select featur vector subspac use nearest class mean base techniqu major vote data augment train predict stage experi challeng vision dataset demonstr abil propos method low complex increment learn achiev significantli better accuraci exist increment counterpart less
558,1810.02019,"
        Ranking is a central task in machine learning and information retrieval. In this task, it is especially important to present the user with a slate of items that is appealing as a whole. This in turn requires taking into account interactions between items, since intuitively, placing an item on the slate affects the decision of which other items should be placed alongside it. In this work, we propose a sequence-to-sequence model for ranking called seq2slate. At each step, the model predicts the next item to place on the slate given the items already selected. The recurrent nature of the model allows complex dependencies between items to be captured directly in a flexible and scalable way. We show how to learn the model end-to-end from weak supervision in the form of easily obtained click-through data. We further demonstrate the usefulness of our approach in experiments on standard ranking benchmarks as well as in a real-world recommendation system.
        △ Less
",rank central task machin learn inform retriev task especi import present user slate item appeal whole turn requir take account interact item sinc intuit place item slate affect decis item place alongsid work propos sequenc sequenc model rank call seq slate step model predict next item place slate given item alreadi select recurr natur model allow complex depend item captur directli flexibl scalabl way show learn model end end weak supervis form easili obtain click data demonstr use approach experi standard rank benchmark well real world recommend system less
559,1810.02017,"
        We describe a multi-phased Wizard-of-Oz approach to collecting human-robot dialogue in a collaborative search and navigation task. The data is being used to train an initial automated robot dialogue system to support collaborative exploration tasks. In the first phase, a wizard freely typed robot utterances to human participants. For the second phase, this data was used to design a GUI that includes buttons for the most common communications, and templates for communications with varying parameters. Comparison of the data gathered in these phases show that the GUI enabled a faster pace of dialogue while still maintaining high coverage of suitable responses, enabling more efficient targeted data collection, and improvements in natural language understanding using GUI-collected data. As a promising first step towards interactive learning, this work shows that our approach enables the collection of useful training data for navigation-based HRI tasks.
        △ Less
",describ multi phase wizard oz approach collect human robot dialogu collabor search navig task data use train initi autom robot dialogu system support collabor explor task first phase wizard freeli type robot utter human particip second phase data use design gui includ button common commun templat commun vari paramet comparison data gather phase show gui enabl faster pace dialogu still maintain high coverag suitabl respons enabl effici target data collect improv natur languag understand use gui collect data promis first step toward interact learn work show approach enabl collect use train data navig base hri task less
560,1810.02013,"
        In this paper, we present a probabilistic framework to assess the impacts of different network tariffs on the consumption pattern of electricity consumers with distributed energy resources such as thermostatically controlled loads and battery storage; and the resultant effects on the distribution network. A mixed integer linear programming-based home energy management system with implicit modeling of peak demand charge is used to schedule the controllable devices of residential customers connected to a low voltage network in order to analyze the impacts of \textit{energy-} and \textit{demand-based tariffs} on network performance. The simulation results show that flat tariffs with a peak demand component perform best in terms of electricity cost reduction for the customer, as well as in mitigating the level of binding network constraints. This is beneficial for distribution network service providers where there is high PV-battery penetration.
        △ Less
",paper present probabilist framework assess impact differ network tariff consumpt pattern electr consum distribut energi resourc thermostat control load batteri storag result effect distribut network mix integ linear program base home energi manag system implicit model peak demand charg use schedul control devic residenti custom connect low voltag network order analyz impact textit energi textit demand base tariff network perform simul result show flat tariff peak demand compon perform best term electr cost reduct custom well mitig level bind network constraint benefici distribut network servic provid high pv batteri penetr less
561,1810.02010,"
        There is growing interest in object detection in advanced driver assistance systems and autonomous robots and vehicles. To enable such innovative systems, we need faster object detection. In this work, we investigate the trade-off between accuracy and speed with domain-specific approximations, i.e. category-aware image size scaling and proposals scaling, for two state-of-the-art deep learning-based object detection meta-architectures. We study the effectiveness of applying approximation both statically and dynamically to understand the potential and the applicability of them. By conducting experiments on the ImageNet VID dataset, we show that domain-specific approximation has great potential to improve the speed of the system without deteriorating the accuracy of object detectors, i.e. up to 7.5x speedup for dynamic domain-specific approximation. To this end, we present our insights toward harvesting domain-specific approximation as well as devise a proof-of-concept runtime, AutoFocus, that exploits dynamic domain-specific approximation.
        △ Less
",grow interest object detect advanc driver assist system autonom robot vehicl enabl innov system need faster object detect work investig trade accuraci speed domain specif approxim e categori awar imag size scale propos scale two state art deep learn base object detect meta architectur studi effect appli approxim static dynam understand potenti applic conduct experi imagenet vid dataset show domain specif approxim great potenti improv speed system without deterior accuraci object detector e x speedup dynam domain specif approxim end present insight toward harvest domain specif approxim well devis proof concept runtim autofocu exploit dynam domain specif approxim less
562,1810.02003,"
        A popular methodology for building binary decision-making classifiers in the presence of imperfect information is to first construct a non-binary ""scoring"" classifier that is calibrated over all protected groups, and then to post-process this score to obtain a binary decision. We study the feasibility of achieving various fairness properties by post-processing calibrated scores, and then show that deferring post-processors allow for more fairness conditions to hold on the final decision. Specifically, we show:
  1. There does not exist a general way to post-process a calibrated classifier to equalize protected groups' positive or negative predictive value (PPV or NPV). For certain ""nice"" calibrated classifiers, either PPV or NPV can be equalized when the post-processor uses different thresholds across protected groups, though there exist distributions of calibrated scores for which the two measures cannot be both equalized. When the post-processing consists of a single global threshold across all groups, natural fairness properties, such as equalizing PPV in a nontrivial way, do not hold even for ""nice"" classifiers.
  2. When the post-processing is allowed to `defer' on some decisions (that is, to avoid making a decision by handing off some examples to a separate process), then for the non-deferred decisions, the resulting classifier can be made to equalize PPV, NPV, false positive rate (FPR) and false negative rate (FNR) across the protected groups. This suggests a way to partially evade the impossibility results of Chouldechova and Kleinberg et al., which preclude equalizing all of these measures simultaneously. We also present different deferring strategies and show how they affect the fairness properties of the overall system.
  We evaluate our post-processing techniques using the COMPAS data set from 2016.
        △ Less
",popular methodolog build binari decis make classifi presenc imperfect inform first construct non binari score classifi calibr protect group post process score obtain binari decis studi feasibl achiev variou fair properti post process calibr score show defer post processor allow fair condit hold final decis specif show exist gener way post process calibr classifi equal protect group posit neg predict valu ppv npv certain nice calibr classifi either ppv npv equal post processor use differ threshold across protect group though exist distribut calibr score two measur cannot equal post process consist singl global threshold across group natur fair properti equal ppv nontrivi way hold even nice classifi post process allow defer decis avoid make decis hand exampl separ process non defer decis result classifi made equal ppv npv fals posit rate fpr fals neg rate fnr across protect group suggest way partial evad imposs result chouldechova kleinberg et al preclud equal measur simultan also present differ defer strategi show affect fair properti overal system evalu post process techniqu use compa data set less
563,1810.02002,"
        Social relationships can be divided into different classes based on the regularity with which they occur and the similarity among them. Thus, rare and somewhat similar relationships are random and cause noise in a social network, thus hiding the actual structure of the network and preventing an accurate analysis of it. In this context, in this paper we propose a process to handle social network data that exploits temporal features to improve the detection of communities by existing algorithms. By removing random interactions, we observe that social networks converge to a topology with more purely social relationships and more modular communities.
        △ Less
",social relationship divid differ class base regular occur similar among thu rare somewhat similar relationship random caus nois social network thu hide actual structur network prevent accur analysi context paper propos process handl social network data exploit tempor featur improv detect commun exist algorithm remov random interact observ social network converg topolog pure social relationship modular commun less
564,1810.02001,"
        Multi-modal approaches employ data from multiple input streams such as textual and visual domains. Deep neural networks have been successfully employed for these approaches. In this paper, we present a novel multi-modal approach that fuses images and text descriptions to improve multi-modal classification performance in real-world scenarios. The proposed approach embeds an encoded text onto an image to obtain an information-enriched image. To learn feature representations of resulting images, standard Convolutional Neural Networks (CNNs) are employed for the classification task. We demonstrate how a CNN based pipeline can be used to learn representations of the novel fusion approach. We compare our approach with individual sources on two large-scale multi-modal classification datasets while obtaining encouraging results. Furthermore, we evaluate our approach against two famous multi-modal strategies namely early fusion and late fusion.
        △ Less
",multi modal approach employ data multipl input stream textual visual domain deep neural network success employ approach paper present novel multi modal approach fuse imag text descript improv multi modal classif perform real world scenario propos approach emb encod text onto imag obtain inform enrich imag learn featur represent result imag standard convolut neural network cnn employ classif task demonstr cnn base pipelin use learn represent novel fusion approach compar approach individu sourc two larg scale multi modal classif dataset obtain encourag result furthermor evalu approach two famou multi modal strategi name earli fusion late fusion less
565,1810.01997,"
        Research in transaction processing has made significant progress in improving the performance of multi-core in-memory transactional systems. However, the focus has mainly been on low-contention workloads. Modern transactional systems perform poorly on workloads with transactions accessing a few highly contended data items. We observe that most transactional workloads, including those with high contention, can be divided into clusters of data conflict-free transactions and a small set of residuals. In this paper, we introduce a new concurrency control protocol called Strife that leverages the above observation. Strife executes transactions in batches, where each batch is partitioned into clusters of conflict-free transactions and a small set of residual transactions. The conflict-free clusters are executed in parallel without any concurrency control, followed by executing the residual cluster either serially or with concurrency control. We present a low-overhead algorithm that partitions a batch of transactions into clusters that do not have cross-cluster conflicts and a small residual cluster. We evaluate Strife against the optimistic concurrency control protocol and several variants of two-phase locking, where the latter is known to perform better than other concurrency protocols under high contention, and show that Strife can improve transactional throughput by up to 2x. We also perform an in-depth micro-benchmark analysis to empirically characterize the performance and quality of our clustering algorithm
        △ Less
",research transact process made signific progress improv perform multi core memori transact system howev focu mainli low content workload modern transact system perform poorli workload transact access highli contend data item observ transact workload includ high content divid cluster data conflict free transact small set residu paper introduc new concurr control protocol call strife leverag observ strife execut transact batch batch partit cluster conflict free transact small set residu transact conflict free cluster execut parallel without concurr control follow execut residu cluster either serial concurr control present low overhead algorithm partit batch transact cluster cross cluster conflict small residu cluster evalu strife optimist concurr control protocol sever variant two phase lock latter known perform better concurr protocol high content show strife improv transact throughput x also perform depth micro benchmark analysi empir character perform qualiti cluster algorithm less
566,1810.01993,"
        We extract pixel-level masks of extreme weather patterns using variants of Tiramisu and DeepLabv3+ neural networks. We describe improvements to the software frameworks, input pipeline, and the network training algorithms necessary to efficiently scale deep learning on the Piz Daint and Summit systems. The Tiramisu network scales to 5300 P100 GPUs with a sustained throughput of 21.0 PF/s and parallel efficiency of 79.0%. DeepLabv3+ scales up to 27360 V100 GPUs with a sustained throughput of 325.8 PF/s and a parallel efficiency of 90.7% in single precision. By taking advantage of the FP16 Tensor Cores, a half-precision version of the DeepLabv3+ network achieves a peak and sustained throughput of 1.13 EF/s and 999.0 PF/s respectively.
        △ Less
",extract pixel level mask extrem weather pattern use variant tiramisu deeplabv neural network describ improv softwar framework input pipelin network train algorithm necessari effici scale deep learn piz daint summit system tiramisu network scale p gpu sustain throughput pf parallel effici deeplabv scale v gpu sustain throughput pf parallel effici singl precis take advantag fp tensor core half precis version deeplabv network achiev peak sustain throughput ef pf respect less
567,1810.01992,"
        In the field of Automated Planning and Scheduling (APS), intelligent agents by virtue require an action model (blueprints of actions whose interleaved executions effectuates transitions of the system state) in order to plan and solve real world problems. It is, however, becoming increasingly cumbersome to codify this model, and is more efficient to learn it from observed plan execution sequences (training data). While the underlying objective is to subsequently plan from this learnt model, most approaches fall short as anything less than a flawless reconstruction of the underlying model renders it unusable in certain domains. This work presents a novel approach using long short-term memory (LSTM) techniques for the acquisition of the underlying action model. We use the sequence labelling capabilities of LSTMs to isolate from an exhaustive model set a model identical to the one responsible for producing the training data. This isolation capability renders our approach as an effective one.
        △ Less
",field autom plan schedul ap intellig agent virtu requir action model blueprint action whose interleav execut effectu transit system state order plan solv real world problem howev becom increasingli cumbersom codifi model effici learn observ plan execut sequenc train data underli object subsequ plan learnt model approach fall short anyth less flawless reconstruct underli model render unus certain domain work present novel approach use long short term memori lstm techniqu acquisit underli action model use sequenc label capabl lstm isol exhaust model set model ident one respons produc train data isol capabl render approach effect one less
568,1810.01989,"
        This survey presents an overview of verification techniques for autonomous systems, with a focus on safety-critical autonomous cyber-physical systems (CPS) and subcomponents thereof. Autonomy in CPS is enabling by recent advances in artificial intelligence (AI) and machine learning (ML) through approaches such as deep neural networks (DNNs), embedded in so-called learning enabled components (LECs) that accomplish tasks from classification to control. Recently, the formal methods and formal verification community has developed methods to characterize behaviors in these LECs with eventual goals of formally verifying specifications for LECs, and this article presents a survey of many of these recent approaches.
        △ Less
",survey present overview verif techniqu autonom system focu safeti critic autonom cyber physic system cp subcompon thereof autonomi cp enabl recent advanc artifici intellig ai machin learn ml approach deep neural network dnn embed call learn enabl compon lec accomplish task classif control recent formal method formal verif commun develop method character behavior lec eventu goal formal verifi specif lec articl present survey mani recent approach less
569,1810.01987,"
        The Blackbird unmanned aerial vehicle (UAV) dataset is a large-scale, aggressive indoor flight dataset collected using a custom-built quadrotor platform for use in evaluation of agile perception.Inspired by the potential of future high-speed fully-autonomous drone racing, the Blackbird dataset contains over 10 hours of flight data from 168 flights over 17 flight trajectories and 5 environments at velocities up to $7.0ms^-1$. Each flight includes sensor data from 120Hz stereo and downward-facing photorealistic virtual cameras, 100Hz IMU, $\sim190Hz$ motor speed sensors, and 360Hz millimeter-accurate motion capture ground truth. Camera images for each flight were photorealistically rendered using FlightGoggles across a variety of environments to facilitate easy experimentation of high performance perception algorithms. The dataset is available for download at http://blackbird-dataset.mit.edu/
        △ Less
",blackbird unman aerial vehicl uav dataset larg scale aggress indoor flight dataset collect use custom built quadrotor platform use evalu agil percept inspir potenti futur high speed fulli autonom drone race blackbird dataset contain hour flight data flight flight trajectori environ veloc ms flight includ sensor data hz stereo downward face photorealist virtual camera hz imu sim hz motor speed sensor hz millimet accur motion captur ground truth camera imag flight photorealist render use flightgoggl across varieti environ facilit easi experiment high perform percept algorithm dataset avail download http blackbird dataset mit edu less
570,1810.01982,"
        While E-commerce has been growing explosively and online shopping has become popular and even dominant in the present era, online transaction fraud control has drawn considerable attention in business practice and academic research. Conventional fraud control considers mainly the interactions of two major involved decision parties, i.e. merchants and fraudsters, to make fraud classification decision without paying much attention to dynamic looping effect arose from the decisions made by other profit-related parties. This paper proposes a novel fraud control framework that can quantify interactive effects of decisions made by different parties and can adjust fraud control strategies using data analytics, artificial intelligence, and dynamic optimization techniques. Three control models, Naive, Myopic and Prospective Controls, were developed based on the availability of data attributes and levels of label maturity. The proposed models are purely data-driven and self-adaptive in a real-time manner. The field test on Microsoft real online transaction data suggested that new systems could sizably improve the company's profit.
        △ Less
",e commerc grow explos onlin shop becom popular even domin present era onlin transact fraud control drawn consider attent busi practic academ research convent fraud control consid mainli interact two major involv decis parti e merchant fraudster make fraud classif decis without pay much attent dynam loop effect aros decis made profit relat parti paper propos novel fraud control framework quantifi interact effect decis made differ parti adjust fraud control strategi use data analyt artifici intellig dynam optim techniqu three control model naiv myopic prospect control develop base avail data attribut level label matur propos model pure data driven self adapt real time manner field test microsoft real onlin transact data suggest new system could sizabl improv compani profit less
571,1810.01977,"
        The design of feedback controllers for bipedal robots is challenging due to the hybrid nature of its dynamics and the complexity imposed by high-dimensional bipedal models. In this paper, we present a novel approach for the design of feedback controllers using Reinforcement Learning (RL) and Hybrid Zero Dynamics (HZD). Existing RL approaches for bipedal walking are inefficient as they do not consider the underlying physics, often requires substantial training, and the resulting controller may not be applicable to real robots. HZD is a powerful tool for bipedal control with local stability guarantees of the walking limit cycles. In this paper, we propose a non traditional RL structure that embeds the HZD framework into the policy learning. More specifically, we propose to use RL to find a control policy that maps from the robot's reduced order states to a set of parameters that define the desired trajectories for the robot's joints through the virtual constraints. Then, these trajectories are tracked using an adaptive PD controller. The method results in a stable and robust control policy that is able to track variable speed within a continuous interval. Robustness of the policy is evaluated by applying external forces to the torso of the robot. The proposed RL framework is implemented and demonstrated in OpenAI Gym with the MuJoCo physics engine based on the well-known RABBIT robot model.
        △ Less
",design feedback control biped robot challeng due hybrid natur dynam complex impos high dimension biped model paper present novel approach design feedback control use reinforc learn rl hybrid zero dynam hzd exist rl approach biped walk ineffici consid underli physic often requir substanti train result control may applic real robot hzd power tool biped control local stabil guarante walk limit cycl paper propos non tradit rl structur emb hzd framework polici learn specif propos use rl find control polici map robot reduc order state set paramet defin desir trajectori robot joint virtual constraint trajectori track use adapt pd control method result stabl robust control polici abl track variabl speed within continu interv robust polici evalu appli extern forc torso robot propos rl framework implement demonstr openai gym mujoco physic engin base well known rabbit robot model less
572,1810.01973,"
        The reconfigurability, energy-efficiency, and massive parallelism on FPGAs make them one of the best choices for implementing efficient deep learning accelerators. However, state-of-art implementations seldom consider the balance between high throughput of computation power and the ability of the memory subsystem to support it. In this paper, we implement an accelerator on FPGA by combining the sparse Winograd convolution, clusters of small-scale systolic arrays, and a tailored memory layout design. We also provide an analytical model analysis for the general Winograd convolution algorithm as a design reference. Experimental results on VGG16 show that it achieves very high computational resource utilization, 20x ~ 30x energy efficiency, and more than 5x speedup compared with the dense implementation.
        △ Less
",reconfigur energi effici massiv parallel fpga make one best choic implement effici deep learn acceler howev state art implement seldom consid balanc high throughput comput power abil memori subsystem support paper implement acceler fpga combin spars winograd convolut cluster small scale systol array tailor memori layout design also provid analyt model analysi gener winograd convolut algorithm design refer experiment result vgg show achiev high comput resourc util x x energi effici x speedup compar dens implement less
573,1810.01969,"
        Using a mild variant of polar codes we design linear compression schemes compressing Hidden Markov sources (where the source is a Markov chain, but whose state is not necessarily observable from its output), and to decode from Hidden Markov channels (where the channel has a state and the error introduced depends on the state). We give the first polynomial time algorithms that manage to compress and decompress (or encode and decode) at input lengths that are polynomial $\it{both}$ in the gap to capacity and the mixing time of the Markov chain. Prior work achieved capacity only asymptotically in the limit of large lengths, and polynomial bounds were not available with respect to either the gap to capacity or mixing time. Our results operate in the setting where the source (or the channel) is $\it{known}$. If the source is $\it{unknown}$ then compression at such short lengths would lead to effective algorithms for learning parity with noise -- thus our results are the first to suggest a separation between the complexity of the problem when the source is known versus when it is unknown.
        △ Less
",use mild variant polar code design linear compress scheme compress hidden markov sourc sourc markov chain whose state necessarili observ output decod hidden markov channel channel state error introduc depend state give first polynomi time algorithm manag compress decompress encod decod input length polynomi gap capac mix time markov chain prior work achiev capac asymptot limit larg length polynomi bound avail respect either gap capac mix time result oper set sourc channel known sourc unknown compress short length would lead effect algorithm learn pariti nois thu result first suggest separ complex problem sourc known versu unknown less
574,1810.01967,"
        Current popular methods for Magnetic Resonance Fingerprint (MRF) recovery are bottlenecked by the heavy computations of a matched-filtering step due to the growing size and complexity of the fingerprint dictionaries in multi-parametric quantitative MRI applications. We address this shortcoming by arranging dictionary atoms in the form of cover tree structures and adopt the corresponding fast approximate nearest neighbour searches to accelerate matched-filtering. For datasets belonging to smooth low-dimensional manifolds cover trees offer search complexities logarithmic in terms of data population. With this motivation we propose an iterative reconstruction algorithm, named CoverBLIP, to address large-size MRF problems where the fingerprint dictionary i.e. discrete manifold of Bloch responses, encodes several intrinsic NMR parameters. We study different forms of convergence for this algorithm and we show that provided with a notion of embedding, the inexact and non-convex iterations of CoverBLIP linearly convergence toward a near-global solution with the same order of accuracy as using exact brute-force searches. Our further examinations on both synthetic and real-world datasets and using different sampling strategies, indicates between 2 to 3 orders of magnitude reduction in total search computations. Cover trees are robust against the curse-of-dimensionality and therefore CoverBLIP provides a notion of scalability -- a consistent gain in time-accuracy performance-- for searching high-dimensional atoms which may not be easily preprocessed (i.e. for dimensionality reduction) due to the increasing degrees of non-linearities appearing in the emerging multi-parametric MRF dictionaries.
        △ Less
",current popular method magnet reson fingerprint mrf recoveri bottleneck heavi comput match filter step due grow size complex fingerprint dictionari multi parametr quantit mri applic address shortcom arrang dictionari atom form cover tree structur adopt correspond fast approxim nearest neighbour search acceler match filter dataset belong smooth low dimension manifold cover tree offer search complex logarithm term data popul motiv propos iter reconstruct algorithm name coverblip address larg size mrf problem fingerprint dictionari e discret manifold bloch respons encod sever intrins nmr paramet studi differ form converg algorithm show provid notion embed inexact non convex iter coverblip linearli converg toward near global solut order accuraci use exact brute forc search examin synthet real world dataset use differ sampl strategi indic order magnitud reduct total search comput cover tree robust curs dimension therefor coverblip provid notion scalabl consist gain time accuraci perform search high dimension atom may easili preprocess e dimension reduct due increas degre non linear appear emerg multi parametr mrf dictionari less
575,1810.01966,"
        We characterize the accuracy of analyzing the performance of a NOMA system where users are ranked according to their distances instead of instantaneous channel gains, i.e., product of distance-based path-loss and fading channel gains. Distance-based ranking is analytically tractable and can lead to important insights. However, it may not be appropriate in a multipath fading environment where a near user suffers from severe fading while a far user experiences weak fading. Since the ranking of users in a NOMA system has a direct impact on coverage probability analysis, impact of the traditional distance-based ranking, as opposed to instantaneous signal power-based ranking, needs to be understood. This will enable us to identify scenarios where distance-based ranking, which is easier to implement compared to instantaneous signal power-based ranking, is acceptable for system performance analysis. To this end, in this paper, we derive the probability of the event when distance-based ranking yields the same results as instantaneous signal power-based ranking, which is referred to as the accuracy probability. We characterize the probability of accuracy considering Nakagami-m fading channels and three different spatial distribution models of user locations in NOMA. We illustrate the impact of accuracy probability on uplink and downlink coverage probability.
        △ Less
",character accuraci analyz perform noma system user rank accord distanc instead instantan channel gain e product distanc base path loss fade channel gain distanc base rank analyt tractabl lead import insight howev may appropri multipath fade environ near user suffer sever fade far user experi weak fade sinc rank user noma system direct impact coverag probabl analysi impact tradit distanc base rank oppos instantan signal power base rank need understood enabl us identifi scenario distanc base rank easier implement compar instantan signal power base rank accept system perform analysi end paper deriv probabl event distanc base rank yield result instantan signal power base rank refer accuraci probabl character probabl accuraci consid nakagami fade channel three differ spatial distribut model user locat noma illustr impact accuraci probabl uplink downlink coverag probabl less
576,1810.01965,"
        Earthquake signal detection is at the core of observational seismology. A good detection algorithm should be sensitive to small and weak events with a variety of waveform shapes, robust to background noise and non-earthquake signals, and efficient for processing large data volumes. Here, we introduce the Cnn-Rnn Earthquake Detector (CRED), a detector based on deep neural networks. The network uses a combination of convolutional layers and bi-directional long-short-term memory units in a residual structure. It learns the time-frequency characteristics of the dominant phases in an earthquake signal from three component data recorded on a single station. We train the network using 500,000 seismograms (250k associated with tectonic earthquakes and 250k identified as noise) recorded in Northern California and tested it with an F-score of 99.95. The robustness of the trained model with respect to the noise level and non-earthquake signals is shown by applying it to a set of semi-synthetic signals. The model is applied to one month of continuous data recorded at Central Arkansas to demonstrate its efficiency, generalization, and sensitivity. Our model is able to detect more than 700 microearthquakes as small as -1.3 ML induced during hydraulic fracturing far away than the training region. The performance of the model is compared with STA/LTA, template matching, and FAST algorithms. Our results indicate an efficient and reliable performance of CRED. This framework holds great promise in lowering the detection threshold while minimizing false positive detection rates.
        △ Less
",earthquak signal detect core observ seismolog good detect algorithm sensit small weak event varieti waveform shape robust background nois non earthquak signal effici process larg data volum introduc cnn rnn earthquak detector cred detector base deep neural network network use combin convolut layer bi direct long short term memori unit residu structur learn time frequenc characterist domin phase earthquak signal three compon data record singl station train network use seismogram k associ tecton earthquak k identifi nois record northern california test f score robust train model respect nois level non earthquak signal shown appli set semi synthet signal model appli one month continu data record central arkansa demonstr effici gener sensit model abl detect microearthquak small ml induc hydraul fractur far away train region perform model compar sta lta templat match fast algorithm result indic effici reliabl perform cred framework hold great promis lower detect threshold minim fals posit detect rate less
577,1810.01963,"
        Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems, however, use simple generalized heuristics and ignore workload structure, since developing and tuning a bespoke heuristic for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically.
  Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond specifying a high-level objective such as minimizing average job completion time. Off-the-shelf RL techniques, however, cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent new RL training methods for continuous job arrivals.
  Our prototype integration with Spark on a 25-node cluster shows that Decima outperforms several heuristics, including hand-tuned ones, by at least 21%. Further experiments with an industrial production workload trace demonstrate that Decima delivers up to a 17% reduction in average job completion time and scales to large clusters.
        △ Less
",effici schedul data process job distribut comput cluster requir complex algorithm current system howev use simpl gener heurist ignor workload structur sinc develop tune bespok heurist workload infeas paper show modern machin learn techniqu gener highli effici polici automat decima use reinforc learn rl neural network learn workload specif schedul algorithm without human instruct beyond specifi high level object minim averag job complet time shelf rl techniqu howev cannot handl complex scale schedul problem build decima develop new represent job depend graph design scalabl rl model invent new rl train method continu job arriv prototyp integr spark node cluster show decima outperform sever heurist includ hand tune one least experi industri product workload trace demonstr decima deliv reduct averag job complet time scale larg cluster less
578,1810.01946,"
        Given a grid terrain T and a viewpoint v, the viewshed of v is the set of grid points of T that are visible from v. To decide whether a point p is visible one needs to interpolate the elevation of the terrain along the line-of-sight vp. Existing viewshed algorithms differ widely in what points they chose to interpolate and how they interpolate the terrain. These choices crucially affect the running time and accuracy of the algorithms. This paper describes I/O-efficient algorithms for computing visibility maps in a couple of different models.
  First, we describe two algorithms that sweep the terrain by rotating a ray around the viewpoint while maintaining the terrain profile along the ray. Second, we describe an algorithm which sweeps the terrain centrifugally, growing a star-shaped region around the viewpoint while maintaining the approximate visible horizon of the terrain within the swept region. Our last two algorithms are based on computing and merging horizons. All algorithms are I/O-efficient in the I/O-model of Aggarwal and Vitter. We present an experimental analysis on large terrains obtained from NASA SRTM data. All our algorithms are scalable to volumes of data that are over 50 times larger than main memory. Our main finding is that, in practice, horizons are significantly smaller than their theoretical worst case bound, which makes horizon-based approaches very fast. Our last two algorithms, which compute the most accurate viewshed, turn out to be very fast in practice, although their worst-case bound is inferior.
        △ Less
",given grid terrain viewpoint v viewsh v set grid point visibl v decid whether point p visibl one need interpol elev terrain along line sight vp exist viewsh algorithm differ wide point chose interpol interpol terrain choic crucial affect run time accuraci algorithm paper describ effici algorithm comput visibl map coupl differ model first describ two algorithm sweep terrain rotat ray around viewpoint maintain terrain profil along ray second describ algorithm sweep terrain centrifug grow star shape region around viewpoint maintain approxim visibl horizon terrain within swept region last two algorithm base comput merg horizon algorithm effici model aggarw vitter present experiment analysi larg terrain obtain nasa srtm data algorithm scalabl volum data time larger main memori main find practic horizon significantli smaller theoret worst case bound make horizon base approach fast last two algorithm comput accur viewsh turn fast practic although worst case bound inferior less
579,1810.01945,"
        A growing issue in the modern cyberspace world is the direct identification of malicious activity over network connections. The boom of the machine learning industry in the past few years has led to the increasing usage of machine learning technologies, which are especially prevalent in the network intrusion detection research community. When utilizing these fairly contemporary techniques, the community has realized that datasets are pivotal for identifying malicious packets and connections, particularly ones associated with information concerning labeling in order to construct learning models. However, there exists a shortage of publicly available, relevant datasets to researchers in the network intrusion detection community. Thus, in this paper, we introduce a method to construct labeled flow data by combining the packet meta-information with IDS logs to infer labels for intrusion detection research. Specifically, we designed a NetFlow-compatible format due to the capability of a a large body of network devices, such as routers and switches, to export NetFlow records from raw traffic. In doing so, the introduced method at hand would aid researchers to access relevant network flow datasets along with label information.
        △ Less
",grow issu modern cyberspac world direct identif malici activ network connect boom machin learn industri past year led increas usag machin learn technolog especi preval network intrus detect research commun util fairli contemporari techniqu commun realiz dataset pivot identifi malici packet connect particularli one associ inform concern label order construct learn model howev exist shortag publicli avail relev dataset research network intrus detect commun thu paper introduc method construct label flow data combin packet meta inform id log infer label intrus detect research specif design netflow compat format due capabl larg bodi network devic router switch export netflow record raw traffic introduc method hand would aid research access relev network flow dataset along label inform less
580,1810.01943,"
        Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license {https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.
  The package includes a comprehensive set of fairness metrics for datasets and models, explanations for these metrics, and algorithms to mitigate bias in datasets and models. It also includes an interactive Web experience (https://aif360.mybluemix.net) that provides a gentle introduction to the concepts and capabilities for line-of-business users, as well as extensive documentation, usage guidance, and industry-specific tutorials to enable data scientists and practitioners to incorporate the most appropriate tool for their problem into their work products. The architecture of the package has been engineered to conform to a standard paradigm used in data science, thereby further improving usability for practitioners. Such architectural design and abstractions enable researchers and developers to extend the toolkit with their new algorithms and improvements, and to use it for performance benchmarking. A built-in testing infrastructure maintains code quality.
        △ Less
",fair increasingli import concern machin learn model use support decis make high stake applic mortgag lend hire prison sentenc paper introduc new open sourc python toolkit algorithm fair ai fair aif releas apach v licens http github com ibm aif main object toolkit help facilit transit fair research algorithm use industri set provid common framework fair research share evalu algorithm packag includ comprehens set fair metric dataset model explan metric algorithm mitig bia dataset model also includ interact web experi http aif mybluemix net provid gentl introduct concept capabl line busi user well extens document usag guidanc industri specif tutori enabl data scientist practition incorpor appropri tool problem work product architectur packag engin conform standard paradigm use data scienc therebi improv usabl practition architectur design abstract enabl research develop extend toolkit new algorithm improv use perform benchmark built test infrastructur maintain code qualiti less
581,1810.01940,"
        Designing optimal controllers continues to be challenging as systems are becoming complex and are inherently nonlinear. The principal advantage of reinforcement learning (RL) is its ability to learn from the interaction with the environment and provide optimal control strategy. In this paper, RL is explored in the context of control of the benchmark cartpole dynamical system with no prior knowledge of the dynamics. RL algorithms such as temporal-difference, policy gradient actor-critic, and value function approximation are compared in this context with the standard LQR solution. Further, we propose a novel approach to integrate RL and swing-up controllers.
        △ Less
",design optim control continu challeng system becom complex inher nonlinear princip advantag reinforc learn rl abil learn interact environ provid optim control strategi paper rl explor context control benchmark cartpol dynam system prior knowledg dynam rl algorithm tempor differ polici gradient actor critic valu function approxim compar context standard lqr solut propos novel approach integr rl swing control less
582,1810.01937,"
        Knowledge distillation (KD) is a popular method for reducing the computational overhead of deep network inference, in which the output of a teacher model is used to train a smaller, faster student model. Hint training (i.e., FitNets) extends KD by regressing a student model's intermediate representation to a teacher model's intermediate representation. In this work, we introduce bLock-wise Intermediate representation Training (LIT), a novel model compression technique that extends the use of intermediate representations in deep network compression, outperforming KD and hint training. LIT has two key ideas: 1) LIT trains a student of the same width (but shallower depth) as the teacher by directly comparing the intermediate representations, and 2) LIT uses the intermediate representation from the previous block in the teacher model as an input to the current student block during training, avoiding unstable intermediate representations in the student network. We show that LIT provides substantial reductions in network depth without loss in accuracy -- for example, LIT can compress a ResNeXt-110 to a ResNeXt-20 (5.5x) on CIFAR10 and a VDCNN-29 to a VDCNN-9 (3.2x) on Amazon Reviews without loss in accuracy, outperforming KD and hint training in network size for a given accuracy. We also show that applying LIT to identical student/teacher architectures increases the accuracy of the student model above the teacher model, outperforming the recently-proposed Born Again Networks procedure on ResNet, ResNeXt, and VDCNN. Finally, we show that LIT can effectively compress GAN generators, which are not supported in the KD framework because GANs output pixels as opposed to probabilities.
        △ Less
",knowledg distil kd popular method reduc comput overhead deep network infer output teacher model use train smaller faster student model hint train e fitnet extend kd regress student model intermedi represent teacher model intermedi represent work introduc block wise intermedi represent train lit novel model compress techniqu extend use intermedi represent deep network compress outperform kd hint train lit two key idea lit train student width shallow depth teacher directli compar intermedi represent lit use intermedi represent previou block teacher model input current student block train avoid unstabl intermedi represent student network show lit provid substanti reduct network depth without loss accuraci exampl lit compress resnext resnext x cifar vdcnn vdcnn x amazon review without loss accuraci outperform kd hint train network size given accuraci also show appli lit ident student teacher architectur increas accuraci student model teacher model outperform recent propos born network procedur resnet resnext vdcnn final show lit effect compress gan gener support kd framework gan output pixel oppos probabl less
583,1810.01928,"
        For proper generalization performance of convolutional neural networks (CNNs) in medical image segmentation, the learnt features should be invariant under particular non-linear shape variations of the input. To induce invariance in CNNs to such transformations, we propose Probabilistic Augmentation of Data using Diffeomorphic Image Transformation (PADDIT) -- a systematic framework for generating realistic transformations that can be used to augment data for training CNNs. We show that CNNs trained with PADDIT outperforms CNNs trained without augmentation and with generic augmentation in segmenting white matter hyperintensities from T1 and FLAIR brain MRI scans.
        △ Less
",proper gener perform convolut neural network cnn medic imag segment learnt featur invari particular non linear shape variat input induc invari cnn transform propos probabilist augment data use diffeomorph imag transform paddit systemat framework gener realist transform use augment data train cnn show cnn train paddit outperform cnn train without augment gener augment segment white matter hyperintens flair brain mri scan less
584,1810.01926,"
        Challenges for physical solitaire puzzle games are typically designed in advance by humans and limited in number. Alternately, some games incorporate stochastic setup rules, where the human solver randomly sets up the game board before solving the challenge, which can greatly increase the number of possible challenges. However, these setup rules can often generate unsolvable or uninteresting challenges. To better understand these setup processes, we apply a taxonomy for procedural content generation algorithms to solitaire puzzle games. In particular, for the game Fujisan, we examine how different stochastic challenge generation algorithms attempt to minimize undesirable challenges, and we report their affect on ease of physical setup, challenge solvability, and challenge difficulty. We find that algorithms can be simple for the solver yet generate solvable and difficult challenges, by constraining randomness through embedding sub-elements of the puzzle mechanics into the physical pieces of the game.
        △ Less
",challeng physic solitair puzzl game typic design advanc human limit number altern game incorpor stochast setup rule human solver randomli set game board solv challeng greatli increas number possibl challeng howev setup rule often gener unsolv uninterest challeng better understand setup process appli taxonomi procedur content gener algorithm solitair puzzl game particular game fujisan examin differ stochast challeng gener algorithm attempt minim undesir challeng report affect eas physic setup challeng solvabl challeng difficulti find algorithm simpl solver yet gener solvabl difficult challeng constrain random embed sub element puzzl mechan physic piec game less
585,1810.01925,"
        This paper examines the long-run behavior of learning with bandit feedback in non-cooperative concave games. The bandit framework accounts for extremely low-information environments where the agents may not even know they are playing a game; as such, the agents' most sensible choice in this setting would be to employ a no-regret learning algorithm. In general, this does not mean that the players' behavior stabilizes in the long run: no-regret learning may lead to cycles, even with perfect gradient information. However, if a standard monotonicity condition is satisfied, our analysis shows that no-regret learning based on mirror descent with bandit feedback converges to Nash equilibrium with probability $1$. We also derive an upper bound for the convergence rate of the process that nearly matches the best attainable rate for single-agent bandit stochastic optimization.
        △ Less
",paper examin long run behavior learn bandit feedback non cooper concav game bandit framework account extrem low inform environ agent may even know play game agent sensibl choic set would employ regret learn algorithm gener mean player behavior stabil long run regret learn may lead cycl even perfect gradient inform howev standard monoton condit satisfi analysi show regret learn base mirror descent bandit feedback converg nash equilibrium probabl also deriv upper bound converg rate process nearli match best attain rate singl agent bandit stochast optim less
586,1810.01921,"
        Complex networks have become powerful mechanisms for studying a variety of realworld systems. Consequently, many human-designed network models are proposed that reproduce nontrivial properties of complex networks, such as long-tail degree distribution or high clustering coefficient. Therefore, we may utilize network models in order to generate graphs similar to desired networks. However, a desired network structure may deviate from emerging structure of any generative model, because no selected single model may support all the needed properties of the target graph and instead, each network model reflects a subset of the required features. In contrast to the classical approach of network modeling, an appropriate modern network model should adapt the desired features of the target network. In this paper, we propose an automatic approach for constructing network models that are adapted to the desired network features. We employ Genetic Algorithms in order to evolve network models based on the characteristics of the target networks. The experimental evaluations show that our proposed framework, called NetMix, results network models that outperform baseline models according to the compliance with the desired features of the target networks.
        △ Less
",complex network becom power mechan studi varieti realworld system consequ mani human design network model propos reproduc nontrivi properti complex network long tail degre distribut high cluster coeffici therefor may util network model order gener graph similar desir network howev desir network structur may deviat emerg structur gener model select singl model may support need properti target graph instead network model reflect subset requir featur contrast classic approach network model appropri modern network model adapt desir featur target network paper propos automat approach construct network model adapt desir network featur employ genet algorithm order evolv network model base characterist target network experiment evalu show propos framework call netmix result network model outperform baselin model accord complianc desir featur target network less
587,1810.01920,"
        Inverse optimization is a powerful paradigm for learning preferences and restrictions that explain the behavior of a decision maker, based on a set of external signal and the corresponding decision pairs. However, the majority of existing inverse optimization algorithms are designed specifically in batch setting, where all the data are available in advance. As a consequence, there has been rare use of these methods in an online setting that actually is more suitable for real-time applications. In this paper, we propose a general framework for inverse optimization through online learning. Specifically, we develop an online learning algorithm that uses an implicit update rule which can handle noisy data. Moreover, under additional regularity assumptions in terms of the data and the model, we prove that our algorithm converges at a rate of $\mathcal{O}(1/\sqrt{T})$ and is statistically consistent. In our experiments, we show the online learning approach can learn the parameters with a great accuracy and is very robust to noises, and achieves a drastic improvement in computational efficacy over the batch learning approach.
        △ Less
",invers optim power paradigm learn prefer restrict explain behavior decis maker base set extern signal correspond decis pair howev major exist invers optim algorithm design specif batch set data avail advanc consequ rare use method onlin set actual suitabl real time applic paper propos gener framework invers optim onlin learn specif develop onlin learn algorithm use implicit updat rule handl noisi data moreov addit regular assumpt term data model prove algorithm converg rate mathcal sqrt statist consist experi show onlin learn approach learn paramet great accuraci robust nois achiev drastic improv comput efficaci batch learn approach less
588,1810.01916,"
        Optical machine learning offers advantages in terms of power efficiency, scalability and computation speed. Recently, an optical machine learning method based on diffractive deep neural networks (D2NNs) has been introduced to execute a function as the input light diffracts through passive layers, designed by deep learning using a computer. Here we introduce improvements to D2NNs by changing the training loss function and reducing the impact of vanishing gradients in the error back-propagation step. Using five phase-only diffractive layers, we numerically achieved a classification accuracy of 97.18% and 87.67% for optical recognition of handwritten digits and fashion products, respectively; using both phase and amplitude modulation (complex-valued) at each layer, our inference performance improved to 97.81% and 88.76%, respectively. Furthermore, we report the integration of D2NNs with electronic neural networks to create hybrid classifiers that significantly reduce the number of input pixels into an electronic network using an ultra-compact front-end D2NN with a layer-to-layer distance of a few wavelengths, also reducing the complexity of the successive electronic network. Using a 5-layer phase-only D2NN jointly-optimized with a single fully-connected electronic layer, we achieved a classification accuracy of 98.17% and 89.90% for the recognition of handwritten digits and fashion products, respectively. Moreover, the input to the electronic network was compressed by >7.8 times down to 10x10 pixels, making the jointly-optimized hybrid system perform classification with a simple electronic layer. Beyond creating low-power and high-frame rate ubiquitous machine learning platforms, such D2NN-based hybrid neural networks will find applications in optical imager and sensor design.
        △ Less
",optic machin learn offer advantag term power effici scalabl comput speed recent optic machin learn method base diffract deep neural network nn introduc execut function input light diffract passiv layer design deep learn use comput introduc improv nn chang train loss function reduc impact vanish gradient error back propag step use five phase diffract layer numer achiev classif accuraci optic recognit handwritten digit fashion product respect use phase amplitud modul complex valu layer infer perform improv respect furthermor report integr nn electron neural network creat hybrid classifi significantli reduc number input pixel electron network use ultra compact front end nn layer layer distanc wavelength also reduc complex success electron network use layer phase nn jointli optim singl fulli connect electron layer achiev classif accuraci recognit handwritten digit fashion product respect moreov input electron network compress time x pixel make jointli optim hybrid system perform classif simpl electron layer beyond creat low power high frame rate ubiquit machin learn platform nn base hybrid neural network find applic optic imag sensor design less
589,1810.01912,"
        This study proposes a novel way of identifying the sentiment of the phrases used in the legal domain. The added complexity of the language used in law, and the inability of the existing systems to accurately predict the sentiments of words in law are the main motivations behind this study. This is a transfer learning approach, which can be used for other domain adaptation tasks as well. The proposed methodology achieves an improvement of over 6\% compared to the source model's accuracy in the legal domain.
        △ Less
",studi propos novel way identifi sentiment phrase use legal domain ad complex languag use law inabl exist system accur predict sentiment word law main motiv behind studi transfer learn approach use domain adapt task well propos methodolog achiev improv compar sourc model accuraci legal domain less
590,1810.01904,"
        The paper introduces RPSE, Reification as a Paradigm of Software Engineering, and enumerates the most important theoretical and practical problems of the development and application of this paradigm. Main thesis: Software engineering is the reification (materialization of ideas) via the transformation of mental models into code executed on computers . Within the proposed paradigm: 1.All basic processes of software engineering are concrete variants (implementations) of the process of constructing chains of mental and material models I1, I2,..In, M1, M2, ..Mm. The last most specific model in this chain is, as a rule, program code. 2.The essence of software engineering is the construction of such chains. 3.All main issues of optimizing the development, its cost, and quality can be reduced to the optimization of construction of the corresponding chain of models.
        △ Less
",paper introduc rpse reific paradigm softwar engin enumer import theoret practic problem develop applic paradigm main thesi softwar engin reific materi idea via transform mental model code execut comput within propos paradigm basic process softwar engin concret variant implement process construct chain mental materi model mm last specif model chain rule program code essenc softwar engin construct chain main issu optim develop cost qualiti reduc optim construct correspond chain model less
591,1810.01898,"
        Face recognition in images is an active area of interest among the computer vision researchers. However, recognizing human face in an unconstrained environment, is a relatively less-explored area of research. Multiple face recognition in unconstrained environment is a challenging task, due to the variation of view-point, scale, pose, illumination and expression of the face images. Partial occlusion of faces makes the recognition task even more challenging. The contribution of this paper is two-folds: introducing a challenging multiface dataset (i.e., IIITS MFace Dataset) for face recognition in unconstrained environment and evaluating the performance of state-of-the-art hand-designed and deep learning based face descriptors on the dataset. The proposed IIITS MFace dataset contains faces with challenges like pose variation, occlusion, mask, spectacle, expressions, change of illumination, etc. We experiment with several state-of-the-art face descriptors, including recent deep learning based face descriptors like VGGFace, and compare with the existing benchmark face datasets. Results of the experiments clearly show that the difficulty level of the proposed dataset is much higher compared to the benchmark datasets.
        △ Less
",face recognit imag activ area interest among comput vision research howev recogn human face unconstrain environ rel less explor area research multipl face recognit unconstrain environ challeng task due variat view point scale pose illumin express face imag partial occlus face make recognit task even challeng contribut paper two fold introduc challeng multifac dataset e iiit mface dataset face recognit unconstrain environ evalu perform state art hand design deep learn base face descriptor dataset propos iiit mface dataset contain face challeng like pose variat occlus mask spectacl express chang illumin etc experi sever state art face descriptor includ recent deep learn base face descriptor like vggface compar exist benchmark face dataset result experi clearli show difficulti level propos dataset much higher compar benchmark dataset less
592,1810.01878,"
        This paper proposes a centroid-based clustering algorithm which is capable of clustering data-points with n-features in real-time, without having to specify the number of clusters to be formed. We present the core logic behind the algorithm, a similarity measure, which collectively decides whether to assign an incoming data-point to a pre-existing cluster, or create a new cluster & assign the data-point to it. The implementation of the proposed algorithm clearly demonstrates how efficiently a data-point with high dimensionality of features is assigned to an appropriate cluster with minimal operations. The proposed algorithm is very application specific and is applicable when the need is perform clustering analysis of real-time data-points, where the similarity measure between an incoming data-point and the cluster to which the data-point is to be associated with, is greater than predefined Level-of-Similarity.
        △ Less
",paper propos centroid base cluster algorithm capabl cluster data point n featur real time without specifi number cluster form present core logic behind algorithm similar measur collect decid whether assign incom data point pre exist cluster creat new cluster assign data point implement propos algorithm clearli demonstr effici data point high dimension featur assign appropri cluster minim oper propos algorithm applic specif applic need perform cluster analysi real time data point similar measur incom data point cluster data point associ greater predefin level similar less
593,1810.01877,"
        This paper presents a general framework for norm-based capacity control for $L_{p,q}$ weight normalized deep neural networks. We establish the upper bound on the Rademacher complexities of this family. With an $L_{p,q}$ normalization where $q\le p^*$, and $1/p+1/p^{*}=1$, we discuss properties of a width-independent capacity control, which only depends on depth by a square root term. We further analyze the approximation properties of $L_{p,q}$ weight normalized deep neural networks. In particular, for an $L_{1,\infty}$ weight normalized network, the approximation error can be controlled by the $L_1$ norm of the output layer, and the corresponding generalization error only depends on the architecture by the square root of the depth.
        △ Less
",paper present gener framework norm base capac control l p q weight normal deep neural network establish upper bound rademach complex famili l p q normal q le p p p discuss properti width independ capac control depend depth squar root term analyz approxim properti l p q weight normal deep neural network particular l infti weight normal network approxim error control l norm output layer correspond gener error depend architectur squar root depth less
594,1810.01876,"
        Traditional wisdom in generative modeling literature is that spurious samples that a model can generate are errors and they should be avoided. Recent research, however, has shown interest in studying or even exploiting such samples instead of eliminating them. In this paper, we ask the question whether such samples can be eliminated all together without sacrificing coverage of the generating distribution. For the class of models we consider, we experimentally demonstrate that this is not possible without losing the ability to model some of the test samples. While our results need to be confirmed on a broader set of model families, these initial findings provide partial evidence that spurious samples share structural properties with the learned dataset, which, in turn, suggests they are not simply errors but a feature of deep generative nets.
        △ Less
",tradit wisdom gener model literatur spuriou sampl model gener error avoid recent research howev shown interest studi even exploit sampl instead elimin paper ask question whether sampl elimin togeth without sacrif coverag gener distribut class model consid experiment demonstr possibl without lose abil model test sampl result need confirm broader set model famili initi find provid partial evid spuriou sampl share structur properti learn dataset turn suggest simpli error featur deep gener net less
595,1810.01875,"
        Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices. In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure. Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid. These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization. We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent. We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.
        △ Less
",neural network quantiz becom import research area due great impact deploy larg model resourc constrain devic order train network effect discret without loss perform introduc differenti quantiz procedur differenti achiev transform continu distribut weight activ network categor distribut quantiz grid subsequ relax continu surrog allow effici gradient base optim show stochast round seen special case propos approach formul quantiz grid also optim gradient descent experiment valid perform method mnist cifar imagenet classif less
596,1810.01873,"
        This paper presents a new optimisation approach to train Deep Neural Networks (DNNs) with discriminative sequence criteria. At each iteration, the method combines information from the Natural Gradient (NG) direction with local curvature information of the error surface that enables better paths on the parameter manifold to be traversed. The method is derived using an alternative derivation of Taylor's theorem using the concepts of manifolds, tangent vectors and directional derivatives from the perspective of Information Geometry. The efficacy of the method is shown within a Hessian Free (HF) style optimisation framework to sequence train both standard fully-connected DNNs and Time Delay Neural Networks as speech recognition acoustic models. It is shown that for the same number of updates the proposed approach achieves larger reductions in the word error rate (WER) than both NG and HF, and also leads to a lower WER than standard stochastic gradient descent. The paper also addresses the issue of over-fitting due to mismatch between training criterion and Word Error Rate (WER) that primarily arises during sequence training of ReLU-DNN models.
        △ Less
",paper present new optimis approach train deep neural network dnn discrimin sequenc criteria iter method combin inform natur gradient ng direct local curvatur inform error surfac enabl better path paramet manifold travers method deriv use altern deriv taylor theorem use concept manifold tangent vector direct deriv perspect inform geometri efficaci method shown within hessian free hf style optimis framework sequenc train standard fulli connect dnn time delay neural network speech recognit acoust model shown number updat propos approach achiev larger reduct word error rate wer ng hf also lead lower wer standard stochast gradient descent paper also address issu fit due mismatch train criterion word error rate wer primarili aris sequenc train relu dnn model less
597,1810.01872,"
        The design of robotic systems is largely dictated by our purely human intuition about how we perceive the world. This intuition has been proven incorrect with regard to a number of critical issues, such as visual change blindness. In order to develop truly autonomous robots, we must step away from this intuition and let robotic agents develop their own way of perceiving. The robot should start from scratch and gradually develop perceptual notions, under no prior assumptions, exclusively by looking into its sensorimotor experience and identifying repetitive patterns and invariants. One of the most fundamental perceptual notions, space, cannot be an exception to this requirement. In this paper we look into the prerequisites for the emergence of simplified spatial notions on the basis of a robot's sensorimotor flow. We show that the notion of space as environment-independent cannot be deduced solely from exteroceptive information, which is highly variable and is mainly determined by the contents of the environment. The environment-independent definition of space can be approached by looking into the functions that link the motor commands to changes in exteroceptive inputs. In a sufficiently rich environment, the kernels of these functions correspond uniquely to the spatial configuration of the agent's exteroceptors. We simulate a redundant robotic arm with a retina installed at its end-point and show how this agent can learn the configuration space of its retina. The resulting manifold has the topology of the Cartesian product of a plane and a circle, and corresponds to the planar position and orientation of the retina.
        △ Less
",design robot system larg dictat pure human intuit perceiv world intuit proven incorrect regard number critic issu visual chang blind order develop truli autonom robot must step away intuit let robot agent develop way perceiv robot start scratch gradual develop perceptu notion prior assumpt exclus look sensorimotor experi identifi repetit pattern invari one fundament perceptu notion space cannot except requir paper look prerequisit emerg simplifi spatial notion basi robot sensorimotor flow show notion space environ independ cannot deduc sole exterocept inform highli variabl mainli determin content environ environ independ definit space approach look function link motor command chang exterocept input suffici rich environ kernel function correspond uniqu spatial configur agent exteroceptor simul redund robot arm retina instal end point show agent learn configur space retina result manifold topolog cartesian product plane circl correspond planar posit orient retina less
598,1810.01871,"
        Artificial perception is traditionally handled by hand-designing task specific algorithms. However, a truly autonomous robot should develop perceptive abilities on its own, by interacting with its environment, and adapting to new situations. The sensorimotor contingencies theory proposes to ground the development of those perceptive abilities in the way the agent can actively transform its sensory inputs. We propose a sensorimotor approach, inspired by this theory, in which the agent explores the world and discovers its properties by capturing the sensorimotor regularities they induce. This work presents an application of this approach to the discovery of a so-called visual field as the set of regularities that a visual sensor imposes on a naive agent's experience. A formalism is proposed to describe how those regularities can be captured in a sensorimotor predictive model. Finally, the approach is evaluated on a simulated system coarsely inspired from the human retina.
        △ Less
",artifici percept tradit handl hand design task specif algorithm howev truli autonom robot develop percept abil interact environ adapt new situat sensorimotor conting theori propos ground develop percept abil way agent activ transform sensori input propos sensorimotor approach inspir theori agent explor world discov properti captur sensorimotor regular induc work present applic approach discoveri call visual field set regular visual sensor impos naiv agent experi formal propos describ regular captur sensorimotor predict model final approach evalu simul system coars inspir human retina less
599,1810.01870,"
        Sensorimotor contingency theory offers a promising account of the nature of perception, a topic rarely addressed in the robotics community. We propose a developmental framework to address the problem of the autonomous acquisition of sensorimotor contingencies by a naive robot. While exploring the world, the robot internally encodes contingencies as predictive models that capture the structure they imply in its sensorimotor experience. Three preliminary applications are presented to illustrate our approach to the acquisition of perceptive abilities: discovering the environment, discovering objects, and discovering a visual field.
        △ Less
",sensorimotor conting theori offer promis account natur percept topic rare address robot commun propos development framework address problem autonom acquisit sensorimotor conting naiv robot explor world robot intern encod conting predict model captur structur impli sensorimotor experi three preliminari applic present illustr approach acquisit percept abil discov environ discov object discov visual field less
